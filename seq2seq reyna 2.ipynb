{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s_qNSzzyaCbD"
   },
   "source": [
    "##### Copyright 2019 The TensorFlow Authors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "tnxXKDjq3jEL"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "import unicodedata\n",
    "import re\n",
    "import numpy as np\n",
    "import os\n",
    "import io\n",
    "import time\n",
    "\n",
    "# from nltk import ngrams\n",
    "# from collections import Counter\n",
    "import  nltk.translate.bleu_score as bleu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wfodePkj3jEa"
   },
   "source": [
    "## Download and prepare the dataset\n",
    "\n",
    "We'll use a language dataset provided by http://www.manythings.org/anki/. This dataset contains language translation pairs in the format:\n",
    "\n",
    "```\n",
    "May I borrow this book?\t¿Puedo tomar prestado este libro?\n",
    "```\n",
    "\n",
    "There are a variety of languages available, but we'll use the English-Spanish dataset. For convenience, we've hosted a copy of this dataset on Google Cloud, but you can also download your own copy. After downloading the dataset, here are the steps we'll take to prepare the data:\n",
    "\n",
    "1. Add a *start* and *end* token to each sentence.\n",
    "2. Clean the sentences by removing special characters.\n",
    "3. Create a word index and reverse word index (dictionaries mapping from word → id and id → word).\n",
    "4. Pad each sentence to a maximum length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples, metadata = tfds.load('ted_hrlr_translate/pt_to_en', with_info=True,\n",
    "                               as_supervised=True)\n",
    "train_examples, val_examples = examples['train'], examples['validation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "rd0jw-eC3jEh"
   },
   "outputs": [],
   "source": [
    "# Converts the unicode file to ascii\n",
    "# def unicode_to_ascii(s):\n",
    "#   return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
    "#       if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "\n",
    "def preprocess_sentence(w):\n",
    "#   w = unicode_to_ascii(w.lower().strip())\n",
    "  w = w.lower().strip()\n",
    "\n",
    "  # creating a space between a word and the punctuation following it\n",
    "  # eg: \"he is a boy.\" => \"he is a boy .\"\n",
    "  # Reference:- https://stackoverflow.com/questions/3645931/python-padding-punctuation-with-white-spaces-keeping-punctuation\n",
    "  w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n",
    "  w = re.sub(r'[\" \"]+', \" \", w)\n",
    "\n",
    "  # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n",
    "  w = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", w)\n",
    "\n",
    "  w = w.strip()\n",
    "\n",
    "  # adding a start and an end token to the sentence\n",
    "  # so that the model know when to start and stop predicting.\n",
    "  w = '<start> ' + w + ' <end>'\n",
    "  return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "opI2GzOt479E"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> may i borrow this book ? <end>\n",
      "b'<start> \\xc2\\xbf puedo tomar prestado este libro ? <end>'\n"
     ]
    }
   ],
   "source": [
    "en_sentence = u\"May I borrow this book?\"\n",
    "sp_sentence = u\"¿Puedo tomar prestado este libro?\"\n",
    "print(preprocess_sentence(en_sentence))\n",
    "print(preprocess_sentence(sp_sentence).encode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(examples, num1,num2):\n",
    "#   lines = io.open(path, encoding='UTF-8').read().strip().split('\\n')\n",
    "\n",
    "  word_pairs = [[preprocess_sentence(pt_.numpy().decode(\"utf-8\", \"ignore\")), preprocess_sentence(en_.numpy().decode(\"utf-8\", \"ignore\"))]  for pt_, en_ in examples]\n",
    "  word_pairs = word_pairs[num1:num2]\n",
    "  return zip(*word_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "cTbSbBz55QtF"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> os meus alunos t m problemas , problemas sociais , emocionais e econ micos , que voc s nem podem imaginar . <end>\n",
      "<start> my students have problems social , emotional and economic problems you could never imagine . <end>\n"
     ]
    }
   ],
   "source": [
    "pt, en = create_dataset(train_examples,None,None)\n",
    "print(pt[-1])\n",
    "print(en[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "bIOn8RCNDJXG"
   },
   "outputs": [],
   "source": [
    "def tokenize(lang):\n",
    "  lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "      filters='')\n",
    "  lang_tokenizer.fit_on_texts(lang)\n",
    "\n",
    "  tensor = lang_tokenizer.texts_to_sequences(lang)\n",
    "\n",
    "  tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor,\n",
    "                                                         padding='post')\n",
    "\n",
    "  return tensor, lang_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "eAY9k49G3jE_"
   },
   "outputs": [],
   "source": [
    "def load_dataset(examples,num1,num2):\n",
    "  # creating cleaned input, output pairs\n",
    "  inp_lang, targ_lang = create_dataset(examples,num1,num2)\n",
    "\n",
    "  input_tensor, inp_lang_tokenizer = tokenize(inp_lang)\n",
    "  target_tensor, targ_lang_tokenizer = tokenize(targ_lang)\n",
    "\n",
    "  return input_tensor, target_tensor, inp_lang_tokenizer, targ_lang_tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GOi42V79Ydlr"
   },
   "source": [
    "### Limit the size of the dataset to experiment faster (optional)\n",
    "\n",
    "Training on the complete dataset of >100,000 sentences will take a long time. To train faster, we can limit the size of the dataset to 30,000 sentences (of course, translation quality degrades with less data):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "cnxC7q-j3jFD"
   },
   "outputs": [],
   "source": [
    "# Try experimenting with the size of that dataset\n",
    "# num_examples = 30000\n",
    "num1 =0\n",
    "num2 = 30000\n",
    "input_tensor, target_tensor, inp_lang, targ_lang = load_dataset(train_examples,num1,num2)\n",
    "\n",
    "# Calculate max_length of the target tensors\n",
    "max_length_targ, max_length_inp = target_tensor.shape[1], input_tensor.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30000 30000\n"
     ]
    }
   ],
   "source": [
    "input_tensor_train = input_tensor\n",
    "target_tensor_train = target_tensor\n",
    "\n",
    "print(len(input_tensor_train), len(target_tensor_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   3,    9,   44, ...,    0,    0,    0],\n",
       "       [   3,   25,    9, ...,    0,    0,    0],\n",
       "       [   3,   25,   61, ...,    0,    0,    0],\n",
       "       ...,\n",
       "       [   3,   30,  161, ...,    0,    0,    0],\n",
       "       [   3,  216,   13, ...,    0,    0,    0],\n",
       "       [   3,   15, 5956, ...,    0,    0,    0]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_tensor_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rgCLkfv5uO3d"
   },
   "source": [
    "### Create a tf.data dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "TqHsArVZ3jFS"
   },
   "outputs": [],
   "source": [
    "BUFFER_SIZE = len(input_tensor_train)\n",
    "BATCH_SIZE = 4\n",
    "steps_per_epoch = len(input_tensor_train)//BATCH_SIZE\n",
    "embedding_dim = 16 #100\n",
    "units = 64 #1024\n",
    "vocab_inp_size = len(inp_lang.word_index)+1\n",
    "vocab_tar_size = len(targ_lang.word_index)+1\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "qc6-NK1GtWQt"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([4, 165]), TensorShape([4, 137]))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_input_batch, example_target_batch = next(iter(dataset))\n",
    "example_input_batch.shape, example_target_batch.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "nZ2rI24i3jFg"
   },
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "  def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
    "    super(Encoder, self).__init__()\n",
    "    self.batch_sz = batch_sz\n",
    "    self.enc_units = enc_units\n",
    "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "    self.gru = tf.keras.layers.GRU(self.enc_units,\n",
    "                                   return_sequences=True,\n",
    "                                   return_state=True,\n",
    "                                   recurrent_initializer='glorot_uniform')\n",
    "\n",
    "  def call(self, x, hidden):\n",
    "    x = self.embedding(x)\n",
    "    output, state = self.gru(x, initial_state = hidden)\n",
    "    return output, state\n",
    "\n",
    "  def initialize_hidden_state(self):\n",
    "    return tf.zeros((self.batch_sz, self.enc_units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "60gSVh05Jl6l"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder output shape: (batch size, sequence length, units) (4, 165, 64)\n",
      "Encoder Hidden state shape: (batch size, units) (4, 64)\n"
     ]
    }
   ],
   "source": [
    "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
    "\n",
    "# sample input\n",
    "sample_hidden = encoder.initialize_hidden_state()\n",
    "sample_output, sample_hidden = encoder(example_input_batch, sample_hidden)\n",
    "print ('Encoder output shape: (batch size, sequence length, units) {}'.format(sample_output.shape))\n",
    "print ('Encoder Hidden state shape: (batch size, units) {}'.format(sample_hidden.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k534zTHiDjQU"
   },
   "source": [
    "# Decoder Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "  def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
    "    super(Decoder, self).__init__()\n",
    "    self.batch_sz = batch_sz\n",
    "    self.dec_units = dec_units\n",
    "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "    self.gru = tf.keras.layers.GRU(self.dec_units,\n",
    "                                   return_sequences=True,\n",
    "                                   return_state=True,\n",
    "                                   recurrent_initializer='glorot_uniform')\n",
    "    self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "    # used for attention\n",
    "#     self.attention = BahdanauAttention(self.dec_units)\n",
    "\n",
    "  def call(self, x, hidden, enc_output):\n",
    "\n",
    "    x = self.embedding(x)\n",
    "\n",
    "\n",
    "    # passing the concatenated vector to the GRU\n",
    "    output, state = self.gru(x)\n",
    "\n",
    "    # output shape == (batch_size * 1, hidden_size)\n",
    "    output = tf.reshape(output, (-1, output.shape[2]))\n",
    "\n",
    "    # output shape == (batch_size, vocab)\n",
    "    x = self.fc(output)\n",
    "\n",
    "    return x, state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "P5UY8wko3jFp"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder output shape: (batch_size, vocab size) (4, 20847)\n"
     ]
    }
   ],
   "source": [
    "decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)\n",
    "\n",
    "sample_decoder_output, _ = decoder(tf.random.uniform((BATCH_SIZE, 1)),\n",
    "                                      sample_hidden, sample_output)\n",
    "\n",
    "print ('Decoder output shape: (batch_size, vocab size) {}'.format(sample_decoder_output.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_ch_71VbIRfK"
   },
   "source": [
    "## Define the optimizer and the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "WmTHr5iV3jFr"
   },
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "  loss_ = loss_object(real, pred)\n",
    "\n",
    "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "  loss_ *= mask\n",
    "\n",
    "  return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DMVWzzsfNl4e"
   },
   "source": [
    "## Checkpoints (Object-based saving)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "Zj8bXQTgNwrF"
   },
   "outputs": [],
   "source": [
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
    "                                 encoder=encoder,\n",
    "                                 decoder=decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hpObfY22IddU"
   },
   "source": [
    "## Training\n",
    "\n",
    "1. Pass the *input* through the *encoder* which return *encoder output* and the *encoder hidden state*.\n",
    "2. The encoder output, encoder hidden state and the decoder input (which is the *start token*) is passed to the decoder.\n",
    "3. The decoder returns the *predictions* and the *decoder hidden state*.\n",
    "4. The decoder hidden state is then passed back into the model and the predictions are used to calculate the loss.\n",
    "5. Use *teacher forcing* to decide the next input to the decoder.\n",
    "6. *Teacher forcing* is the technique where the *target word* is passed as the *next input* to the decoder.\n",
    "7. The final step is to calculate the gradients and apply it to the optimizer and backpropagate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "sC9ArXSsVfqn"
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(inp, targ, enc_hidden):\n",
    "  loss = 0\n",
    "\n",
    "  with tf.GradientTape() as tape:\n",
    "    enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
    "\n",
    "    dec_hidden = enc_hidden\n",
    "\n",
    "    dec_input = tf.expand_dims([targ_lang.word_index['<start>']] * BATCH_SIZE, 1)\n",
    "\n",
    "    # Teacher forcing - feeding the target as the next input\n",
    "    for t in range(1, targ.shape[1]):\n",
    "      # passing enc_output to the decoder\n",
    "      predictions, dec_hidden = decoder(dec_input, dec_hidden, enc_output)\n",
    "\n",
    "      loss += loss_function(targ[:, t], predictions)\n",
    "\n",
    "      # using teacher forcing\n",
    "      dec_input = tf.expand_dims(targ[:, t], 1)\n",
    "\n",
    "  batch_loss = (loss / int(targ.shape[1]))\n",
    "\n",
    "  variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "\n",
    "  gradients = tape.gradient(loss, variables)\n",
    "\n",
    "  optimizer.apply_gradients(zip(gradients, variables))\n",
    "\n",
    "  return batch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "ddefjBMa3jF0",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['encoder/embedding/embeddings:0', 'encoder/gru/kernel:0', 'encoder/gru/recurrent_kernel:0', 'encoder/gru/bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['encoder/embedding/embeddings:0', 'encoder/gru/kernel:0', 'encoder/gru/recurrent_kernel:0', 'encoder/gru/bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['encoder/embedding/embeddings:0', 'encoder/gru/kernel:0', 'encoder/gru/recurrent_kernel:0', 'encoder/gru/bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['encoder/embedding/embeddings:0', 'encoder/gru/kernel:0', 'encoder/gru/recurrent_kernel:0', 'encoder/gru/bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Loss 1.1070\n",
      "Epoch 1 Batch 100 Loss 1.4963\n",
      "Epoch 1 Batch 200 Loss 0.9471\n",
      "Epoch 1 Batch 300 Loss 0.7991\n",
      "Epoch 1 Batch 400 Loss 0.6944\n",
      "Epoch 1 Batch 500 Loss 0.6334\n",
      "Epoch 1 Batch 600 Loss 0.6683\n",
      "Epoch 1 Batch 700 Loss 1.1812\n",
      "Epoch 1 Batch 800 Loss 0.6730\n",
      "Epoch 1 Batch 900 Loss 0.8666\n",
      "Epoch 1 Batch 1000 Loss 0.8624\n",
      "Epoch 1 Batch 1100 Loss 0.8810\n",
      "Epoch 1 Batch 1200 Loss 0.4791\n",
      "Epoch 1 Batch 1300 Loss 0.4143\n",
      "Epoch 1 Batch 1400 Loss 0.3142\n",
      "Epoch 1 Batch 1500 Loss 1.3091\n",
      "Epoch 1 Batch 1600 Loss 0.9892\n",
      "Epoch 1 Batch 1700 Loss 0.8338\n",
      "Epoch 1 Batch 1800 Loss 1.3493\n",
      "Epoch 1 Batch 1900 Loss 1.0124\n",
      "Epoch 1 Batch 2000 Loss 0.5209\n",
      "Epoch 1 Batch 2100 Loss 1.3138\n",
      "Epoch 1 Batch 2200 Loss 0.6569\n",
      "Epoch 1 Batch 2300 Loss 1.1101\n",
      "Epoch 1 Batch 2400 Loss 0.5306\n",
      "Epoch 1 Batch 2500 Loss 0.8378\n",
      "Epoch 1 Batch 2600 Loss 0.6960\n",
      "Epoch 1 Batch 2700 Loss 1.7259\n",
      "Epoch 1 Batch 2800 Loss 0.4000\n",
      "Epoch 1 Batch 2900 Loss 0.4398\n",
      "Epoch 1 Batch 3000 Loss 0.4738\n",
      "Epoch 1 Batch 3100 Loss 1.1205\n",
      "Epoch 1 Batch 3200 Loss 0.4493\n",
      "Epoch 1 Batch 3300 Loss 0.9605\n",
      "Epoch 1 Batch 3400 Loss 0.8644\n",
      "Epoch 1 Batch 3500 Loss 1.1711\n",
      "Epoch 1 Batch 3600 Loss 0.8271\n",
      "Epoch 1 Batch 3700 Loss 1.3946\n",
      "Epoch 1 Batch 3800 Loss 1.3524\n",
      "Epoch 1 Batch 3900 Loss 1.0235\n",
      "Epoch 1 Batch 4000 Loss 0.9914\n",
      "Epoch 1 Batch 4100 Loss 0.9259\n",
      "Epoch 1 Batch 4200 Loss 0.7642\n",
      "Epoch 1 Batch 4300 Loss 0.9174\n",
      "Epoch 1 Batch 4400 Loss 0.6679\n",
      "Epoch 1 Batch 4500 Loss 0.7474\n",
      "Epoch 1 Batch 4600 Loss 0.9677\n",
      "Epoch 1 Batch 4700 Loss 0.9906\n",
      "Epoch 1 Batch 4800 Loss 0.7377\n",
      "Epoch 1 Batch 4900 Loss 0.8498\n",
      "Epoch 1 Batch 5000 Loss 0.6620\n",
      "Epoch 1 Batch 5100 Loss 0.4670\n",
      "Epoch 1 Batch 5200 Loss 0.9533\n",
      "Epoch 1 Batch 5300 Loss 0.7251\n",
      "Epoch 1 Batch 5400 Loss 0.5842\n",
      "Epoch 1 Batch 5500 Loss 0.7737\n",
      "Epoch 1 Batch 5600 Loss 1.0590\n",
      "Epoch 1 Batch 5700 Loss 0.5866\n",
      "Epoch 1 Batch 5800 Loss 0.9252\n",
      "Epoch 1 Batch 5900 Loss 0.7997\n",
      "Epoch 1 Batch 6000 Loss 0.6420\n",
      "Epoch 1 Batch 6100 Loss 0.3692\n",
      "Epoch 1 Batch 6200 Loss 0.4660\n",
      "Epoch 1 Batch 6300 Loss 0.7962\n",
      "Epoch 1 Batch 6400 Loss 0.8524\n",
      "Epoch 1 Batch 6500 Loss 0.8305\n",
      "Epoch 1 Batch 6600 Loss 1.0662\n",
      "Epoch 1 Batch 6700 Loss 1.1007\n",
      "Epoch 1 Batch 6800 Loss 0.5607\n",
      "Epoch 1 Batch 6900 Loss 0.5893\n",
      "Epoch 1 Batch 7000 Loss 0.7308\n",
      "Epoch 1 Batch 7100 Loss 0.7082\n",
      "Epoch 1 Batch 7200 Loss 1.0917\n",
      "Epoch 1 Batch 7300 Loss 0.5265\n",
      "Epoch 1 Batch 7400 Loss 0.6981\n",
      "Epoch 1 Loss 0.7845\n",
      "Time taken for 1 epoch 876.1645565032959 sec\n",
      "\n",
      "Epoch 2 Batch 0 Loss 0.6045\n",
      "Epoch 2 Batch 100 Loss 0.8203\n",
      "Epoch 2 Batch 200 Loss 0.6841\n",
      "Epoch 2 Batch 300 Loss 0.4978\n",
      "Epoch 2 Batch 400 Loss 1.0366\n",
      "Epoch 2 Batch 500 Loss 0.7299\n",
      "Epoch 2 Batch 600 Loss 0.8856\n",
      "Epoch 2 Batch 700 Loss 0.5068\n",
      "Epoch 2 Batch 800 Loss 0.5668\n",
      "Epoch 2 Batch 900 Loss 0.6976\n",
      "Epoch 2 Batch 1000 Loss 0.4946\n",
      "Epoch 2 Batch 1100 Loss 0.5395\n",
      "Epoch 2 Batch 1200 Loss 0.8240\n",
      "Epoch 2 Batch 1300 Loss 0.3443\n",
      "Epoch 2 Batch 1400 Loss 0.4609\n",
      "Epoch 2 Batch 1500 Loss 0.9524\n",
      "Epoch 2 Batch 1600 Loss 1.1645\n",
      "Epoch 2 Batch 1700 Loss 0.7238\n",
      "Epoch 2 Batch 1800 Loss 0.6054\n",
      "Epoch 2 Batch 1900 Loss 0.4636\n",
      "Epoch 2 Batch 2000 Loss 0.3688\n",
      "Epoch 2 Batch 2100 Loss 1.2732\n",
      "Epoch 2 Batch 2200 Loss 0.8598\n",
      "Epoch 2 Batch 2300 Loss 0.9515\n",
      "Epoch 2 Batch 2400 Loss 0.9307\n",
      "Epoch 2 Batch 2500 Loss 0.5814\n",
      "Epoch 2 Batch 2600 Loss 1.0862\n",
      "Epoch 2 Batch 2700 Loss 0.5069\n",
      "Epoch 2 Batch 2800 Loss 1.1440\n",
      "Epoch 2 Batch 2900 Loss 0.6216\n",
      "Epoch 2 Batch 3000 Loss 0.4663\n",
      "Epoch 2 Batch 3100 Loss 0.3959\n",
      "Epoch 2 Batch 3200 Loss 0.7714\n",
      "Epoch 2 Batch 3300 Loss 0.7234\n",
      "Epoch 2 Batch 3400 Loss 0.5284\n",
      "Epoch 2 Batch 3500 Loss 0.8545\n",
      "Epoch 2 Batch 3600 Loss 0.6456\n",
      "Epoch 2 Batch 3700 Loss 1.2372\n",
      "Epoch 2 Batch 3800 Loss 0.4592\n",
      "Epoch 2 Batch 3900 Loss 0.8614\n",
      "Epoch 2 Batch 4000 Loss 0.9260\n",
      "Epoch 2 Batch 4100 Loss 0.4313\n",
      "Epoch 2 Batch 4200 Loss 0.6509\n",
      "Epoch 2 Batch 4300 Loss 1.1229\n",
      "Epoch 2 Batch 4400 Loss 0.7006\n",
      "Epoch 2 Batch 4500 Loss 0.7725\n",
      "Epoch 2 Batch 4600 Loss 0.5457\n",
      "Epoch 2 Batch 4700 Loss 0.3780\n",
      "Epoch 2 Batch 4800 Loss 0.7366\n",
      "Epoch 2 Batch 4900 Loss 0.7901\n",
      "Epoch 2 Batch 5000 Loss 0.6344\n",
      "Epoch 2 Batch 5100 Loss 0.8645\n",
      "Epoch 2 Batch 5200 Loss 0.8201\n",
      "Epoch 2 Batch 5300 Loss 0.5661\n",
      "Epoch 2 Batch 5400 Loss 0.5368\n",
      "Epoch 2 Batch 5500 Loss 0.4820\n",
      "Epoch 2 Batch 5600 Loss 0.7144\n",
      "Epoch 2 Batch 5700 Loss 0.2992\n",
      "Epoch 2 Batch 5800 Loss 0.3894\n",
      "Epoch 2 Batch 5900 Loss 0.3222\n",
      "Epoch 2 Batch 6000 Loss 1.1643\n",
      "Epoch 2 Batch 6100 Loss 0.7677\n",
      "Epoch 2 Batch 6200 Loss 0.8479\n",
      "Epoch 2 Batch 6300 Loss 0.7145\n",
      "Epoch 2 Batch 6400 Loss 0.7310\n",
      "Epoch 2 Batch 6500 Loss 0.9802\n",
      "Epoch 2 Batch 6600 Loss 0.4540\n",
      "Epoch 2 Batch 6700 Loss 0.6531\n",
      "Epoch 2 Batch 6800 Loss 0.4632\n",
      "Epoch 2 Batch 6900 Loss 0.4930\n",
      "Epoch 2 Batch 7000 Loss 1.4509\n",
      "Epoch 2 Batch 7100 Loss 0.6234\n",
      "Epoch 2 Batch 7200 Loss 0.5771\n",
      "Epoch 2 Batch 7300 Loss 0.5376\n",
      "Epoch 2 Batch 7400 Loss 0.3247\n",
      "Epoch 2 Loss 0.7066\n",
      "Time taken for 1 epoch 744.771648645401 sec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 2\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "  start = time.time()\n",
    "\n",
    "  enc_hidden = encoder.initialize_hidden_state()\n",
    "  total_loss = 0\n",
    "\n",
    "  for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
    "    batch_loss = train_step(inp, targ, enc_hidden)\n",
    "    total_loss += batch_loss\n",
    "\n",
    "    if batch % 100 == 0:\n",
    "      print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                                   batch,\n",
    "                                                   batch_loss.numpy()))\n",
    "  # saving (checkpoint) the model every 2 epochs\n",
    "  if (epoch + 1) % 2 == 0:\n",
    "    checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "\n",
    "  print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                      total_loss / steps_per_epoch))\n",
    "  print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27.0"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(876+744)/60"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mU3Ce8M6I3rz"
   },
   "source": [
    "## Translate\n",
    "\n",
    "* The evaluate function is similar to the training loop, except we don't use *teacher forcing* here. The input to the decoder at each time step is its previous predictions along with the hidden state and the encoder output.\n",
    "* Stop predicting when the model predicts the *end token*.\n",
    "* And store the *attention weights for every time step*.\n",
    "\n",
    "Note: The encoder output is calculated only once for one input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "EbQpyYs13jF_"
   },
   "outputs": [],
   "source": [
    "def evaluate(sentence):\n",
    "\n",
    "\n",
    "  sentence = preprocess_sentence(sentence)\n",
    "\n",
    "  inputs = [inp_lang.word_index[i] for i in sentence.split(' ')]\n",
    "  inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n",
    "                                                         maxlen=max_length_inp,\n",
    "                                                         padding='post')\n",
    "  inputs = tf.convert_to_tensor(inputs)\n",
    "\n",
    "  result = ''\n",
    "\n",
    "  hidden = [tf.zeros((1, units))]\n",
    "  enc_out, enc_hidden = encoder(inputs, hidden)\n",
    "\n",
    "  dec_hidden = enc_hidden\n",
    "  dec_input = tf.expand_dims([targ_lang.word_index['<start>']], 0)\n",
    "\n",
    "  for t in range(max_length_targ):\n",
    "    predictions, dec_hidden = decoder(dec_input, dec_hidden, enc_out)\n",
    "\n",
    "\n",
    "    predicted_id = tf.argmax(predictions[0]).numpy()\n",
    "\n",
    "    result += targ_lang.index_word[predicted_id] + ' '\n",
    "\n",
    "    if targ_lang.index_word[predicted_id] == '<end>':\n",
    "      return result, sentence\n",
    "\n",
    "    # the predicted ID is fed back into the model\n",
    "    dec_input = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "  return result, sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "sl9zUHzg3jGI"
   },
   "outputs": [],
   "source": [
    "def translate(sentence):\n",
    "  result, sentence = evaluate(sentence)\n",
    "\n",
    "  print('Input: %s' % (sentence))\n",
    "  print('Predicted translation: {}'.format(result))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n250XbnjOaqP"
   },
   "source": [
    "## Restore the latest checkpoint and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "UJpT9D5_OgP6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x2317a26c488>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# restoring the latest checkpoint in checkpoint_dir\n",
    "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'.': 1,\n",
       " ',': 2,\n",
       " '<start>': 3,\n",
       " '<end>': 4,\n",
       " 'o': 5,\n",
       " 'a': 6,\n",
       " 'de': 7,\n",
       " 'que': 8,\n",
       " 'e': 9,\n",
       " 'n': 10,\n",
       " 's': 11,\n",
       " 'um': 12,\n",
       " 'uma': 13,\n",
       " 'para': 14,\n",
       " 'os': 15,\n",
       " 'se': 16,\n",
       " 'em': 17,\n",
       " 'as': 18,\n",
       " 'do': 19,\n",
       " 'com': 20,\n",
       " 'da': 21,\n",
       " 'm': 22,\n",
       " 'como': 23,\n",
       " 'no': 24,\n",
       " 'mas': 25,\n",
       " 'por': 26,\n",
       " '?': 27,\n",
       " 'na': 28,\n",
       " 'mais': 29,\n",
       " 'eu': 30,\n",
       " 'es': 31,\n",
       " 'est': 32,\n",
       " 'nos': 33,\n",
       " 'muito': 34,\n",
       " 'isso': 35,\n",
       " 'isto': 36,\n",
       " 'ser': 37,\n",
       " 'ou': 38,\n",
       " 'pessoas': 39,\n",
       " 'me': 40,\n",
       " 't': 41,\n",
       " 'dos': 42,\n",
       " 'ao': 43,\n",
       " 'quando': 44,\n",
       " 'fazer': 45,\n",
       " 'h': 46,\n",
       " 'ncia': 47,\n",
       " 'porque': 48,\n",
       " 'foi': 49,\n",
       " 'ent': 50,\n",
       " 'esta': 51,\n",
       " 'vel': 52,\n",
       " 'era': 53,\n",
       " 'das': 54,\n",
       " 'este': 55,\n",
       " 'todos': 56,\n",
       " 'anos': 57,\n",
       " 'temos': 58,\n",
       " 'ele': 59,\n",
       " 'sobre': 60,\n",
       " 'eles': 61,\n",
       " 'c': 62,\n",
       " 'v': 63,\n",
       " 'ver': 64,\n",
       " 'tamb': 65,\n",
       " 'aqui': 66,\n",
       " 'agora': 67,\n",
       " 'mundo': 68,\n",
       " 'tem': 69,\n",
       " 'ter': 70,\n",
       " 'minha': 71,\n",
       " 'mesmo': 72,\n",
       " 'meu': 73,\n",
       " 'podem': 74,\n",
       " 'bem': 75,\n",
       " 'ria': 76,\n",
       " 'come': 77,\n",
       " 'forma': 78,\n",
       " 'podemos': 79,\n",
       " 'mos': 80,\n",
       " 'voc': 81,\n",
       " 'pode': 82,\n",
       " 'coisas': 83,\n",
       " 'vida': 84,\n",
       " 'pr': 85,\n",
       " 'apenas': 86,\n",
       " 'onde': 87,\n",
       " 'rio': 88,\n",
       " 'l': 89,\n",
       " 'vamos': 90,\n",
       " 'depois': 91,\n",
       " 'sua': 92,\n",
       " 'vos': 93,\n",
       " 'assim': 94,\n",
       " 'at': 95,\n",
       " 'p': 96,\n",
       " 'tempo': 97,\n",
       " 'vez': 98,\n",
       " 'risos': 99,\n",
       " 'nossa': 100,\n",
       " 'estava': 101,\n",
       " 'tudo': 102,\n",
       " 'tinha': 103,\n",
       " 'num': 104,\n",
       " 'coisa': 105,\n",
       " 'd': 106,\n",
       " 'j': 107,\n",
       " 'dia': 108,\n",
       " 'aplausos': 109,\n",
       " 'dizer': 110,\n",
       " 'seu': 111,\n",
       " 'numa': 112,\n",
       " 'f': 113,\n",
       " 'hist': 114,\n",
       " 'estamos': 115,\n",
       " 'algo': 116,\n",
       " 'cada': 117,\n",
       " 'nas': 118,\n",
       " 'ela': 119,\n",
       " 'ar': 120,\n",
       " 'veis': 121,\n",
       " 'nosso': 122,\n",
       " 'ainda': 123,\n",
       " 'estas': 124,\n",
       " 'parte': 125,\n",
       " 'hoje': 126,\n",
       " 'tica': 127,\n",
       " 'disse': 128,\n",
       " 'tr': 129,\n",
       " 'amos': 130,\n",
       " 'estes': 131,\n",
       " 'grande': 132,\n",
       " 'todo': 133,\n",
       " 'pelo': 134,\n",
       " 'realmente': 135,\n",
       " 'tipo': 136,\n",
       " 'todas': 137,\n",
       " 'vou': 138,\n",
       " 'verdade': 139,\n",
       " 'falar': 140,\n",
       " 'pela': 141,\n",
       " 'faz': 142,\n",
       " 'portanto': 143,\n",
       " 'qualquer': 144,\n",
       " 'pa': 145,\n",
       " 'vezes': 146,\n",
       " 'problema': 147,\n",
       " 'sem': 148,\n",
       " 'aos': 149,\n",
       " 'ir': 150,\n",
       " 'milh': 151,\n",
       " 'lo': 152,\n",
       " 'entre': 153,\n",
       " 'quero': 154,\n",
       " 'poder': 155,\n",
       " 'vai': 156,\n",
       " 'dois': 157,\n",
       " 'seus': 158,\n",
       " 'sempre': 159,\n",
       " 'pouco': 160,\n",
       " 'tenho': 161,\n",
       " 'melhor': 162,\n",
       " 'trabalho': 163,\n",
       " 'outra': 164,\n",
       " 'exemplo': 165,\n",
       " 'toda': 166,\n",
       " 'primeiro': 167,\n",
       " 'rias': 168,\n",
       " 'outros': 169,\n",
       " 'alguns': 170,\n",
       " 'crian': 171,\n",
       " 'cil': 172,\n",
       " 'menos': 173,\n",
       " 'durante': 174,\n",
       " 'essa': 175,\n",
       " 'muitas': 176,\n",
       " 'nunca': 177,\n",
       " 'ideia': 178,\n",
       " 'rios': 179,\n",
       " 'talvez': 180,\n",
       " 'nossos': 181,\n",
       " 'maior': 182,\n",
       " 'quem': 183,\n",
       " 'pensar': 184,\n",
       " 'ano': 185,\n",
       " 'obrigado': 186,\n",
       " 'muitos': 187,\n",
       " 'mim': 188,\n",
       " 'estou': 189,\n",
       " 'estar': 190,\n",
       " 'neste': 191,\n",
       " 'sistema': 192,\n",
       " 'qual': 193,\n",
       " 'duas': 194,\n",
       " 'nossas': 195,\n",
       " 'esse': 196,\n",
       " 'lado': 197,\n",
       " 'informa': 198,\n",
       " 'outro': 199,\n",
       " 'casa': 200,\n",
       " 'antes': 201,\n",
       " 'outras': 202,\n",
       " 'elas': 203,\n",
       " 'suas': 204,\n",
       " 'diferentes': 205,\n",
       " 'lhe': 206,\n",
       " 'precisamos': 207,\n",
       " 'facto': 208,\n",
       " 'pol': 209,\n",
       " 'sica': 210,\n",
       " 'importante': 211,\n",
       " 'sa': 212,\n",
       " 'algumas': 213,\n",
       " 'cio': 214,\n",
       " 'dar': 215,\n",
       " 'nem': 216,\n",
       " 'sou': 217,\n",
       " 'eram': 218,\n",
       " 'mulheres': 219,\n",
       " 'usar': 220,\n",
       " 'rebro': 221,\n",
       " 'qu': 222,\n",
       " 'criar': 223,\n",
       " 'quest': 224,\n",
       " 'algu': 225,\n",
       " 'tentar': 226,\n",
       " 'meus': 227,\n",
       " 'ncias': 228,\n",
       " 'nova': 229,\n",
       " 'dias': 230,\n",
       " 'claro': 231,\n",
       " 'penso': 232,\n",
       " 'quer': 233,\n",
       " 'primeira': 234,\n",
       " '!': 235,\n",
       " 'pessoa': 236,\n",
       " 'significa': 237,\n",
       " 'enquanto': 238,\n",
       " 'atrav': 239,\n",
       " 'seja': 240,\n",
       " 'momento': 241,\n",
       " 'experi': 242,\n",
       " 'dif': 243,\n",
       " 'sabem': 244,\n",
       " 'foram': 245,\n",
       " 'nica': 246,\n",
       " 'raz': 247,\n",
       " 'dados': 248,\n",
       " 'cerca': 249,\n",
       " 'am': 250,\n",
       " 'volta': 251,\n",
       " 'espa': 252,\n",
       " 'mesma': 253,\n",
       " 'bom': 254,\n",
       " 'diferente': 255,\n",
       " 'tecnologia': 256,\n",
       " 'la': 257,\n",
       " 'somos': 258,\n",
       " 'alguma': 259,\n",
       " 'dentro': 260,\n",
       " 'sim': 261,\n",
       " 'nada': 262,\n",
       " 'for': 263,\n",
       " 'tico': 264,\n",
       " 'r': 265,\n",
       " 'disso': 266,\n",
       " 'quase': 267,\n",
       " 'esp': 268,\n",
       " 'novo': 269,\n",
       " 'seria': 270,\n",
       " 'gua': 271,\n",
       " 'acontece': 272,\n",
       " 'sabemos': 273,\n",
       " 'sei': 274,\n",
       " 'fam': 275,\n",
       " 'mero': 276,\n",
       " 'acho': 277,\n",
       " 'parece': 278,\n",
       " 'quanto': 279,\n",
       " 'certo': 280,\n",
       " 'lia': 281,\n",
       " 'diz': 282,\n",
       " 'mil': 283,\n",
       " 'desta': 284,\n",
       " 'estavam': 285,\n",
       " 'realidade': 286,\n",
       " 'los': 287,\n",
       " 'energia': 288,\n",
       " 'encontrar': 289,\n",
       " 'poss': 290,\n",
       " 'futuro': 291,\n",
       " 'fazem': 292,\n",
       " 'maneira': 293,\n",
       " 'mostrar': 294,\n",
       " 'essas': 295,\n",
       " 'fora': 296,\n",
       " 'mudar': 297,\n",
       " 'trabalhar': 298,\n",
       " 'lugar': 299,\n",
       " 'nesta': 300,\n",
       " 'palavras': 301,\n",
       " 'aquilo': 302,\n",
       " 'simples': 303,\n",
       " 'acontecer': 304,\n",
       " 'ci': 305,\n",
       " 'terra': 306,\n",
       " 'dinheiro': 307,\n",
       " 'sido': 308,\n",
       " 'ses': 309,\n",
       " 'altura': 310,\n",
       " 'doen': 311,\n",
       " 'cie': 312,\n",
       " 'tal': 313,\n",
       " 'constru': 314,\n",
       " 'feito': 315,\n",
       " 'desde': 316,\n",
       " 'modo': 317,\n",
       " 'lhes': 318,\n",
       " 'rela': 319,\n",
       " 'havia': 320,\n",
       " 'resposta': 321,\n",
       " 'mudan': 322,\n",
       " 'escola': 323,\n",
       " 'bastante': 324,\n",
       " 'existem': 325,\n",
       " 'meio': 326,\n",
       " 'imagem': 327,\n",
       " 'vossa': 328,\n",
       " 'tinham': 329,\n",
       " 'existe': 330,\n",
       " 'ponto': 331,\n",
       " 'ali': 332,\n",
       " 'posso': 333,\n",
       " 'pequeno': 334,\n",
       " 'ca': 335,\n",
       " 'cidade': 336,\n",
       " 'deste': 337,\n",
       " 'maioria': 338,\n",
       " 'ningu': 339,\n",
       " 'lares': 340,\n",
       " 'passado': 341,\n",
       " 'luz': 342,\n",
       " 'vemos': 343,\n",
       " 'simplesmente': 344,\n",
       " 'uns': 345,\n",
       " 'deles': 346,\n",
       " 'queremos': 347,\n",
       " 'mal': 348,\n",
       " 'fico': 349,\n",
       " 'internet': 350,\n",
       " 'dio': 351,\n",
       " 'quatro': 352,\n",
       " 'construir': 353,\n",
       " 'ajudar': 354,\n",
       " 'ticas': 355,\n",
       " 'homem': 356,\n",
       " 'fui': 357,\n",
       " 'fazemos': 358,\n",
       " 'nico': 359,\n",
       " 'contar': 360,\n",
       " 'al': 361,\n",
       " 'cios': 362,\n",
       " 'obrigada': 363,\n",
       " 'fez': 364,\n",
       " 'ticos': 365,\n",
       " 'funciona': 366,\n",
       " 'aconteceu': 367,\n",
       " 'ssemos': 368,\n",
       " 'problemas': 369,\n",
       " 'fa': 370,\n",
       " 'interessante': 371,\n",
       " 'amigos': 372,\n",
       " 'cinco': 373,\n",
       " 'queria': 374,\n",
       " 'humano': 375,\n",
       " 'acerca': 376,\n",
       " 'vidas': 377,\n",
       " 'tornar': 378,\n",
       " 'humanos': 379,\n",
       " 'frente': 380,\n",
       " 'culo': 381,\n",
       " 'caso': 382,\n",
       " 'si': 383,\n",
       " 'sala': 384,\n",
       " 'esses': 385,\n",
       " 'vosso': 386,\n",
       " 'sociedade': 387,\n",
       " 'corpo': 388,\n",
       " 'contra': 389,\n",
       " 'quais': 390,\n",
       " 'saber': 391,\n",
       " 'podia': 392,\n",
       " 'segundo': 393,\n",
       " 'estado': 394,\n",
       " 'pequena': 395,\n",
       " 'olhar': 396,\n",
       " 'completamente': 397,\n",
       " 'grandes': 398,\n",
       " 'ok': 399,\n",
       " 'las': 400,\n",
       " 'fim': 401,\n",
       " 'homens': 402,\n",
       " 'aprender': 403,\n",
       " 'longo': 404,\n",
       " 'gente': 405,\n",
       " 'provavelmente': 406,\n",
       " 'novas': 407,\n",
       " 'aquele': 408,\n",
       " 'viver': 409,\n",
       " 'caminho': 410,\n",
       " 'deo': 411,\n",
       " 'poderia': 412,\n",
       " 'guerra': 413,\n",
       " 'g': 414,\n",
       " 'not': 415,\n",
       " 'tanto': 416,\n",
       " 'in': 417,\n",
       " 'computador': 418,\n",
       " 'real': 419,\n",
       " 'processo': 420,\n",
       " 'conseguimos': 421,\n",
       " 'boa': 422,\n",
       " 'milhares': 423,\n",
       " 'comunidade': 424,\n",
       " 'querem': 425,\n",
       " 'conseguem': 426,\n",
       " 'empresas': 427,\n",
       " 'chamado': 428,\n",
       " 'stico': 429,\n",
       " 'carro': 430,\n",
       " 'pergunta': 431,\n",
       " 'perceber': 432,\n",
       " 'pai': 433,\n",
       " 'ficar': 434,\n",
       " 'nio': 435,\n",
       " 'matem': 436,\n",
       " 'meses': 437,\n",
       " 'aquela': 438,\n",
       " 'comecei': 439,\n",
       " 'estados': 440,\n",
       " 'arte': 441,\n",
       " 'partir': 442,\n",
       " 'lulas': 443,\n",
       " 'frica': 444,\n",
       " 'melhores': 445,\n",
       " 'causa': 446,\n",
       " 'medo': 447,\n",
       " 'cia': 448,\n",
       " 'algum': 449,\n",
       " 'sentido': 450,\n",
       " 'muita': 451,\n",
       " 'passar': 452,\n",
       " 'preciso': 453,\n",
       " 'rea': 454,\n",
       " 'local': 455,\n",
       " 'chegar': 456,\n",
       " 'fiz': 457,\n",
       " 'imaginem': 458,\n",
       " 'fosse': 459,\n",
       " 'gr': 460,\n",
       " 'fica': 461,\n",
       " 'projeto': 462,\n",
       " 'mulher': 463,\n",
       " 'minhas': 464,\n",
       " 'b': 465,\n",
       " 'incr': 466,\n",
       " 'grupo': 467,\n",
       " 'pelos': 468,\n",
       " 'tarde': 469,\n",
       " 'termos': 470,\n",
       " 'econ': 471,\n",
       " 'stica': 472,\n",
       " 'noite': 473,\n",
       " 'chama': 474,\n",
       " 'carros': 475,\n",
       " 'formas': 476,\n",
       " 'tamanho': 477,\n",
       " 'metros': 478,\n",
       " 'imagens': 479,\n",
       " 'livro': 480,\n",
       " 'economia': 481,\n",
       " 'aqueles': 482,\n",
       " 'jovens': 483,\n",
       " 'atr': 484,\n",
       " 'global': 485,\n",
       " 'eua': 486,\n",
       " 'filhos': 487,\n",
       " 'diferen': 488,\n",
       " 'fizemos': 489,\n",
       " 'vis': 490,\n",
       " 'edif': 491,\n",
       " 'modelo': 492,\n",
       " 'olhos': 493,\n",
       " 'ind': 494,\n",
       " 'solu': 495,\n",
       " 'sabia': 496,\n",
       " 'rede': 497,\n",
       " 'enorme': 498,\n",
       " 'palavra': 499,\n",
       " 'padr': 500,\n",
       " 'ideias': 501,\n",
       " 'tipos': 502,\n",
       " 'viol': 503,\n",
       " 'liga': 504,\n",
       " 'pais': 505,\n",
       " 'desenvolvimento': 506,\n",
       " 'governo': 507,\n",
       " 'digo': 508,\n",
       " 'dele': 509,\n",
       " 'rica': 510,\n",
       " 'destas': 511,\n",
       " 'tive': 512,\n",
       " 'cidades': 513,\n",
       " 'horas': 514,\n",
       " 'certa': 515,\n",
       " 'juntos': 516,\n",
       " 'mica': 517,\n",
       " 'sabe': 518,\n",
       " 'te': 519,\n",
       " 'chamada': 520,\n",
       " 'descobrir': 521,\n",
       " 'nhamos': 522,\n",
       " 'cabe': 523,\n",
       " 'logo': 524,\n",
       " 'social': 525,\n",
       " 'seguran': 526,\n",
       " 'popula': 527,\n",
       " 'comida': 528,\n",
       " 'situa': 529,\n",
       " 'neg': 530,\n",
       " 'maiores': 531,\n",
       " 'teve': 532,\n",
       " 'nome': 533,\n",
       " 'natureza': 534,\n",
       " 'universo': 535,\n",
       " 'humana': 536,\n",
       " 'avan': 537,\n",
       " 'prio': 538,\n",
       " 'fant': 539,\n",
       " 'educa': 540,\n",
       " 'sociais': 541,\n",
       " 'adn': 542,\n",
       " 'planeta': 543,\n",
       " 'voltar': 544,\n",
       " 'porqu': 545,\n",
       " 'base': 546,\n",
       " 'seguir': 547,\n",
       " 'nia': 548,\n",
       " 'mico': 549,\n",
       " 'destes': 550,\n",
       " 'ora': 551,\n",
       " 'ambiente': 552,\n",
       " 'tornou': 553,\n",
       " 'dizem': 554,\n",
       " 'dela': 555,\n",
       " 'ajuda': 556,\n",
       " 'segunda': 557,\n",
       " 'deixar': 558,\n",
       " 'pelas': 559,\n",
       " 'conjunto': 560,\n",
       " 'pria': 561,\n",
       " 'pe': 562,\n",
       " 'deixem': 563,\n",
       " 'torna': 564,\n",
       " 'levar': 565,\n",
       " 'houve': 566,\n",
       " 'serem': 567,\n",
       " 'compreender': 568,\n",
       " 'minutos': 569,\n",
       " 'som': 570,\n",
       " 'unidos': 571,\n",
       " 'pensem': 572,\n",
       " 'irm': 573,\n",
       " 'seguinte': 574,\n",
       " 'papel': 575,\n",
       " 'mundial': 576,\n",
       " 'conseguir': 577,\n",
       " 'pensei': 578,\n",
       " 'ouvir': 579,\n",
       " 'voz': 580,\n",
       " 'vem': 581,\n",
       " 'cr': 582,\n",
       " 'professores': 583,\n",
       " 'basicamente': 584,\n",
       " 'cancro': 585,\n",
       " 'ximo': 586,\n",
       " 'animais': 587,\n",
       " 'baixo': 588,\n",
       " 'natural': 589,\n",
       " 'oportunidade': 590,\n",
       " 'ltimos': 591,\n",
       " 'sucesso': 592,\n",
       " 'an': 593,\n",
       " 'cias': 594,\n",
       " 'capacidade': 595,\n",
       " 'meros': 596,\n",
       " 'necess': 597,\n",
       " 'locais': 598,\n",
       " 'medida': 599,\n",
       " 'delas': 600,\n",
       " 'final': 601,\n",
       " 'usando': 602,\n",
       " 'seis': 603,\n",
       " 'empresa': 604,\n",
       " 'vi': 605,\n",
       " 'certeza': 606,\n",
       " 'valor': 607,\n",
       " 'nenhum': 608,\n",
       " 'culas': 609,\n",
       " 'conhecimento': 610,\n",
       " 'escala': 611,\n",
       " 'ap': 612,\n",
       " 'idade': 613,\n",
       " 'ia': 614,\n",
       " 'ferramentas': 615,\n",
       " 'semana': 616,\n",
       " 'ted': 617,\n",
       " 'cient': 618,\n",
       " 'campo': 619,\n",
       " 'tecnologias': 620,\n",
       " 'laborat': 621,\n",
       " 'demasiado': 622,\n",
       " 'decis': 623,\n",
       " 'gica': 624,\n",
       " 'gera': 625,\n",
       " 'lan': 626,\n",
       " 'deus': 627,\n",
       " 'sejam': 628,\n",
       " 'sistemas': 629,\n",
       " 'nios': 630,\n",
       " 'cria': 631,\n",
       " 'entanto': 632,\n",
       " 'cima': 633,\n",
       " 'confian': 634,\n",
       " 'importantes': 635,\n",
       " 'blico': 636,\n",
       " 'livros': 637,\n",
       " 'haver': 638,\n",
       " 'escrever': 639,\n",
       " 'cultura': 640,\n",
       " 'disto': 641,\n",
       " 'pensamos': 642,\n",
       " 'pequenos': 643,\n",
       " 'perto': 644,\n",
       " 'devido': 645,\n",
       " 'partilhar': 646,\n",
       " 'finalmente': 647,\n",
       " 'hora': 648,\n",
       " 'sentir': 649,\n",
       " 'iria': 650,\n",
       " 'conta': 651,\n",
       " 'comum': 652,\n",
       " 'mi': 653,\n",
       " 'institui': 654,\n",
       " 'sob': 655,\n",
       " 'dessa': 656,\n",
       " 'trata': 657,\n",
       " 'aplica': 658,\n",
       " 'mente': 659,\n",
       " 'linha': 660,\n",
       " 'dico': 661,\n",
       " 'manter': 662,\n",
       " 'mar': 663,\n",
       " 'ndia': 664,\n",
       " 'reas': 665,\n",
       " 'passa': 666,\n",
       " 'pensam': 667,\n",
       " 'filho': 668,\n",
       " 'vejam': 669,\n",
       " 'dan': 670,\n",
       " 'di': 671,\n",
       " 'longe': 672,\n",
       " 'rob': 673,\n",
       " 'ltima': 674,\n",
       " 'universidade': 675,\n",
       " 'parar': 676,\n",
       " 'semanas': 677,\n",
       " 'nenhuma': 678,\n",
       " 'gosto': 679,\n",
       " 'movimento': 680,\n",
       " 'fizeram': 681,\n",
       " 'centenas': 682,\n",
       " 'stria': 683,\n",
       " 'europa': 684,\n",
       " 'cento': 685,\n",
       " 'aram': 686,\n",
       " 'china': 687,\n",
       " 'pontos': 688,\n",
       " 'pessoal': 689,\n",
       " 'colocar': 690,\n",
       " 'resultado': 691,\n",
       " 'viagem': 692,\n",
       " 'estrutura': 693,\n",
       " 'acesso': 694,\n",
       " 'estudantes': 695,\n",
       " 'gostaria': 696,\n",
       " 'capaz': 697,\n",
       " 'vossos': 698,\n",
       " 'sair': 699,\n",
       " 'imaginar': 700,\n",
       " 'fundo': 701,\n",
       " 'prote': 702,\n",
       " 'carbono': 703,\n",
       " 'controlo': 704,\n",
       " 'redes': 705,\n",
       " 'crescimento': 706,\n",
       " 'norte': 707,\n",
       " 'disseram': 708,\n",
       " 'servi': 709,\n",
       " 'equipa': 710,\n",
       " 'perguntar': 711,\n",
       " 'tivemos': 712,\n",
       " 'descobrimos': 713,\n",
       " 'mo': 714,\n",
       " 'quina': 715,\n",
       " 'prios': 716,\n",
       " 'organiza': 717,\n",
       " 'rie': 718,\n",
       " 'acredito': 719,\n",
       " 'per': 720,\n",
       " 'amor': 721,\n",
       " 'tio': 722,\n",
       " 'u': 723,\n",
       " 'capazes': 724,\n",
       " 'totalmente': 725,\n",
       " 'vista': 726,\n",
       " 'umas': 727,\n",
       " 'neur': 728,\n",
       " 'novos': 729,\n",
       " 'suficiente': 730,\n",
       " 'design': 731,\n",
       " 'estudo': 732,\n",
       " 'aprendizagem': 733,\n",
       " 'ltimo': 734,\n",
       " 'apesar': 735,\n",
       " 'deve': 736,\n",
       " 'el': 737,\n",
       " 'partes': 738,\n",
       " 'amigo': 739,\n",
       " 'resultados': 740,\n",
       " 'sico': 741,\n",
       " 'continuar': 742,\n",
       " 'quantidade': 743,\n",
       " 'precisam': 744,\n",
       " 'aten': 745,\n",
       " 'acabar': 746,\n",
       " 'pois': 747,\n",
       " 'pesquisa': 748,\n",
       " 'cora': 749,\n",
       " 'leo': 750,\n",
       " 'impacto': 751,\n",
       " 'mesmos': 752,\n",
       " 'sol': 753,\n",
       " 'passo': 754,\n",
       " 'rapidamente': 755,\n",
       " 'teoria': 756,\n",
       " 'nisso': 757,\n",
       " 'normal': 758,\n",
       " 'av': 759,\n",
       " 'resolver': 760,\n",
       " 'morte': 761,\n",
       " 'desafio': 762,\n",
       " 'obter': 763,\n",
       " 'mensagem': 764,\n",
       " 'pio': 765,\n",
       " 'is': 766,\n",
       " 'precisa': 767,\n",
       " 'cor': 768,\n",
       " 'risco': 769,\n",
       " 'seres': 770,\n",
       " 'sete': 771,\n",
       " 'centro': 772,\n",
       " 'alunos': 773,\n",
       " 'especial': 774,\n",
       " 'direito': 775,\n",
       " 'programa': 776,\n",
       " 'perguntas': 777,\n",
       " 'rua': 778,\n",
       " 'foto': 779,\n",
       " 'li': 780,\n",
       " 'direitos': 781,\n",
       " 'culos': 782,\n",
       " 'computadores': 783,\n",
       " 'ler': 784,\n",
       " 'google': 785,\n",
       " 'permite': 786,\n",
       " 'passe': 787,\n",
       " 'colegas': 788,\n",
       " 'mostra': 789,\n",
       " 'esperan': 790,\n",
       " 'aumentar': 791,\n",
       " 'fiquei': 792,\n",
       " 'fotografia': 793,\n",
       " 'sul': 794,\n",
       " 'entrar': 795,\n",
       " 'comigo': 796,\n",
       " 'evolu': 797,\n",
       " 'revolu': 798,\n",
       " 'mudou': 799,\n",
       " 'devemos': 800,\n",
       " 'embora': 801,\n",
       " 'procura': 802,\n",
       " 'nisto': 803,\n",
       " 'cl': 804,\n",
       " 'nicos': 805,\n",
       " 'escolas': 806,\n",
       " 'geral': 807,\n",
       " 'extraordin': 808,\n",
       " 'xima': 809,\n",
       " 'cientistas': 810,\n",
       " 'jovem': 811,\n",
       " 'telem': 812,\n",
       " 'consigo': 813,\n",
       " 'mercado': 814,\n",
       " 'dist': 815,\n",
       " 'leva': 816,\n",
       " 'cies': 817,\n",
       " 'lugares': 818,\n",
       " 'boas': 819,\n",
       " 'beb': 820,\n",
       " 'alcan': 821,\n",
       " 'indiv': 822,\n",
       " 'online': 823,\n",
       " 'comunica': 824,\n",
       " 'materiais': 825,\n",
       " 'ensinar': 826,\n",
       " 'material': 827,\n",
       " 'devem': 828,\n",
       " 'uso': 829,\n",
       " 'novamente': 830,\n",
       " 'pequenas': 831,\n",
       " 'pobres': 832,\n",
       " 'normalmente': 833,\n",
       " 'resto': 834,\n",
       " 'sangue': 835,\n",
       " 'nessa': 836,\n",
       " 'mem': 837,\n",
       " 'grupos': 838,\n",
       " 'consegue': 839,\n",
       " 'humanidade': 840,\n",
       " 'dicos': 841,\n",
       " 'hip': 842,\n",
       " 'funcionar': 843,\n",
       " 'dessas': 844,\n",
       " 'possam': 845,\n",
       " 'especialmente': 846,\n",
       " 'comportamento': 847,\n",
       " 'amea': 848,\n",
       " 'visto': 849,\n",
       " 'ado': 850,\n",
       " 'lidar': 851,\n",
       " 'fun': 852,\n",
       " 'regras': 853,\n",
       " 'recursos': 854,\n",
       " 'andar': 855,\n",
       " 'teria': 856,\n",
       " 'naquele': 857,\n",
       " 'ficam': 858,\n",
       " 'particular': 859,\n",
       " 'espero': 860,\n",
       " 'posi': 861,\n",
       " 'lias': 862,\n",
       " 'nesse': 863,\n",
       " 'alta': 864,\n",
       " 'metade': 865,\n",
       " 'crescer': 866,\n",
       " 'tirar': 867,\n",
       " 'princ': 868,\n",
       " 'chamamos': 869,\n",
       " 'condi': 870,\n",
       " 'tu': 871,\n",
       " 'petr': 872,\n",
       " 'tenha': 873,\n",
       " 'sexo': 874,\n",
       " 'exatamente': 875,\n",
       " 'iorque': 876,\n",
       " 'peda': 877,\n",
       " 'aula': 878,\n",
       " 'religi': 879,\n",
       " 'ch': 880,\n",
       " 'rus': 881,\n",
       " 'vir': 882,\n",
       " 'nuclear': 883,\n",
       " 'aprendi': 884,\n",
       " 'chave': 885,\n",
       " 'ngua': 886,\n",
       " 'podiam': 887,\n",
       " 'cidad': 888,\n",
       " 'via': 889,\n",
       " 'custo': 890,\n",
       " 'inova': 891,\n",
       " 'pegar': 892,\n",
       " 'superf': 893,\n",
       " 'mental': 894,\n",
       " 'terem': 895,\n",
       " 'tenham': 896,\n",
       " 'fomos': 897,\n",
       " 'sequer': 898,\n",
       " 'esquerda': 899,\n",
       " 'fazia': 900,\n",
       " 'usam': 901,\n",
       " 'plano': 902,\n",
       " 'vejo': 903,\n",
       " 'us': 904,\n",
       " 'cara': 905,\n",
       " 'deres': 906,\n",
       " 'espera': 907,\n",
       " 'fotos': 908,\n",
       " 'sociedades': 909,\n",
       " 'mapa': 910,\n",
       " 'contr': 911,\n",
       " 'conversa': 912,\n",
       " 'altera': 913,\n",
       " 'professor': 914,\n",
       " 'pre': 915,\n",
       " 'pior': 916,\n",
       " 'ficou': 917,\n",
       " 'tomar': 918,\n",
       " 'consci': 919,\n",
       " 'naquela': 920,\n",
       " 'procurar': 921,\n",
       " 'efeito': 922,\n",
       " 'digital': 923,\n",
       " 'primeiros': 924,\n",
       " 'lula': 925,\n",
       " 'possamos': 926,\n",
       " 'sensa': 927,\n",
       " 'conseguia': 928,\n",
       " 'presente': 929,\n",
       " 'pedir': 930,\n",
       " 'tivesse': 931,\n",
       " 'exactamente': 932,\n",
       " 'mol': 933,\n",
       " 'linguagem': 934,\n",
       " 'recentemente': 935,\n",
       " 'explicar': 936,\n",
       " 'repente': 937,\n",
       " 'sticas': 938,\n",
       " 'liberdade': 939,\n",
       " 'possa': 940,\n",
       " 'precis': 941,\n",
       " 'desenvolver': 942,\n",
       " 'correr': 943,\n",
       " 'produtos': 944,\n",
       " 'clim': 945,\n",
       " 'bons': 946,\n",
       " 'palestra': 947,\n",
       " 'ambos': 948,\n",
       " 'forte': 949,\n",
       " 'errado': 950,\n",
       " 'ensino': 951,\n",
       " 'produzir': 952,\n",
       " 'levou': 953,\n",
       " 'topo': 954,\n",
       " 'prias': 955,\n",
       " 'oh': 956,\n",
       " 'dispon': 957,\n",
       " 'part': 958,\n",
       " 'principal': 959,\n",
       " 'gico': 960,\n",
       " 'km': 961,\n",
       " 'projecto': 962,\n",
       " 'sorte': 963,\n",
       " 'timas': 964,\n",
       " 'ingl': 965,\n",
       " 'micos': 966,\n",
       " 'gen': 967,\n",
       " 'teste': 968,\n",
       " 'genes': 969,\n",
       " 'veem': 970,\n",
       " 'adoro': 971,\n",
       " 'qualidade': 972,\n",
       " 'velocidade': 973,\n",
       " 'media': 974,\n",
       " 'dor': 975,\n",
       " 'tua': 976,\n",
       " 'prazo': 977,\n",
       " 'animal': 978,\n",
       " 'melhorar': 979,\n",
       " 'usamos': 980,\n",
       " 'caixa': 981,\n",
       " 'massa': 982,\n",
       " 'desse': 983,\n",
       " 'comprar': 984,\n",
       " 'pobreza': 985,\n",
       " 'metro': 986,\n",
       " 'custos': 987,\n",
       " 'respostas': 988,\n",
       " 'ex': 989,\n",
       " 'blica': 990,\n",
       " 'aquelas': 991,\n",
       " 'mau': 992,\n",
       " 'vivemos': 993,\n",
       " 'emprego': 994,\n",
       " 'i': 995,\n",
       " 'oito': 996,\n",
       " 'regi': 997,\n",
       " 'odo': 998,\n",
       " 'can': 999,\n",
       " 'mec': 1000,\n",
       " ...}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp_lang.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "WrAM0FDomq3E",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: <start> problema que temos que resolver . <end>\n",
      "Predicted translation: and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and \n"
     ]
    }
   ],
   "source": [
    "translate(\"problema que temos que resolver.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: <start> este um problema que temos que resolver . <end>\n",
      "Predicted translation: and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and \n"
     ]
    }
   ],
   "source": [
    "translate(\"este é um problema que temos que resolver.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "zSx2iM36EZQZ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: <start> os meus vizinhos ouviram sobre esta ideia . <end>\n",
      "Predicted translation: and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and \n"
     ]
    }
   ],
   "source": [
    "translate(\"os meus vizinhos ouviram sobre esta ideia.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "A3LLCx3ZE0Ls"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: <start> os meus alunos t m problemas <end>\n",
      "Predicted translation: and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and \n"
     ]
    }
   ],
   "source": [
    "translate(\"os meus alunos t m problemas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RTe5P5ioMJwN"
   },
   "source": [
    "## Next steps\n",
    "\n",
    "* [Download a different dataset](http://www.manythings.org/anki/) to experiment with translations, for example, English to German, or English to French.\n",
    "* Experiment with training on a larger dataset, or using more epochs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp_lang_val, targ_lang_val = create_dataset(val_examples,None,None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> estava sempre preocupado em ser apanhado e enviado de volta . <end>'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp_lang_val[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> i was always worried about being caught and sent back . <end>'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targ_lang_val[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: <start> tinham comido peixe com batatas fritas ? <end>\n",
      "Predicted Translation: and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and \n",
      "Real translation: did they eat fish and chips ?\n",
      "BLEU Score:  0.0072992700729927\n",
      "--------------------------------------------------------------------------------\n",
      "Input: <start> estava sempre preocupado em ser apanhado e enviado de volta . <end>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\Anaconda3\\envs\\tf-gpu-env\\lib\\site-packages\\nltk\\translate\\bleu_score.py:516: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "C:\\Users\\user\\Anaconda3\\envs\\tf-gpu-env\\lib\\site-packages\\nltk\\translate\\bleu_score.py:516: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "C:\\Users\\user\\Anaconda3\\envs\\tf-gpu-env\\lib\\site-packages\\nltk\\translate\\bleu_score.py:516: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Translation: and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and \n",
      "Real translation: i was always worried about being caught and sent back .\n",
      "BLEU Score:  0.0072992700729927\n",
      "--------------------------------------------------------------------------------\n",
      "Input: <start> e , na minha opini o , uma cura n o s para mim , mas para toda a gente . <end>\n",
      "Predicted Translation: and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and \n",
      "Real translation: and from what i feel , it s a cure for me , but for us all .\n",
      "BLEU Score:  0.021897810218978107\n",
      "--------------------------------------------------------------------------------\n",
      "Input: <start> quando n o conseguia adormecer devido ao intenso frio , ou s dores da fome , eu esperava que , na manh seguinte , a minha irm voltasse para me acordar com a minha comida preferida . <end>\n",
      "Predicted Translation: and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and \n",
      "Real translation: when i could not fall asleep from bitter cold or hunger pains , i hoped that , the next morning , my sister would come back to wake me up with my favorite food .\n",
      "BLEU Score:  0.029197080291970802\n",
      "--------------------------------------------------------------------------------\n",
      "Input: <start> a voz delas torna se a minha voz e a minha identidade . <end>\n",
      "Predicted Translation: and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and \n",
      "Real translation: and their voice becomes my voice and identity .\n",
      "BLEU Score:  0.014598540145985401\n",
      "--------------------------------------------------------------------------------\n",
      "Input: <start> se pensarmos mais profundamente em coisas como estas , a resposta bvia das pessoas deve ser aquele , ok , isso soa mal , mas n o me afeta realmente porque sou um cidad o legal . <end>\n",
      "Predicted Translation: and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and \n",
      "Real translation: now when we think deeper about things like these , the obvious response from people should be that , okay , that sounds bad , but that does n t really affect me because i m a legal citizen .\n",
      "BLEU Score:  0.0364963503649635\n",
      "--------------------------------------------------------------------------------\n",
      "Input: <start> um trabalho em desenvolvimento de uma hist ria pessoal para uma hist ria global . <end>\n",
      "Predicted Translation: and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and \n",
      "Real translation: it s a work in progress from a personal story to a global history .\n",
      "BLEU Score:  0\n",
      "--------------------------------------------------------------------------------\n",
      "Input: <start> foi incr vel que as pessoas tenham come ado a partilhar os seus pensamentos sobre o trabalho . <end>\n",
      "Predicted Translation: and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and \n",
      "Real translation: it was amazing that people started to share their thoughts about the work with me .\n",
      "BLEU Score:  0.0072992700729927\n",
      "--------------------------------------------------------------------------------\n",
      "Input: <start> at na coreia do norte eu era um estudante med ocre . <end>\n",
      "Predicted Translation: and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and \n",
      "Real translation: even in north korea , i was an f student .\n",
      "BLEU Score:  0.0072992700729927\n",
      "--------------------------------------------------------------------------------\n",
      "Input: <start> chamamos a isto a miss o zero . <end>\n",
      "Predicted Translation: and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and \n",
      "Real translation: we call this mission zero .\n",
      "BLEU Score:  0\n",
      "--------------------------------------------------------------------------------\n",
      "Input: <start> sou artista , e informa o p blica no meu trabalho . <end>\n",
      "Predicted Translation: and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and \n",
      "Real translation: i m an artist , i use several forms of open source technologies and open information in my practice .\n",
      "BLEU Score:  0.014598540145985401\n",
      "--------------------------------------------------------------------------------\n",
      "Input: <start> sem piada nenhuma . <end>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Translation: and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and \n",
      "Real translation: and without the humor .\n",
      "BLEU Score:  0.014598540145985401\n",
      "--------------------------------------------------------------------------------\n",
      "Input: <start> e estas s o as quest es com que temos de nos preocupar durante os pr ximos anos . <end>\n",
      "Predicted Translation: and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and \n",
      "Real translation: and these are the questions that we have to worry about for the next years .\n",
      "BLEU Score:  0.021897810218978107\n",
      "--------------------------------------------------------------------------------\n",
      "Input: <start> se forem um f sico de part culas , v o querer saber o que acontece quando fazemos chocar coisas umas contra as outras . <end>\n",
      "Predicted Translation: and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and \n",
      "Real translation: if you re a particle physicist , you want to know what happens when we smash things together .\n",
      "BLEU Score:  0.0072992700729927\n",
      "--------------------------------------------------------------------------------\n",
      "Input: <start> no futuro , a maioria do crime ir acontecer online . <end>\n",
      "Predicted Translation: and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and the world , and \n",
      "Real translation: in the future , the majority of crime will be happening online .\n",
      "BLEU Score:  0.021897810218978107\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "bleuplus = 0\n",
    "bleu_ = 0\n",
    "\n",
    "for i in [0,1,5,70,80,101,6,99,87,999,105,33,45,89,96]: #0,1,5\n",
    "    pt_test = inp_lang_val[i]\n",
    "    print('Input: {}'.format(pt_test))\n",
    "#     pt_test = pt_.numpy().decode(\"utf-8\", \"ignore\")\n",
    "    result, sentence = evaluate(pt_test)\n",
    "# #     predicted = translate(pt_test)\n",
    "    print('Predicted Translation: {}'.format(result))\n",
    "    en_test = targ_lang_val[i].split(' ', 1)[1].rsplit(' ', 1)[0]\n",
    "    print('Real translation: {}'.format(en_test))\n",
    "    bleu_ = bleu.sentence_bleu([en_test.split()], result.split(), weights=(1,0,0,0))\n",
    "    print(\"BLEU Score: \", bleu_)\n",
    "    print(\"----\" *20)\n",
    "\n",
    "    bleuplus += bleu_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.014111922141119223"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bleuplus/15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "nmt_with_attention.ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
