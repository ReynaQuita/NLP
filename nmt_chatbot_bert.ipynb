{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "import string\n",
    "from string import digits\n",
    "import re\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import collections\n",
    "import unicodedata\n",
    "\n",
    "import os\n",
    "import io\n",
    "\n",
    "\n",
    "from bert import BertModelLayer\n",
    "from bert.loader import StockBertConfig, load_stock_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\Project From Alex NLP\\chi-eng\\cmn.txt\n"
     ]
    }
   ],
   "source": [
    "path_to_zip = 'C:\\\\Users\\\\user\\\\Project From Alex NLP\\\\chi-eng.zip'\n",
    "path_to_file = os.path.dirname(path_to_zip)+\"\\\\chi-eng\\\\cmn.txt\"\n",
    "print(path_to_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentence(sentence):\n",
    "  sentence = sentence.lower().strip()\n",
    "  # creating a space between a word and the punctuation following it\n",
    "  # eg: \"he is a boy.\" => \"he is a boy .\"\n",
    "  sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)\n",
    "  sentence = re.sub(r'[\" \"]+', \" \", sentence)\n",
    "  # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n",
    "  sentence = re.sub(r\"[^a-zA-Z?.!,]+\", \" \", sentence)\n",
    "  sentence = sentence.strip()\n",
    "  # adding a start and an end token to the sentence\n",
    "  # so that the model know when to start and stop predicting.\n",
    "#   sentence = '<start> ' + sentence + ' <end>'\n",
    "  return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(path, num1,num2):\n",
    "  english = []\n",
    "  chinese = []\n",
    "  lines = io.open(path, encoding='UTF-8').read().strip().split('\\n')\n",
    "\n",
    "#   word_pairs = [[preprocess_sentence(w) for w in l.split('\\t')]  for l in lines[:num_examples]]\n",
    "  for l in lines[num1:num2]:\n",
    "      eng = preprocess_sentence(l.split('\\t')[0])\n",
    "      chi = l.split('\\t')[1]\n",
    "      english.append(eng)\n",
    "      chinese.append(chi)\n",
    "        \n",
    "  return english, chinese"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one man s meat is another man s poison .\n",
      "甲之蜜糖，乙之砒霜。\n"
     ]
    }
   ],
   "source": [
    "en_tr, chi_tr = create_dataset(path_to_file, 0,20000)\n",
    "print(en_tr[-1])\n",
    "print(chi_tr[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_zip = tf.keras.utils.get_file(\n",
    "    'cornell_movie_dialogs.zip',\n",
    "    origin=\n",
    "    'http://www.cs.cornell.edu/~cristian/data/cornell_movie_dialogs_corpus.zip',\n",
    "    extract=True)\n",
    "\n",
    "path_to_dataset = os.path.join(\n",
    "    os.path.dirname(path_to_zip), \"cornell movie-dialogs corpus\")\n",
    "\n",
    "path_to_movie_lines = os.path.join(path_to_dataset, 'movie_lines.txt')\n",
    "path_to_movie_conversations = os.path.join(path_to_dataset,\n",
    "                                           'movie_conversations.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_conversations(num1,num2):\n",
    "  # dictionary of line id to text\n",
    "  id2line = {}\n",
    "  with open(path_to_movie_lines, errors='ignore') as file:\n",
    "    lines = file.readlines()\n",
    "  for line in lines:\n",
    "    parts = line.replace('\\n', '').split(' +++$+++ ')\n",
    "    id2line[parts[0]] = parts[4]\n",
    "\n",
    "  inputs, outputs = [], []\n",
    "  with open(path_to_movie_conversations, 'r') as file:\n",
    "    lines = file.readlines()\n",
    "  for line in lines[num1:num2]:\n",
    "    parts = line.replace('\\n', '').split(' +++$+++ ')\n",
    "    # get conversation in a list of line ID\n",
    "    conversation = [line[1:-1] for line in parts[3][1:-1].split(', ')]\n",
    "    for i in range(len(conversation) - 1):\n",
    "      inputs.append(preprocess_sentence(id2line[conversation[i]]))\n",
    "      outputs.append(preprocess_sentence(id2line[conversation[i + 1]]))\n",
    "#       if len(inputs) >= MAX_SAMPLES:\n",
    "#         return inputs, outputs\n",
    "  return inputs, outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(53430, 53430)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions, answers = load_conversations(0,20000)\n",
    "len(questions),len(answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_unicode(text):\n",
    "    \"\"\"Converts `text` to Unicode (if it's not already), assuming utf-8 input.\"\"\"\n",
    "    if isinstance(text, str):\n",
    "        return text\n",
    "    elif isinstance(text, bytes):\n",
    "        return text.decode(\"utf-8\", \"ignore\")\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported string type: %s\" % (type(text)))\n",
    "\n",
    "\n",
    "def load_vocab(vocab_file):\n",
    "    \"\"\"Loads a vocabulary file into a dictionary.\"\"\"\n",
    "    vocab = collections.OrderedDict()\n",
    "    index = 0\n",
    "    with tf.io.gfile.GFile(vocab_file, \"r\") as reader:\n",
    "        while True:\n",
    "            token = convert_to_unicode(reader.readline())\n",
    "            if not token:\n",
    "                break\n",
    "            token = token.strip()\n",
    "            vocab[token] = index\n",
    "            index += 1\n",
    "    return vocab\n",
    "\n",
    "\n",
    "def whitespace_tokenize(text):\n",
    "    \"\"\"Runs basic whitespace cleaning and splitting on a piece of text.\"\"\"\n",
    "    text = text.strip()\n",
    "    if not text:\n",
    "        return []\n",
    "    tokens = text.split()\n",
    "    return tokens\n",
    "\n",
    "def convert_by_vocab(vocab, items):\n",
    "    \"\"\"Converts a sequence of [tokens|ids] using the vocab.\"\"\"\n",
    "    output = []\n",
    "    for item in items:\n",
    "        output.append(vocab[item])\n",
    "    return output\n",
    "\n",
    "class FullTokenizer(object):\n",
    "    \"\"\"Runs end-to-end tokenziation.\"\"\"\n",
    "\n",
    "    def __init__(self, vocab_file, do_lower_case=True):\n",
    "        self.vocab = load_vocab(vocab_file)\n",
    "        self.inv_vocab = {v: k for k, v in self.vocab.items()}\n",
    "        self.basic_tokenizer = BasicTokenizer(do_lower_case=do_lower_case)\n",
    "        self.wordpiece_tokenizer = WordpieceTokenizer(vocab=self.vocab)\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        split_tokens = []\n",
    "        for token in self.basic_tokenizer.tokenize(text):\n",
    "            for sub_token in self.wordpiece_tokenizer.tokenize(token):\n",
    "                split_tokens.append(sub_token)\n",
    "\n",
    "        return split_tokens\n",
    "    \n",
    "    def convert_tokens_to_ids(self, tokens):\n",
    "        return convert_by_vocab(self.vocab, tokens)\n",
    "\n",
    "    def convert_ids_to_tokens(self, ids):\n",
    "        return convert_by_vocab(self.inv_vocab, ids)\n",
    "\n",
    "\n",
    "class BasicTokenizer(object):\n",
    "    \"\"\"Runs basic tokenization (punctuation splitting, lower casing, etc.).\"\"\"\n",
    "\n",
    "    def __init__(self, do_lower_case=True):\n",
    "        \"\"\"Constructs a BasicTokenizer.\n",
    "    \n",
    "        Args:\n",
    "          do_lower_case: Whether to lower case the input.\n",
    "        \"\"\"\n",
    "        self.do_lower_case = do_lower_case\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        \"\"\"Tokenizes a piece of text.\"\"\"\n",
    "        text = convert_to_unicode(text)\n",
    "        text = self._clean_text(text)\n",
    "\n",
    "        # This was added on November 1st, 2018 for the multilingual and Chinese\n",
    "        # models. This is also applied to the English models now, but it doesn't\n",
    "        # matter since the English models were not trained on any Chinese data\n",
    "        # and generally don't have any Chinese data in them (there are Chinese\n",
    "        # characters in the vocabulary because Wikipedia does have some Chinese\n",
    "        # words in the English Wikipedia.).\n",
    "        text = self._tokenize_chinese_chars(text)\n",
    "\n",
    "        orig_tokens = whitespace_tokenize(text)\n",
    "        split_tokens = []\n",
    "        for token in orig_tokens:\n",
    "            if self.do_lower_case:\n",
    "                token = token.lower()\n",
    "                token = self._run_strip_accents(token)\n",
    "            split_tokens.extend(self._run_split_on_punc(token))\n",
    "\n",
    "        output_tokens = whitespace_tokenize(\" \".join(split_tokens))\n",
    "        return output_tokens\n",
    "\n",
    "    def _run_strip_accents(self, text):\n",
    "        \"\"\"Strips accents from a piece of text.\"\"\"\n",
    "        text = unicodedata.normalize(\"NFD\", text)\n",
    "        output = []\n",
    "        for char in text:\n",
    "            cat = unicodedata.category(char)\n",
    "            if cat == \"Mn\":\n",
    "                continue\n",
    "            output.append(char)\n",
    "        return \"\".join(output)\n",
    "\n",
    "    def _run_split_on_punc(self, text):\n",
    "        \"\"\"Splits punctuation on a piece of text.\"\"\"\n",
    "        chars = list(text)\n",
    "        i = 0\n",
    "        start_new_word = True\n",
    "        output = []\n",
    "        while i < len(chars):\n",
    "            char = chars[i]\n",
    "            if _is_punctuation(char):\n",
    "                output.append([char])\n",
    "                start_new_word = True\n",
    "            else:\n",
    "                if start_new_word:\n",
    "                    output.append([])\n",
    "                start_new_word = False\n",
    "                output[-1].append(char)\n",
    "            i += 1\n",
    "\n",
    "        return [\"\".join(x) for x in output]\n",
    "\n",
    "    def _tokenize_chinese_chars(self, text):\n",
    "        \"\"\"Adds whitespace around any CJK character.\"\"\"\n",
    "        output = []\n",
    "        for char in text:\n",
    "            cp = ord(char)\n",
    "            if self._is_chinese_char(cp):\n",
    "                output.append(\" \")\n",
    "                output.append(char)\n",
    "                output.append(\" \")\n",
    "            else:\n",
    "                output.append(char)\n",
    "        return \"\".join(output)\n",
    "\n",
    "    def _is_chinese_char(self, cp):\n",
    "        \"\"\"Checks whether CP is the codepoint of a CJK character.\"\"\"\n",
    "        # This defines a \"chinese character\" as anything in the CJK Unicode block:\n",
    "        #   https://en.wikipedia.org/wiki/CJK_Unified_Ideographs_(Unicode_block)\n",
    "        #\n",
    "        # Note that the CJK Unicode block is NOT all Japanese and Korean characters,\n",
    "        # despite its name. The modern Korean Hangul alphabet is a different block,\n",
    "        # as is Japanese Hiragana and Katakana. Those alphabets are used to write\n",
    "        # space-separated words, so they are not treated specially and handled\n",
    "        # like the all of the other languages.\n",
    "        if ((cp >= 0x4E00 and cp <= 0x9FFF) or  #\n",
    "                (cp >= 0x3400 and cp <= 0x4DBF) or  #\n",
    "                (cp >= 0x20000 and cp <= 0x2A6DF) or  #\n",
    "                (cp >= 0x2A700 and cp <= 0x2B73F) or  #\n",
    "                (cp >= 0x2B740 and cp <= 0x2B81F) or  #\n",
    "                (cp >= 0x2B820 and cp <= 0x2CEAF) or\n",
    "                (cp >= 0xF900 and cp <= 0xFAFF) or  #\n",
    "                (cp >= 0x2F800 and cp <= 0x2FA1F)):  #\n",
    "            return True\n",
    "\n",
    "        return False\n",
    "\n",
    "    def _clean_text(self, text):\n",
    "        \"\"\"Performs invalid character removal and whitespace cleanup on text.\"\"\"\n",
    "        output = []\n",
    "        for char in text:\n",
    "            cp = ord(char)\n",
    "            if cp == 0 or cp == 0xfffd or _is_control(char):\n",
    "                continue\n",
    "            if _is_whitespace(char):\n",
    "                output.append(\" \")\n",
    "            else:\n",
    "                output.append(char)\n",
    "        return \"\".join(output)\n",
    "\n",
    "\n",
    "class WordpieceTokenizer(object):\n",
    "    \"\"\"Runs WordPiece tokenziation.\"\"\"\n",
    "\n",
    "    def __init__(self, vocab, unk_token=\"[UNK]\", max_input_chars_per_word=200):\n",
    "        self.vocab = vocab\n",
    "        self.unk_token = unk_token\n",
    "        self.max_input_chars_per_word = max_input_chars_per_word\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        \"\"\"Tokenizes a piece of text into its word pieces.\n",
    "    \n",
    "        This uses a greedy longest-match-first algorithm to perform tokenization\n",
    "        using the given vocabulary.\n",
    "    \n",
    "        For example:\n",
    "          input = \"unaffable\"\n",
    "          output = [\"un\", \"##aff\", \"##able\"]\n",
    "    \n",
    "        Args:\n",
    "          text: A single token or whitespace separated tokens. This should have\n",
    "            already been passed through `BasicTokenizer.\n",
    "    \n",
    "        Returns:\n",
    "          A list of wordpiece tokens.\n",
    "        \"\"\"\n",
    "\n",
    "        text = convert_to_unicode(text)\n",
    "\n",
    "        output_tokens = []\n",
    "        for token in whitespace_tokenize(text):\n",
    "            chars = list(token)\n",
    "            if len(chars) > self.max_input_chars_per_word:\n",
    "                output_tokens.append(self.unk_token)\n",
    "                continue\n",
    "\n",
    "            is_bad = False\n",
    "            start = 0\n",
    "            sub_tokens = []\n",
    "            while start < len(chars):\n",
    "                end = len(chars)\n",
    "                cur_substr = None\n",
    "                while start < end:\n",
    "                    substr = \"\".join(chars[start:end])\n",
    "                    if start > 0:\n",
    "                        substr = \"##\" + substr\n",
    "                    if substr in self.vocab:\n",
    "                        cur_substr = substr\n",
    "                        break\n",
    "                    end -= 1\n",
    "                if cur_substr is None:\n",
    "                    is_bad = True\n",
    "                    break\n",
    "                sub_tokens.append(cur_substr)\n",
    "                start = end\n",
    "\n",
    "            if is_bad:\n",
    "                output_tokens.append(self.unk_token)\n",
    "            else:\n",
    "                output_tokens.extend(sub_tokens)\n",
    "        return output_tokens\n",
    "\n",
    "def _is_whitespace(char):\n",
    "    \"\"\"Checks whether `chars` is a whitespace character.\"\"\"\n",
    "    # \\t, \\n, and \\r are technically contorl characters but we treat them\n",
    "    # as whitespace since they are generally considered as such.\n",
    "    if char == \" \" or char == \"\\t\" or char == \"\\n\" or char == \"\\r\":\n",
    "        return True\n",
    "    cat = unicodedata.category(char)\n",
    "    if cat == \"Zs\":\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def _is_control(char):\n",
    "    \"\"\"Checks whether `chars` is a control character.\"\"\"\n",
    "    # These are technically control characters but we count them as whitespace\n",
    "    # characters.\n",
    "    if char == \"\\t\" or char == \"\\n\" or char == \"\\r\":\n",
    "        return False\n",
    "    cat = unicodedata.category(char)\n",
    "    if cat.startswith(\"C\"):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def _is_punctuation(char):\n",
    "    \"\"\"Checks whether `chars` is a punctuation character.\"\"\"\n",
    "    cp = ord(char)\n",
    "    # We treat all non-letter/number ASCII as punctuation.\n",
    "    # Characters such as \"^\", \"$\", and \"`\" are not in the Unicode\n",
    "    # Punctuation class but we treat them as punctuation anyways, for\n",
    "    # consistency.\n",
    "    if ((cp >= 33 and cp <= 47) or (cp >= 58 and cp <= 64) or\n",
    "            (cp >= 91 and cp <= 96) or (cp >= 123 and cp <= 126)):\n",
    "        return True\n",
    "    cat = unicodedata.category(char)\n",
    "    if cat.startswith(\"P\"):\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_chi_1 = FullTokenizer(\n",
    "    vocab_file= 'chinese_L-12_H-768_A-12/vocab.txt',\n",
    "    do_lower_case=True)\n",
    "\n",
    "tokenizer_en_1 = tfds.features.text.SubwordTextEncoder.build_from_corpus(\n",
    "    en_tr, target_vocab_size=2**13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_ques = FullTokenizer(\n",
    "    vocab_file= 'uncased_L-12_H-768_A-12/vocab.txt',\n",
    "    do_lower_case=True)\n",
    "\n",
    "tokenizer_ans = tfds.features.text.SubwordTextEncoder.build_from_corpus(\n",
    "    answers, target_vocab_size=2**13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_en_2 = FullTokenizer(\n",
    "    vocab_file= 'uncased_L-12_H-768_A-12/vocab.txt',\n",
    "    do_lower_case=True)\n",
    "\n",
    "tokenizer_chi_2 = tfds.features.text.SubwordTextEncoder.build_from_corpus(\n",
    "    chi_tr, target_vocab_size=2**13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQ_LENGTH = 40\n",
    "BUFFER_SIZE = 20000\n",
    "BATCH_SIZE = 64\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# POSITIONAL ENCODING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_angles(pos, i, d_model):\n",
    "  angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(d_model))\n",
    "  return pos * angle_rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding(position, d_model):\n",
    "    angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n",
    "                            np.arange(d_model)[np.newaxis, :],\n",
    "                            d_model)\n",
    "\n",
    "    # apply sin to even indices in the array; 2i\n",
    "    sines = np.sin(angle_rads[:, 0::2])\n",
    "\n",
    "    # apply cos to odd indices in the array; 2i+1\n",
    "    cosines = np.cos(angle_rads[:, 1::2])\n",
    "\n",
    "    pos_encoding = np.concatenate([sines, cosines], axis=-1)\n",
    "\n",
    "    pos_encoding = pos_encoding[np.newaxis, ...]\n",
    "\n",
    "    return tf.cast(pos_encoding, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_padding_mask(seq):\n",
    "  seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
    "  \n",
    "  # add extra dimensions so that we can add the padding\n",
    "  # to the attention logits.\n",
    "  return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_look_ahead_mask(size):\n",
    "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "    return mask  # (seq_len, seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(q, k, v, mask):\n",
    "    \"\"\"Calculate the attention weights.\n",
    "    q, k, v must have matching leading dimensions.\n",
    "    k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\n",
    "    The mask has different shapes depending on its type(padding or look ahead) \n",
    "    but it must be broadcastable for addition.\n",
    "    \n",
    "    Args:\n",
    "      q: query shape == (..., seq_len_q, depth)\n",
    "      k: key shape == (..., seq_len_k, depth)\n",
    "      v: value shape == (..., seq_len_v, depth_v)\n",
    "      mask: Float tensor with shape broadcastable \n",
    "            to (..., seq_len_q, seq_len_k). Defaults to None.\n",
    "      \n",
    "    Returns:\n",
    "      output, attention_weights\n",
    "    \"\"\"\n",
    "\n",
    "    matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n",
    "\n",
    "    # scale matmul_qk\n",
    "    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
    "\n",
    "    # add the mask to the scaled tensor.\n",
    "    if mask is not None:\n",
    "        scaled_attention_logits += (mask * -1e9)\n",
    "\n",
    "        # softmax is normalized on the last axis (seq_len_k) so that the scores\n",
    "    # add up to 1.\n",
    "    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\n",
    "\n",
    "    output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n",
    "\n",
    "    return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "\n",
    "        assert d_model % self.num_heads == 0\n",
    "\n",
    "        self.depth = d_model // self.num_heads\n",
    "\n",
    "        self.wq = tf.keras.layers.Dense(d_model)\n",
    "        self.wk = tf.keras.layers.Dense(d_model)\n",
    "        self.wv = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "        self.dense = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "    def split_heads(self, x, batch_size):\n",
    "        \"\"\"Split the last dimension into (num_heads, depth).\n",
    "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
    "        \"\"\"\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "\n",
    "    def call(self, v, k, q, mask):\n",
    "        batch_size = tf.shape(q)[0]\n",
    "\n",
    "        q = self.wq(q)  # (batch_size, seq_len, d_model)\n",
    "        k = self.wk(k)  # (batch_size, seq_len, d_model)\n",
    "        v = self.wv(v)  # (batch_size, seq_len, d_model)\n",
    "\n",
    "        q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n",
    "        k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n",
    "        v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n",
    "\n",
    "        # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n",
    "        # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "        scaled_attention, attention_weights = scaled_dot_product_attention(\n",
    "            q, k, v, mask)\n",
    "\n",
    "        scaled_attention = tf.transpose(scaled_attention,\n",
    "                                        perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n",
    "\n",
    "        concat_attention = tf.reshape(scaled_attention,\n",
    "                                      (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n",
    "\n",
    "        output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n",
    "\n",
    "        return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([1, 60, 512]), TensorShape([1, 8, 60, 60]))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_mha = MultiHeadAttention(d_model=512, num_heads=8)\n",
    "y = tf.random.uniform((1, 60, 768))  # (batch_size, encoder_sequence, d_model)\n",
    "q = tf.random.uniform((1, 60, 512))  # (batch_size, encoder_sequence, d_model)\n",
    "out, attn = temp_mha(y, k=y, q=q, mask=None)\n",
    "out.shape, attn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def point_wise_feed_forward_network(d_model, dff):\n",
    "    return tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(dff, activation='relu'),  # (batch_size, seq_len, dff)\n",
    "        tf.keras.layers.Dense(d_model)  # (batch_size, seq_len, d_model)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_encoder(config_file):\n",
    "    with tf.io.gfile.GFile(config_file, \"r\") as reader:\n",
    "        stock_params = StockBertConfig.from_json_string(reader.read())\n",
    "        bert_params = stock_params.to_bert_model_layer_params()\n",
    "\n",
    "    return BertModelLayer.from_params(bert_params, name=\"bert\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "\n",
    "        self.mha1 = MultiHeadAttention(d_model, num_heads)\n",
    "        self.mha2 = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout3 = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "\n",
    "    def call(self, x, enc_output, training,\n",
    "             look_ahead_mask, padding_mask):\n",
    "        # enc_output.shape == (batch_size, input_seq_len, d_model)\n",
    "\n",
    "        attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)  # (batch_size, target_seq_len, d_model)\n",
    "        attn1 = self.dropout1(attn1, training=training)\n",
    "        out1 = self.layernorm1(attn1 + x)\n",
    "\n",
    "        attn2, attn_weights_block2 = self.mha2(\n",
    "            enc_output, enc_output, out1, padding_mask)  # (batch_size, target_seq_len, d_model)\n",
    "        attn2 = self.dropout2(attn2, training=training)\n",
    "        out2 = self.layernorm2(attn2 + out1)  # (batch_size, target_seq_len, d_model)\n",
    "\n",
    "        ffn_output = self.ffn(out2)  # (batch_size, target_seq_len, d_model)\n",
    "        ffn_output = self.dropout3(ffn_output, training=training)\n",
    "        out3 = self.layernorm3(ffn_output + out2)  # (batch_size, target_seq_len, d_model)\n",
    "\n",
    "        return out3, attn_weights_block1, attn_weights_block2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([64, 50, 512])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_decoder_layer = DecoderLayer(512, 8, 2048)\n",
    "sample_encoder_output = tf.random.uniform((64, 128, 768))\n",
    "\n",
    "sample_decoder_layer_output, _, _ = sample_decoder_layer(\n",
    "    tf.random.uniform((64, 50, 512)), sample_encoder_output,\n",
    "    False, None, None)\n",
    "\n",
    "sample_decoder_layer_output.shape  # (batch_size, target_seq_len, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size,\n",
    "                 rate=0.1):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model)\n",
    "        self.pos_encoding = positional_encoding(target_vocab_size, self.d_model)\n",
    "\n",
    "        self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate)\n",
    "                           for _ in range(num_layers)]\n",
    "        self.dropout = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "    def call(self, x, enc_output, training,\n",
    "             look_ahead_mask, padding_mask):\n",
    "        seq_len = tf.shape(x)[1]\n",
    "        attention_weights = {}\n",
    "\n",
    "        x = self.embedding(x)  # (batch_size, target_seq_len, d_model)\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        x += self.pos_encoding[:, :seq_len, :]\n",
    "\n",
    "        x = self.dropout(x, training=training)\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            x, block1, block2 = self.dec_layers[i](x, enc_output, training,\n",
    "                                                   look_ahead_mask, padding_mask)\n",
    "\n",
    "            attention_weights['decoder_layer{}_block1'.format(i + 1)] = block1\n",
    "            attention_weights['decoder_layer{}_block2'.format(i + 1)] = block2\n",
    "\n",
    "        # x.shape == (batch_size, target_seq_len, d_model)\n",
    "        return x, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([64, 26, 512]), TensorShape([64, 8, 26, 128]))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_decoder = Decoder(num_layers=2, d_model=512, num_heads=8, \n",
    "                         dff=2048, target_vocab_size=8000)\n",
    "\n",
    "output, attn = sample_decoder(tf.random.uniform((64, 26)), \n",
    "                              enc_output=sample_encoder_output, \n",
    "                              training=False, look_ahead_mask=None, \n",
    "                              padding_mask=None)\n",
    "\n",
    "output.shape, attn['decoder_layer2_block2'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config(object):\n",
    "  def __init__(self, num_layers, d_model, dff, num_heads):\n",
    "    self.num_layers = num_layers\n",
    "    self.d_model = d_model\n",
    "    self.dff = dff\n",
    "    self.num_heads= num_heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bert.loader import map_to_stock_variable_name\n",
    "# /content/drive/My Drive/machine translation/transformer/bert\n",
    "class Transformer(tf.keras.Model):\n",
    "  def __init__(self, config,\n",
    "               target_vocab_size, \n",
    "               bert_config_file,\n",
    "               bert_training=False, \n",
    "               rate=0.1,\n",
    "               name='transformer'):\n",
    "      super(Transformer, self).__init__(name=name)\n",
    "\n",
    "      self.encoder = build_encoder(config_file=bert_config_file)\n",
    "      self.encoder.trainable = bert_training\n",
    "\n",
    "      self.decoder = Decoder(config.num_layers, config.d_model, \n",
    "                             config.num_heads, config.dff, target_vocab_size, rate)\n",
    "\n",
    "      self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
    "\n",
    "\n",
    "  def load_stock_weights(self, bert: BertModelLayer, ckpt_file):\n",
    "      assert isinstance(bert, BertModelLayer), \"Expecting a BertModelLayer instance as first argument\"\n",
    "      assert tf.compat.v1.train.checkpoint_exists(ckpt_file), \"Checkpoint does not exist: {}\".format(ckpt_file)\n",
    "      ckpt_reader = tf.train.load_checkpoint(ckpt_file)\n",
    "\n",
    "      bert_prefix = 'transformer/bert'\n",
    "\n",
    "      weights = []\n",
    "      for weight in bert.weights:\n",
    "          stock_name = map_to_stock_variable_name(weight.name, bert_prefix)\n",
    "          if ckpt_reader.has_tensor(stock_name):\n",
    "              value = ckpt_reader.get_tensor(stock_name)\n",
    "              weights.append(value)\n",
    "          else:\n",
    "              raise ValueError(\"No value for:[{}], i.e.:[{}] in:[{}]\".format(weight.name, stock_name, ckpt_file))\n",
    "      bert.set_weights(weights)\n",
    "      print(\"Done loading {} BERT weights from: {} into {} (prefix:{})\".format(\n",
    "          len(weights), ckpt_file, bert, bert_prefix))\n",
    "\n",
    "  def restore_encoder(self, bert_ckpt_file):\n",
    "      # loading the original pre-trained weights into the BERT layer:\n",
    "      self.load_stock_weights(self.encoder, bert_ckpt_file)\n",
    "\n",
    "  def call(self, inp, tar, training, look_ahead_mask, dec_padding_mask):\n",
    "      enc_output = self.encoder(inp, training=self.encoder.trainable)  # (batch_size, inp_seq_len, d_model)\n",
    "\n",
    "      # dec_output.shape == (batch_size, tar_seq_len, d_model)\n",
    "      dec_output, attention_weights = self.decoder(\n",
    "          tar, enc_output, training, look_ahead_mask, dec_padding_mask)\n",
    "\n",
    "      final_output = self.final_layer(dec_output)  # (batch_size, tar_seq_len, target_vocab_size)\n",
    "\n",
    "      return final_output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_vocab_size_1 = tokenizer_en_1.vocab_size + 2\n",
    "target_vocab_size_2 = tokenizer_ans.vocab_size + 2\n",
    "target_vocab_size_3 = tokenizer_chi_2.vocab_size + 2\n",
    "dropout_rate = 0.1\n",
    "config = Config(num_layers=6, d_model=512, dff=1024, num_heads=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 40)\n",
      "(64, 40, 7475)\n",
      "Done loading 196 BERT weights from: uncased_L-12_H-768_A-12\\bert_model.ckpt into <bert.model.BertModelLayer object at 0x0000023CC8508548> (prefix:transformer/bert)\n"
     ]
    }
   ],
   "source": [
    "# gs_folder_bert\n",
    "# uncased_L-12_H-768_A-12\n",
    "MODEL_DIR = \"uncased_L-12_H-768_A-12\"\n",
    "bert_config_file = os.path.join(MODEL_DIR, \"bert_config.json\")\n",
    "bert_ckpt_file = os.path.join(MODEL_DIR, 'bert_model.ckpt')\n",
    "\n",
    "# with tpu_strategy.scope():\n",
    "transformer_1 = Transformer(config=config,\n",
    "                          target_vocab_size=target_vocab_size_1,\n",
    "                          bert_config_file=bert_config_file)\n",
    "  \n",
    "inp = tf.random.uniform((BATCH_SIZE, MAX_SEQ_LENGTH))\n",
    "tar_inp = tf.random.uniform((BATCH_SIZE, MAX_SEQ_LENGTH))\n",
    "fn_out, _ = transformer_1(inp, tar_inp, \n",
    "                        True,\n",
    "                        look_ahead_mask=None,\n",
    "                        dec_padding_mask=None)\n",
    "print(tar_inp.shape) # (batch_size, tar_seq_len) \n",
    "print(fn_out.shape)  # (batch_size, tar_seq_len, target_vocab_size) \n",
    "\n",
    "# init bert pre-trained weights\n",
    "transformer_1.restore_encoder(bert_ckpt_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"transformer\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bert (BertModelLayer)        multiple                  108890112 \n",
      "_________________________________________________________________\n",
      "decoder_3 (Decoder)          multiple                  24326656  \n",
      "_________________________________________________________________\n",
      "dense_216 (Dense)            multiple                  3834675   \n",
      "=================================================================\n",
      "Total params: 137,051,443\n",
      "Trainable params: 28,161,331\n",
      "Non-trainable params: 108,890,112\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "transformer_1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 40)\n",
      "(64, 40, 8027)\n",
      "Done loading 196 BERT weights from: uncased_L-12_H-768_A-12\\bert_model.ckpt into <bert.model.BertModelLayer object at 0x0000023A842A3CC8> (prefix:transformer/bert)\n"
     ]
    }
   ],
   "source": [
    "# gs_folder_bert\n",
    "# uncased_L-12_H-768_A-12\n",
    "# MODEL_DIR = \"uncased_L-12_H-768_A-12\"\n",
    "# bert_config_file = os.path.join(MODEL_DIR, \"bert_config.json\")\n",
    "# bert_ckpt_file = os.path.join(MODEL_DIR, 'bert_model.ckpt')\n",
    "\n",
    "# with tpu_strategy.scope():\n",
    "transformer_2 = Transformer(config=config,\n",
    "                          target_vocab_size=target_vocab_size_2,\n",
    "                          bert_config_file=bert_config_file)\n",
    "  \n",
    "inp = tf.random.uniform((BATCH_SIZE, MAX_SEQ_LENGTH))\n",
    "tar_inp = tf.random.uniform((BATCH_SIZE, MAX_SEQ_LENGTH))\n",
    "fn_out, _ = transformer_2(inp, tar_inp, \n",
    "                        True,\n",
    "                        look_ahead_mask=None,\n",
    "                        dec_padding_mask=None)\n",
    "print(tar_inp.shape) # (batch_size, tar_seq_len) \n",
    "print(fn_out.shape)  # (batch_size, tar_seq_len, target_vocab_size) \n",
    "\n",
    "# init bert pre-trained weights\n",
    "transformer_2.restore_encoder(bert_ckpt_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"transformer\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bert (BertModelLayer)        multiple                  108890112 \n",
      "_________________________________________________________________\n",
      "decoder_4 (Decoder)          multiple                  24609280  \n",
      "_________________________________________________________________\n",
      "dense_277 (Dense)            multiple                  4117851   \n",
      "=================================================================\n",
      "Total params: 137,617,243\n",
      "Trainable params: 28,727,131\n",
      "Non-trainable params: 108,890,112\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "transformer_2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 40)\n",
      "(64, 40, 7320)\n",
      "Done loading 196 BERT weights from: uncased_L-12_H-768_A-12\\bert_model.ckpt into <bert.model.BertModelLayer object at 0x0000023CD12E9988> (prefix:transformer/bert)\n"
     ]
    }
   ],
   "source": [
    "# gs_folder_bert\n",
    "# uncased_L-12_H-768_A-12\n",
    "# MODEL_DIR = \"uncased_L-12_H-768_A-12\"\n",
    "# bert_config_file = os.path.join(MODEL_DIR, \"bert_config.json\")\n",
    "# bert_ckpt_file = os.path.join(MODEL_DIR, 'bert_model.ckpt')\n",
    "\n",
    "# with tpu_strategy.scope():\n",
    "transformer_3 = Transformer(config=config,\n",
    "                          target_vocab_size=target_vocab_size_3,\n",
    "                          bert_config_file=bert_config_file)\n",
    "  \n",
    "inp = tf.random.uniform((BATCH_SIZE, MAX_SEQ_LENGTH))\n",
    "tar_inp = tf.random.uniform((BATCH_SIZE, MAX_SEQ_LENGTH))\n",
    "fn_out, _ = transformer_3(inp, tar_inp, \n",
    "                        True,\n",
    "                        look_ahead_mask=None,\n",
    "                        dec_padding_mask=None)\n",
    "print(tar_inp.shape) # (batch_size, tar_seq_len) \n",
    "print(fn_out.shape)  # (batch_size, tar_seq_len, target_vocab_size) \n",
    "\n",
    "# init bert pre-trained weights\n",
    "transformer_3.restore_encoder(bert_ckpt_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"transformer\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bert (BertModelLayer)        multiple                  108890112 \n",
      "_________________________________________________________________\n",
      "decoder_6 (Decoder)          multiple                  24247296  \n",
      "_________________________________________________________________\n",
      "dense_399 (Dense)            multiple                  3755160   \n",
      "=================================================================\n",
      "Total params: 136,892,568\n",
      "Trainable params: 28,002,456\n",
      "Non-trainable params: 108,890,112\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "transformer_3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_masks(inp, tar):\n",
    "    # Used in the 2nd attention block in the decoder.\n",
    "    # This padding mask is used to mask the encoder outputs.\n",
    "    dec_padding_mask = create_padding_mask(inp)\n",
    "\n",
    "    # Used in the 1st attention block in the decoder.\n",
    "    # It is used to pad and mask future tokens in the input received by \n",
    "    # the decoder.\n",
    "    look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n",
    "    dec_target_padding_mask = create_padding_mask(tar)\n",
    "    combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n",
    "\n",
    "    return combined_mask, dec_padding_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate chi to eng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_chi(chi):\n",
    "    tokens_chi = tokenizer_chi_1.tokenize(chi)\n",
    "    lang1 = tokenizer_chi_1.convert_tokens_to_ids(['[CLS]'] + tokens_chi + ['[SEP]'])\n",
    "    return lang1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_chi_to_eng(transformer, inp_sentence):\n",
    "    # normalize input sentence\n",
    "    inp_sentence = encode_chi(inp_sentence)\n",
    "    encoder_input = tf.expand_dims(inp_sentence, 0)\n",
    "\n",
    "    # as the target is english, the first word to the transformer should be the\n",
    "    # english start token.\n",
    "    decoder_input = [tokenizer_en_1.vocab_size]\n",
    "    output = tf.expand_dims(decoder_input, 0)\n",
    "\n",
    "    for i in range(MAX_SEQ_LENGTH):\n",
    "        combined_mask, dec_padding_mask = create_masks(\n",
    "            encoder_input, output)\n",
    "\n",
    "        # predictions.shape == (batch_size, seq_len, vocab_size)\n",
    "        predictions, attention_weights = transformer(encoder_input,\n",
    "                                                     output,\n",
    "                                                     False,\n",
    "                                                     combined_mask,\n",
    "                                                     dec_padding_mask)\n",
    "\n",
    "        # select the last word from the seq_len dimension\n",
    "        predictions = predictions[:, -1:, :]  # (batch_size, 1, vocab_size)\n",
    "\n",
    "        predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
    "\n",
    "        # return the result if the predicted_id is equal to the end token\n",
    "        if tf.equal(predicted_id, tokenizer_en_1.vocab_size + 1):\n",
    "            return tf.squeeze(output, axis=0), attention_weights\n",
    "\n",
    "        # concatentate the predicted_id to the output which is given to the decoder\n",
    "        # as its input.\n",
    "        output = tf.concat([output, predicted_id], axis=-1)\n",
    "\n",
    "    return tf.squeeze(output, axis=0), attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(41,), dtype=int32, numpy=\n",
       " array([7473,   12, 4915, 1968, 6062, 6062, 6062,   12, 6097, 6097, 6097,\n",
       "        6097, 6097, 6097, 6097, 6097, 6097, 6097, 6097, 6097, 6097, 6097,\n",
       "        6097, 6097, 6097, 6097, 6097, 6097, 6097, 6097, 6097, 6097, 6097,\n",
       "        6097, 6097, 6097, 6097, 6097, 6097, 6097, 6097])>,\n",
       " {'decoder_layer1_block1': <tf.Tensor: shape=(1, 8, 40, 40), dtype=float32, numpy=\n",
       "  array([[[[1.        , 0.        , 0.        , ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           [0.28630912, 0.7136908 , 0.        , ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           [0.4789284 , 0.18223774, 0.33883384, ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           ...,\n",
       "           [0.06341842, 0.03623977, 0.01090709, ..., 0.01626615,\n",
       "            0.        , 0.        ],\n",
       "           [0.05919779, 0.03797729, 0.01341668, ..., 0.0168429 ,\n",
       "            0.02031065, 0.        ],\n",
       "           [0.0574108 , 0.03650632, 0.01572925, ..., 0.01649372,\n",
       "            0.01996945, 0.02441897]],\n",
       "  \n",
       "          [[1.        , 0.        , 0.        , ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           [0.8195278 , 0.1804722 , 0.        , ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           [0.46763992, 0.14544179, 0.38691828, ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           ...,\n",
       "           [0.00631522, 0.00415591, 0.01083375, ..., 0.04471843,\n",
       "            0.        , 0.        ],\n",
       "           [0.0067257 , 0.00341758, 0.0093083 , ..., 0.04019293,\n",
       "            0.04531969, 0.        ],\n",
       "           [0.00796197, 0.00312763, 0.00884712, ..., 0.03773955,\n",
       "            0.04205295, 0.04526467]],\n",
       "  \n",
       "          [[1.        , 0.        , 0.        , ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           [0.44064128, 0.5593587 , 0.        , ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           [0.3408396 , 0.5390691 , 0.12009131, ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           ...,\n",
       "           [0.04475639, 0.01625354, 0.01732641, ..., 0.02486386,\n",
       "            0.        , 0.        ],\n",
       "           [0.0389756 , 0.019486  , 0.02131829, ..., 0.02163853,\n",
       "            0.02141768, 0.        ],\n",
       "           [0.03728382, 0.01843023, 0.0233188 , ..., 0.02105649,\n",
       "            0.02095957, 0.02022036]],\n",
       "  \n",
       "          ...,\n",
       "  \n",
       "          [[1.        , 0.        , 0.        , ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           [0.2126863 , 0.7873137 , 0.        , ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           [0.13073003, 0.52889436, 0.3403756 , ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           ...,\n",
       "           [0.08102096, 0.13569632, 0.01654682, ..., 0.01382161,\n",
       "            0.        , 0.        ],\n",
       "           [0.07617588, 0.14055125, 0.01484076, ..., 0.01380788,\n",
       "            0.01329895, 0.        ],\n",
       "           [0.07980657, 0.14334854, 0.01319314, ..., 0.01261005,\n",
       "            0.01148772, 0.01179592]],\n",
       "  \n",
       "          [[1.        , 0.        , 0.        , ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           [0.4123543 , 0.5876457 , 0.        , ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           [0.33516985, 0.48949197, 0.17533822, ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           ...,\n",
       "           [0.01423032, 0.01805858, 0.01911662, ..., 0.00981127,\n",
       "            0.        , 0.        ],\n",
       "           [0.01496437, 0.01213893, 0.01737195, ..., 0.00975966,\n",
       "            0.01080174, 0.        ],\n",
       "           [0.01652469, 0.00937228, 0.01551015, ..., 0.01018769,\n",
       "            0.01133587, 0.01496182]],\n",
       "  \n",
       "          [[1.        , 0.        , 0.        , ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           [0.4987716 , 0.50122845, 0.        , ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           [0.26408178, 0.25697517, 0.4789431 , ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           ...,\n",
       "           [0.02109744, 0.0185182 , 0.03995242, ..., 0.02227945,\n",
       "            0.        , 0.        ],\n",
       "           [0.02718418, 0.02143635, 0.04394509, ..., 0.02185154,\n",
       "            0.02127188, 0.        ],\n",
       "           [0.03218972, 0.02288673, 0.03811092, ..., 0.02385492,\n",
       "            0.02437035, 0.02233784]]]], dtype=float32)>,\n",
       "  'decoder_layer1_block2': <tf.Tensor: shape=(1, 8, 40, 6), dtype=float32, numpy=\n",
       "  array([[[[0.2261099 , 0.14020026, 0.02181729, 0.06594997, 0.06164671,\n",
       "            0.4842758 ],\n",
       "           [0.12560916, 0.10635057, 0.06543775, 0.1185668 , 0.11181453,\n",
       "            0.4722212 ],\n",
       "           [0.09259611, 0.14556397, 0.09044991, 0.13583909, 0.13267133,\n",
       "            0.4028796 ],\n",
       "           ...,\n",
       "           [0.22168678, 0.17934482, 0.06993665, 0.18251804, 0.2295648 ,\n",
       "            0.11694886],\n",
       "           [0.2378627 , 0.1745532 , 0.06754846, 0.18911935, 0.23322955,\n",
       "            0.09768678],\n",
       "           [0.2669056 , 0.16867739, 0.06691446, 0.17570989, 0.23231797,\n",
       "            0.08947472]],\n",
       "  \n",
       "          [[0.10185539, 0.12017706, 0.08492401, 0.15190965, 0.1018374 ,\n",
       "            0.43929645],\n",
       "           [0.06333663, 0.086027  , 0.13232361, 0.11562791, 0.11736023,\n",
       "            0.48532462],\n",
       "           [0.03017673, 0.13032942, 0.13215412, 0.27609232, 0.09016643,\n",
       "            0.34108102],\n",
       "           ...,\n",
       "           [0.08244739, 0.13744506, 0.23748606, 0.26255336, 0.1515988 ,\n",
       "            0.1284693 ],\n",
       "           [0.07685722, 0.13696763, 0.2271879 , 0.26713306, 0.14705294,\n",
       "            0.14480132],\n",
       "           [0.07949974, 0.1322254 , 0.21545954, 0.26950732, 0.14391322,\n",
       "            0.15939476]],\n",
       "  \n",
       "          [[0.25634015, 0.18680434, 0.06715636, 0.15762605, 0.12248312,\n",
       "            0.20958999],\n",
       "           [0.28563777, 0.24670808, 0.1174233 , 0.17279582, 0.09122236,\n",
       "            0.08621268],\n",
       "           [0.10212332, 0.15994334, 0.11871025, 0.327605  , 0.17236853,\n",
       "            0.11924954],\n",
       "           ...,\n",
       "           [0.16538   , 0.21976382, 0.17233522, 0.1642895 , 0.09003667,\n",
       "            0.18819478],\n",
       "           [0.1815638 , 0.22709496, 0.17660287, 0.14467166, 0.08950689,\n",
       "            0.18055983],\n",
       "           [0.18298945, 0.23181716, 0.17988423, 0.14071503, 0.08828172,\n",
       "            0.17631248]],\n",
       "  \n",
       "          ...,\n",
       "  \n",
       "          [[0.2736026 , 0.10871754, 0.12067771, 0.08775556, 0.11858961,\n",
       "            0.290657  ],\n",
       "           [0.12875591, 0.1366932 , 0.18904187, 0.11886726, 0.12060316,\n",
       "            0.30603868],\n",
       "           [0.13439555, 0.08220581, 0.07647112, 0.05289114, 0.07707296,\n",
       "            0.5769634 ],\n",
       "           ...,\n",
       "           [0.20896293, 0.12324326, 0.1127136 , 0.17393711, 0.12779807,\n",
       "            0.25334504],\n",
       "           [0.1910453 , 0.12052479, 0.1079058 , 0.1845667 , 0.13396733,\n",
       "            0.26199004],\n",
       "           [0.17376861, 0.11651553, 0.09996251, 0.19653207, 0.14311989,\n",
       "            0.27010137]],\n",
       "  \n",
       "          [[0.11290006, 0.10776778, 0.21748509, 0.24473536, 0.16376342,\n",
       "            0.1533483 ],\n",
       "           [0.12267963, 0.11587097, 0.2368099 , 0.21525554, 0.19015989,\n",
       "            0.11922417],\n",
       "           [0.08636916, 0.16690515, 0.2181935 , 0.14612873, 0.16230355,\n",
       "            0.22009982],\n",
       "           ...,\n",
       "           [0.12010965, 0.12523653, 0.08909049, 0.20161925, 0.15350938,\n",
       "            0.31043467],\n",
       "           [0.13213126, 0.1292999 , 0.08847409, 0.19812351, 0.14900088,\n",
       "            0.30297035],\n",
       "           [0.14531794, 0.13330518, 0.08722746, 0.20099252, 0.15393771,\n",
       "            0.2792192 ]],\n",
       "  \n",
       "          [[0.15727904, 0.07785184, 0.21653341, 0.19456379, 0.14622714,\n",
       "            0.2075449 ],\n",
       "           [0.23549607, 0.09630711, 0.11375633, 0.11960486, 0.1454385 ,\n",
       "            0.28939706],\n",
       "           [0.16577615, 0.06756055, 0.07369611, 0.14006107, 0.12503581,\n",
       "            0.4278702 ],\n",
       "           ...,\n",
       "           [0.19682047, 0.10346588, 0.11280528, 0.18894511, 0.15450501,\n",
       "            0.24345824],\n",
       "           [0.19532381, 0.1070944 , 0.11531044, 0.18408956, 0.15626872,\n",
       "            0.24191299],\n",
       "           [0.19626719, 0.11220364, 0.12322233, 0.1824683 , 0.16337672,\n",
       "            0.22246186]]]], dtype=float32)>,\n",
       "  'decoder_layer2_block1': <tf.Tensor: shape=(1, 8, 40, 40), dtype=float32, numpy=\n",
       "  array([[[[1.        , 0.        , 0.        , ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           [0.47741404, 0.5225859 , 0.        , ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           [0.4631124 , 0.32252988, 0.21435769, ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           ...,\n",
       "           [0.02686438, 0.01128297, 0.02010491, ..., 0.0295246 ,\n",
       "            0.        , 0.        ],\n",
       "           [0.02630097, 0.01219309, 0.02086189, ..., 0.03074271,\n",
       "            0.02849431, 0.        ],\n",
       "           [0.0237738 , 0.01340106, 0.01994325, ..., 0.03396221,\n",
       "            0.03126309, 0.02667962]],\n",
       "  \n",
       "          [[1.        , 0.        , 0.        , ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           [0.12632641, 0.87367356, 0.        , ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           [0.11959811, 0.7098217 , 0.1705802 , ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           ...,\n",
       "           [0.01101267, 0.02590851, 0.0128208 , ..., 0.0231639 ,\n",
       "            0.        , 0.        ],\n",
       "           [0.0103807 , 0.0256271 , 0.01217732, ..., 0.02305017,\n",
       "            0.02564286, 0.        ],\n",
       "           [0.00943608, 0.02521558, 0.01096694, ..., 0.0237139 ,\n",
       "            0.02576261, 0.02624138]],\n",
       "  \n",
       "          [[1.        , 0.        , 0.        , ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           [0.5941152 , 0.40588477, 0.        , ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           [0.35486025, 0.38446614, 0.26067367, ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           ...,\n",
       "           [0.02834194, 0.04236915, 0.0400118 , ..., 0.01684911,\n",
       "            0.        , 0.        ],\n",
       "           [0.02927079, 0.04405924, 0.03908376, ..., 0.01730153,\n",
       "            0.0202112 , 0.        ],\n",
       "           [0.02992835, 0.04466523, 0.04203542, ..., 0.01667015,\n",
       "            0.01932101, 0.02240255]],\n",
       "  \n",
       "          ...,\n",
       "  \n",
       "          [[1.        , 0.        , 0.        , ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           [0.6645702 , 0.33542982, 0.        , ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           [0.40948847, 0.21763316, 0.37287837, ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           ...,\n",
       "           [0.01131042, 0.00919351, 0.0111656 , ..., 0.0200625 ,\n",
       "            0.        , 0.        ],\n",
       "           [0.01088374, 0.00948247, 0.01088429, ..., 0.01960208,\n",
       "            0.02004018, 0.        ],\n",
       "           [0.00976245, 0.00942433, 0.01053554, ..., 0.01980766,\n",
       "            0.02019475, 0.01870094]],\n",
       "  \n",
       "          [[1.        , 0.        , 0.        , ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           [0.63986135, 0.3601386 , 0.        , ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           [0.31669998, 0.52081627, 0.16248374, ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           ...,\n",
       "           [0.03841534, 0.05486426, 0.02207277, ..., 0.01171764,\n",
       "            0.        , 0.        ],\n",
       "           [0.02801113, 0.05590626, 0.01850078, ..., 0.01127247,\n",
       "            0.01217376, 0.        ],\n",
       "           [0.02428254, 0.05915041, 0.01784997, ..., 0.01047335,\n",
       "            0.01124111, 0.01379567]],\n",
       "  \n",
       "          [[1.        , 0.        , 0.        , ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           [0.46896228, 0.5310377 , 0.        , ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           [0.38758495, 0.36632827, 0.24608678, ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           ...,\n",
       "           [0.03965683, 0.05158024, 0.01434721, ..., 0.02829009,\n",
       "            0.        , 0.        ],\n",
       "           [0.04008552, 0.04978866, 0.01252702, ..., 0.02868399,\n",
       "            0.03187107, 0.        ],\n",
       "           [0.03825019, 0.04796562, 0.01130321, ..., 0.02843882,\n",
       "            0.03239496, 0.03100448]]]], dtype=float32)>,\n",
       "  'decoder_layer2_block2': <tf.Tensor: shape=(1, 8, 40, 6), dtype=float32, numpy=\n",
       "  array([[[[0.38255516, 0.07283346, 0.08204857, 0.05257938, 0.10567411,\n",
       "            0.3043094 ],\n",
       "           [0.41914693, 0.0856408 , 0.08327515, 0.06585644, 0.12765056,\n",
       "            0.21843015],\n",
       "           [0.32295159, 0.10380882, 0.10346598, 0.07165597, 0.13555579,\n",
       "            0.26256183],\n",
       "           ...,\n",
       "           [0.23373924, 0.07179955, 0.06024376, 0.07440789, 0.08434439,\n",
       "            0.47546515],\n",
       "           [0.2339712 , 0.07234803, 0.05879324, 0.06845544, 0.08572608,\n",
       "            0.4807061 ],\n",
       "           [0.23749812, 0.07444053, 0.06075538, 0.06570899, 0.08741826,\n",
       "            0.47417873]],\n",
       "  \n",
       "          [[0.17942381, 0.13252093, 0.10072602, 0.11966492, 0.08287578,\n",
       "            0.38478854],\n",
       "           [0.23726912, 0.16848451, 0.1555441 , 0.12942407, 0.09259   ,\n",
       "            0.21668819],\n",
       "           [0.21664813, 0.13400234, 0.1550532 , 0.11834811, 0.10217852,\n",
       "            0.27376968],\n",
       "           ...,\n",
       "           [0.1932675 , 0.15290691, 0.09575952, 0.14745791, 0.07376139,\n",
       "            0.3368468 ],\n",
       "           [0.18862745, 0.16526921, 0.10119432, 0.15170792, 0.07477526,\n",
       "            0.3184258 ],\n",
       "           [0.17398511, 0.1766669 , 0.10773844, 0.15697579, 0.07627195,\n",
       "            0.3083618 ]],\n",
       "  \n",
       "          [[0.2124994 , 0.11260725, 0.10947094, 0.18765281, 0.09681742,\n",
       "            0.28095216],\n",
       "           [0.27276495, 0.11518615, 0.07285262, 0.12014881, 0.07752233,\n",
       "            0.3415251 ],\n",
       "           [0.28533706, 0.09748854, 0.05896577, 0.10237204, 0.07270457,\n",
       "            0.383132  ],\n",
       "           ...,\n",
       "           [0.1373672 , 0.12523049, 0.16938846, 0.14785157, 0.09801336,\n",
       "            0.32214892],\n",
       "           [0.12269762, 0.1277192 , 0.18375544, 0.15478759, 0.10187152,\n",
       "            0.30916864],\n",
       "           [0.11390534, 0.13023518, 0.18219355, 0.15802047, 0.0997804 ,\n",
       "            0.31586504]],\n",
       "  \n",
       "          ...,\n",
       "  \n",
       "          [[0.15128052, 0.18668753, 0.14326201, 0.15295357, 0.14026879,\n",
       "            0.22554761],\n",
       "           [0.1424672 , 0.13765618, 0.12472111, 0.15049465, 0.09264624,\n",
       "            0.3520146 ],\n",
       "           [0.2044384 , 0.1492863 , 0.1482163 , 0.14211282, 0.09504664,\n",
       "            0.26089954],\n",
       "           ...,\n",
       "           [0.12999235, 0.16017751, 0.13413988, 0.18720682, 0.1410816 ,\n",
       "            0.24740185],\n",
       "           [0.13522175, 0.1556372 , 0.12733345, 0.18152173, 0.13974537,\n",
       "            0.26054057],\n",
       "           [0.1450479 , 0.15607871, 0.12218528, 0.16606559, 0.13869423,\n",
       "            0.27192828]],\n",
       "  \n",
       "          [[0.04830027, 0.10577765, 0.32088742, 0.25677666, 0.2292131 ,\n",
       "            0.03904492],\n",
       "           [0.08276604, 0.10434647, 0.3312166 , 0.2572375 , 0.18095578,\n",
       "            0.0434776 ],\n",
       "           [0.06371313, 0.11184922, 0.34147683, 0.22444136, 0.19751644,\n",
       "            0.06100298],\n",
       "           ...,\n",
       "           [0.14966932, 0.13167208, 0.16783778, 0.18356657, 0.21993132,\n",
       "            0.14732286],\n",
       "           [0.1492177 , 0.14098223, 0.16800976, 0.17686714, 0.22037487,\n",
       "            0.14454825],\n",
       "           [0.14378816, 0.1490967 , 0.1760763 , 0.17592853, 0.22067523,\n",
       "            0.1344351 ]],\n",
       "  \n",
       "          [[0.20979063, 0.21784034, 0.11097872, 0.14800252, 0.10637627,\n",
       "            0.20701146],\n",
       "           [0.24325342, 0.22591954, 0.1251011 , 0.10082   , 0.1292199 ,\n",
       "            0.175686  ],\n",
       "           [0.29130313, 0.21063413, 0.11777887, 0.09307996, 0.12081145,\n",
       "            0.16639248],\n",
       "           ...,\n",
       "           [0.14246249, 0.27833334, 0.13937348, 0.16556124, 0.14369051,\n",
       "            0.13057892],\n",
       "           [0.14578228, 0.26869112, 0.14720519, 0.15611586, 0.15651987,\n",
       "            0.12568566],\n",
       "           [0.15288466, 0.26192993, 0.1479069 , 0.15036602, 0.16548257,\n",
       "            0.12142988]]]], dtype=float32)>,\n",
       "  'decoder_layer3_block1': <tf.Tensor: shape=(1, 8, 40, 40), dtype=float32, numpy=\n",
       "  array([[[[1.        , 0.        , 0.        , ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           [0.56662107, 0.43337893, 0.        , ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           [0.4093233 , 0.28375152, 0.30692512, ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           ...,\n",
       "           [0.02407589, 0.01262673, 0.03070029, ..., 0.01924872,\n",
       "            0.        , 0.        ],\n",
       "           [0.02197588, 0.01199999, 0.02948349, ..., 0.01863632,\n",
       "            0.02078099, 0.        ],\n",
       "           [0.02039418, 0.01181451, 0.02793282, ..., 0.01823556,\n",
       "            0.02053557, 0.01947135]],\n",
       "  \n",
       "          [[1.        , 0.        , 0.        , ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           [0.5721181 , 0.4278819 , 0.        , ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           [0.28412187, 0.27822816, 0.43764994, ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           ...,\n",
       "           [0.01977899, 0.01724198, 0.01588539, ..., 0.02556277,\n",
       "            0.        , 0.        ],\n",
       "           [0.01710394, 0.01506088, 0.01464035, ..., 0.02618492,\n",
       "            0.0291805 , 0.        ],\n",
       "           [0.01428237, 0.01271369, 0.01302085, ..., 0.02647254,\n",
       "            0.02946805, 0.03059529]],\n",
       "  \n",
       "          [[1.        , 0.        , 0.        , ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           [0.31525454, 0.68474543, 0.        , ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           [0.23729783, 0.524731  , 0.23797114, ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           ...,\n",
       "           [0.02542877, 0.02997553, 0.0295713 , ..., 0.01261694,\n",
       "            0.        , 0.        ],\n",
       "           [0.02647899, 0.02986955, 0.02746008, ..., 0.01257078,\n",
       "            0.0142166 , 0.        ],\n",
       "           [0.02620887, 0.02736427, 0.02417941, ..., 0.01296538,\n",
       "            0.01463668, 0.01820859]],\n",
       "  \n",
       "          ...,\n",
       "  \n",
       "          [[1.        , 0.        , 0.        , ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           [0.4734974 , 0.5265026 , 0.        , ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           [0.34242964, 0.4057745 , 0.25179592, ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           ...,\n",
       "           [0.07471659, 0.07351375, 0.06263855, ..., 0.02188109,\n",
       "            0.        , 0.        ],\n",
       "           [0.0740437 , 0.07056912, 0.05922895, ..., 0.02166894,\n",
       "            0.02023607, 0.        ],\n",
       "           [0.07415684, 0.07174996, 0.06023751, ..., 0.0217124 ,\n",
       "            0.02019165, 0.01975985]],\n",
       "  \n",
       "          [[1.        , 0.        , 0.        , ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           [0.4179459 , 0.5820541 , 0.        , ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           [0.32897812, 0.45704812, 0.21397375, ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           ...,\n",
       "           [0.03534456, 0.01869082, 0.01404382, ..., 0.02481234,\n",
       "            0.        , 0.        ],\n",
       "           [0.03239136, 0.01742325, 0.01263294, ..., 0.02558861,\n",
       "            0.02310353, 0.        ],\n",
       "           [0.02807133, 0.01598986, 0.01172119, ..., 0.02610012,\n",
       "            0.02346298, 0.02268502]],\n",
       "  \n",
       "          [[1.        , 0.        , 0.        , ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           [0.45030284, 0.54969716, 0.        , ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           [0.16084746, 0.3783788 , 0.46077374, ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           ...,\n",
       "           [0.00220196, 0.01399292, 0.0214925 , ..., 0.03145789,\n",
       "            0.        , 0.        ],\n",
       "           [0.00224637, 0.01390731, 0.01949989, ..., 0.03083951,\n",
       "            0.0344197 , 0.        ],\n",
       "           [0.00256302, 0.01494357, 0.01935291, ..., 0.02994055,\n",
       "            0.03308725, 0.03524648]]]], dtype=float32)>,\n",
       "  'decoder_layer3_block2': <tf.Tensor: shape=(1, 8, 40, 6), dtype=float32, numpy=\n",
       "  array([[[[0.09109754, 0.12274606, 0.16030253, 0.15535061, 0.1891204 ,\n",
       "            0.28138286],\n",
       "           [0.08266065, 0.10454925, 0.16062564, 0.17557013, 0.16078924,\n",
       "            0.31580514],\n",
       "           [0.10016426, 0.12849197, 0.15851888, 0.17490093, 0.17813756,\n",
       "            0.2597864 ],\n",
       "           ...,\n",
       "           [0.10201198, 0.11564597, 0.1608875 , 0.20990995, 0.18915568,\n",
       "            0.22238901],\n",
       "           [0.10162867, 0.12022996, 0.16602908, 0.21371277, 0.19527343,\n",
       "            0.20312612],\n",
       "           [0.09889053, 0.12399406, 0.17049481, 0.21774942, 0.19821186,\n",
       "            0.19065933]],\n",
       "  \n",
       "          [[0.07918331, 0.21727234, 0.19716774, 0.1820547 , 0.21231328,\n",
       "            0.11200864],\n",
       "           [0.11201622, 0.1991848 , 0.15253766, 0.1951149 , 0.22197534,\n",
       "            0.11917114],\n",
       "           [0.11506176, 0.20324555, 0.16448428, 0.17949793, 0.19929978,\n",
       "            0.1384107 ],\n",
       "           ...,\n",
       "           [0.08492766, 0.1271266 , 0.16258018, 0.21358904, 0.17073232,\n",
       "            0.2410442 ],\n",
       "           [0.09000159, 0.12757076, 0.16427316, 0.19667442, 0.17357032,\n",
       "            0.24790977],\n",
       "           [0.09760458, 0.13326858, 0.16126241, 0.19142514, 0.1783212 ,\n",
       "            0.23811801]],\n",
       "  \n",
       "          [[0.19071728, 0.12771162, 0.1590309 , 0.1540706 , 0.19143347,\n",
       "            0.1770361 ],\n",
       "           [0.1769622 , 0.11703297, 0.11924401, 0.10578222, 0.18176393,\n",
       "            0.2992147 ],\n",
       "           [0.14430715, 0.15292695, 0.13644087, 0.11936224, 0.21617085,\n",
       "            0.23079196],\n",
       "           ...,\n",
       "           [0.15175095, 0.15185793, 0.09009624, 0.08802725, 0.16290748,\n",
       "            0.35536018],\n",
       "           [0.1556843 , 0.15530454, 0.09017718, 0.08487717, 0.1574195 ,\n",
       "            0.35653725],\n",
       "           [0.16104631, 0.15878388, 0.09596596, 0.08947413, 0.1591733 ,\n",
       "            0.3355564 ]],\n",
       "  \n",
       "          ...,\n",
       "  \n",
       "          [[0.11345931, 0.20975643, 0.25173035, 0.19290338, 0.1322376 ,\n",
       "            0.09991292],\n",
       "           [0.1055775 , 0.21025471, 0.22558863, 0.18960433, 0.09853602,\n",
       "            0.17043884],\n",
       "           [0.11905815, 0.21984008, 0.25003666, 0.18673886, 0.10510627,\n",
       "            0.11921993],\n",
       "           ...,\n",
       "           [0.15889937, 0.20947023, 0.16849324, 0.19262633, 0.11914751,\n",
       "            0.15136333],\n",
       "           [0.16982748, 0.20503776, 0.17057353, 0.18641327, 0.12668587,\n",
       "            0.14146204],\n",
       "           [0.18446265, 0.20095026, 0.17051288, 0.1832369 , 0.13235903,\n",
       "            0.12847817]],\n",
       "  \n",
       "          [[0.13746461, 0.18944326, 0.17650796, 0.23862134, 0.10396083,\n",
       "            0.15400198],\n",
       "           [0.15802522, 0.14741793, 0.16949739, 0.25466824, 0.11380301,\n",
       "            0.15658824],\n",
       "           [0.14848313, 0.12406863, 0.14317343, 0.29870403, 0.11771881,\n",
       "            0.16785198],\n",
       "           ...,\n",
       "           [0.12469751, 0.16821484, 0.06884691, 0.18427001, 0.09596699,\n",
       "            0.35800377],\n",
       "           [0.12454559, 0.16154636, 0.06891342, 0.18371318, 0.0915275 ,\n",
       "            0.3697539 ],\n",
       "           [0.12691712, 0.15619868, 0.06928886, 0.18641679, 0.08812077,\n",
       "            0.37305778]],\n",
       "  \n",
       "          [[0.08608736, 0.22810498, 0.16053107, 0.13314085, 0.2073901 ,\n",
       "            0.1847456 ],\n",
       "           [0.08923655, 0.20214711, 0.1972327 , 0.11406446, 0.18268943,\n",
       "            0.21462971],\n",
       "           [0.09888367, 0.21112898, 0.19498712, 0.14890826, 0.20214178,\n",
       "            0.14395021],\n",
       "           ...,\n",
       "           [0.08758888, 0.15758081, 0.19998428, 0.22441201, 0.21566609,\n",
       "            0.11476795],\n",
       "           [0.08352482, 0.15062284, 0.21508108, 0.23753485, 0.21305564,\n",
       "            0.10018067],\n",
       "           [0.07992307, 0.1477086 , 0.22506452, 0.24440101, 0.20950063,\n",
       "            0.09340221]]]], dtype=float32)>,\n",
       "  'decoder_layer4_block1': <tf.Tensor: shape=(1, 8, 40, 40), dtype=float32, numpy=\n",
       "  array([[[[1.        , 0.        , 0.        , ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           [0.5559747 , 0.44402522, 0.        , ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           [0.3383384 , 0.34925553, 0.3124061 , ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           ...,\n",
       "           [0.02769732, 0.02266866, 0.02223897, ..., 0.03647466,\n",
       "            0.        , 0.        ],\n",
       "           [0.02613785, 0.02122277, 0.02039164, ..., 0.03570259,\n",
       "            0.03194992, 0.        ],\n",
       "           [0.02333218, 0.0186688 , 0.01777106, ..., 0.03583461,\n",
       "            0.0319236 , 0.02881898]],\n",
       "  \n",
       "          [[1.        , 0.        , 0.        , ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           [0.6399406 , 0.3600594 , 0.        , ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           [0.46913934, 0.24677719, 0.2840835 , ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           ...,\n",
       "           [0.07356113, 0.0386259 , 0.04380335, ..., 0.03717761,\n",
       "            0.        , 0.        ],\n",
       "           [0.068263  , 0.03658339, 0.04102864, ..., 0.03630428,\n",
       "            0.03272956, 0.        ],\n",
       "           [0.06392287, 0.03479202, 0.03872791, ..., 0.035816  ,\n",
       "            0.03215287, 0.02851757]],\n",
       "  \n",
       "          [[1.        , 0.        , 0.        , ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           [0.52501476, 0.47498524, 0.        , ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           [0.28764945, 0.27837804, 0.43397248, ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           ...,\n",
       "           [0.02758544, 0.02531996, 0.03262172, ..., 0.01463732,\n",
       "            0.        , 0.        ],\n",
       "           [0.0281505 , 0.02551742, 0.03120801, ..., 0.01432558,\n",
       "            0.0132583 , 0.        ],\n",
       "           [0.02721993, 0.02478825, 0.02965317, ..., 0.01433304,\n",
       "            0.01321353, 0.01294618]],\n",
       "  \n",
       "          ...,\n",
       "  \n",
       "          [[1.        , 0.        , 0.        , ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           [0.49454796, 0.50545204, 0.        , ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           [0.3235973 , 0.34694624, 0.32945645, ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           ...,\n",
       "           [0.02535437, 0.0194799 , 0.01858274, ..., 0.02761284,\n",
       "            0.        , 0.        ],\n",
       "           [0.02620477, 0.02087573, 0.01940632, ..., 0.02636188,\n",
       "            0.0245486 , 0.        ],\n",
       "           [0.02558831, 0.0215616 , 0.02008639, ..., 0.02543146,\n",
       "            0.02365246, 0.02343303]],\n",
       "  \n",
       "          [[1.        , 0.        , 0.        , ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           [0.4901475 , 0.50985247, 0.        , ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           [0.3299424 , 0.35893142, 0.31112614, ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           ...,\n",
       "           [0.0567213 , 0.04611508, 0.04064795, ..., 0.02065124,\n",
       "            0.        , 0.        ],\n",
       "           [0.0571322 , 0.04699363, 0.04038368, ..., 0.01982382,\n",
       "            0.02217626, 0.        ],\n",
       "           [0.0537887 , 0.04632234, 0.03880638, ..., 0.01927443,\n",
       "            0.02157506, 0.02294559]],\n",
       "  \n",
       "          [[1.        , 0.        , 0.        , ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           [0.4948    , 0.50519997, 0.        , ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           [0.35660416, 0.38122424, 0.2621716 , ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           ...,\n",
       "           [0.05632718, 0.0538048 , 0.05519488, ..., 0.01696805,\n",
       "            0.        , 0.        ],\n",
       "           [0.05889722, 0.05802796, 0.05685223, ..., 0.01657879,\n",
       "            0.01680155, 0.        ],\n",
       "           [0.05823134, 0.05891405, 0.05609246, ..., 0.01669254,\n",
       "            0.01695518, 0.01739891]]]], dtype=float32)>,\n",
       "  'decoder_layer4_block2': <tf.Tensor: shape=(1, 8, 40, 6), dtype=float32, numpy=\n",
       "  array([[[[0.30148655, 0.17398933, 0.13592213, 0.13480961, 0.14231169,\n",
       "            0.11148066],\n",
       "           [0.32313114, 0.1604885 , 0.1307641 , 0.16542894, 0.11628944,\n",
       "            0.10389786],\n",
       "           [0.34505793, 0.19923957, 0.135491  , 0.11669319, 0.12777567,\n",
       "            0.07574268],\n",
       "           ...,\n",
       "           [0.3274342 , 0.1542982 , 0.10796123, 0.10510234, 0.0995011 ,\n",
       "            0.20570296],\n",
       "           [0.34279272, 0.15408406, 0.10427319, 0.10482579, 0.10066334,\n",
       "            0.19336088],\n",
       "           [0.36120275, 0.15294643, 0.10225911, 0.10202453, 0.10167339,\n",
       "            0.17989382]],\n",
       "  \n",
       "          [[0.12960911, 0.2722115 , 0.15858033, 0.20089257, 0.1421854 ,\n",
       "            0.09652104],\n",
       "           [0.13078074, 0.2958445 , 0.15174569, 0.21842137, 0.11818293,\n",
       "            0.08502477],\n",
       "           [0.13971546, 0.263702  , 0.15947519, 0.21025279, 0.12238584,\n",
       "            0.10446871],\n",
       "           ...,\n",
       "           [0.12578113, 0.27096206, 0.18437617, 0.20364869, 0.133413  ,\n",
       "            0.0818189 ],\n",
       "           [0.11902364, 0.2668014 , 0.19217172, 0.20610549, 0.13201463,\n",
       "            0.08388317],\n",
       "           [0.11315005, 0.2659357 , 0.19445376, 0.21269625, 0.13133983,\n",
       "            0.08242442]],\n",
       "  \n",
       "          [[0.17014967, 0.1789419 , 0.11594678, 0.18132046, 0.13357563,\n",
       "            0.22006564],\n",
       "           [0.20449512, 0.1525159 , 0.0777054 , 0.16258906, 0.1212529 ,\n",
       "            0.28144157],\n",
       "           [0.24199401, 0.16930091, 0.08663172, 0.15827729, 0.12803411,\n",
       "            0.21576187],\n",
       "           ...,\n",
       "           [0.24185231, 0.13589507, 0.08880642, 0.2130148 , 0.19016896,\n",
       "            0.13026243],\n",
       "           [0.2430283 , 0.13800885, 0.08781905, 0.21118325, 0.18512283,\n",
       "            0.13483767],\n",
       "           [0.2435444 , 0.1415942 , 0.08779058, 0.20998313, 0.18139227,\n",
       "            0.13569552]],\n",
       "  \n",
       "          ...,\n",
       "  \n",
       "          [[0.12633346, 0.14932403, 0.20196284, 0.20662817, 0.12240782,\n",
       "            0.19334365],\n",
       "           [0.13062735, 0.14669366, 0.20932858, 0.17122582, 0.12736347,\n",
       "            0.21476117],\n",
       "           [0.11688858, 0.16461122, 0.21943669, 0.18963088, 0.11870874,\n",
       "            0.19072384],\n",
       "           ...,\n",
       "           [0.0908913 , 0.2070568 , 0.25664723, 0.1764378 , 0.11888303,\n",
       "            0.15008381],\n",
       "           [0.0901937 , 0.20825076, 0.25352982, 0.17900562, 0.12231307,\n",
       "            0.14670704],\n",
       "           [0.09087184, 0.20915222, 0.24804436, 0.18568888, 0.1208096 ,\n",
       "            0.14543307]],\n",
       "  \n",
       "          [[0.18978061, 0.24201219, 0.15433653, 0.15399672, 0.15890007,\n",
       "            0.10097391],\n",
       "           [0.24454886, 0.21210888, 0.1514102 , 0.12351334, 0.15160485,\n",
       "            0.11681383],\n",
       "           [0.23500174, 0.19011277, 0.15622291, 0.14603248, 0.1323505 ,\n",
       "            0.14027958],\n",
       "           ...,\n",
       "           [0.16810878, 0.2593989 , 0.20305306, 0.14477915, 0.13032866,\n",
       "            0.09433148],\n",
       "           [0.17065573, 0.2585856 , 0.20815347, 0.14258061, 0.12637377,\n",
       "            0.09365075],\n",
       "           [0.17454937, 0.25636622, 0.21172628, 0.1405678 , 0.12218339,\n",
       "            0.09460691]],\n",
       "  \n",
       "          [[0.15827695, 0.11402088, 0.0699255 , 0.08954295, 0.1760101 ,\n",
       "            0.39222366],\n",
       "           [0.18342593, 0.13677801, 0.09168186, 0.12340155, 0.19687258,\n",
       "            0.2678401 ],\n",
       "           [0.17360422, 0.12020522, 0.06678814, 0.10687769, 0.18159467,\n",
       "            0.35093004],\n",
       "           ...,\n",
       "           [0.20281309, 0.10428557, 0.04221085, 0.07442836, 0.1462921 ,\n",
       "            0.42997   ],\n",
       "           [0.20209809, 0.10256204, 0.04108377, 0.07611132, 0.14679115,\n",
       "            0.43135366],\n",
       "           [0.20830807, 0.10161331, 0.0406278 , 0.07722638, 0.14616069,\n",
       "            0.42606378]]]], dtype=float32)>,\n",
       "  'decoder_layer5_block1': <tf.Tensor: shape=(1, 8, 40, 40), dtype=float32, numpy=\n",
       "  array([[[[1.        , 0.        , 0.        , ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           [0.6078597 , 0.39214033, 0.        , ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           [0.39219394, 0.27718616, 0.33061996, ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           ...,\n",
       "           [0.04453811, 0.03703628, 0.04243352, ..., 0.03139241,\n",
       "            0.        , 0.        ],\n",
       "           [0.04566595, 0.03721292, 0.04289948, ..., 0.02983955,\n",
       "            0.03039236, 0.        ],\n",
       "           [0.04750687, 0.03752505, 0.04304555, ..., 0.02818947,\n",
       "            0.02884819, 0.02811086]],\n",
       "  \n",
       "          [[1.        , 0.        , 0.        , ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           [0.47554982, 0.5244502 , 0.        , ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           [0.3843401 , 0.3619717 , 0.25368816, ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           ...,\n",
       "           [0.0351921 , 0.050369  , 0.03267369, ..., 0.0151038 ,\n",
       "            0.        , 0.        ],\n",
       "           [0.03565471, 0.0521565 , 0.03325214, ..., 0.01450719,\n",
       "            0.01502436, 0.        ],\n",
       "           [0.03536873, 0.0522887 , 0.03285183, ..., 0.01409413,\n",
       "            0.01465129, 0.01570631]],\n",
       "  \n",
       "          [[1.        , 0.        , 0.        , ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           [0.6392986 , 0.36070138, 0.        , ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           [0.49704072, 0.27265012, 0.23030922, ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           ...,\n",
       "           [0.01794311, 0.00976357, 0.00760115, ..., 0.03961238,\n",
       "            0.        , 0.        ],\n",
       "           [0.01606026, 0.0089839 , 0.00702705, ..., 0.03827707,\n",
       "            0.04258789, 0.        ],\n",
       "           [0.01471802, 0.00843777, 0.00671163, ..., 0.03658761,\n",
       "            0.04041203, 0.04341569]],\n",
       "  \n",
       "          ...,\n",
       "  \n",
       "          [[1.        , 0.        , 0.        , ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           [0.60869426, 0.39130569, 0.        , ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           [0.43226713, 0.25714478, 0.3105881 , ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           ...,\n",
       "           [0.03820875, 0.03402574, 0.04699903, ..., 0.02478153,\n",
       "            0.        , 0.        ],\n",
       "           [0.0379791 , 0.03328098, 0.04622686, ..., 0.02428809,\n",
       "            0.02511641, 0.        ],\n",
       "           [0.03863089, 0.03333006, 0.04660394, ..., 0.02335135,\n",
       "            0.02417238, 0.02383208]],\n",
       "  \n",
       "          [[1.        , 0.        , 0.        , ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           [0.542108  , 0.45789203, 0.        , ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           [0.2881585 , 0.25782982, 0.45401165, ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           ...,\n",
       "           [0.02673351, 0.02950266, 0.05216191, ..., 0.02555417,\n",
       "            0.        , 0.        ],\n",
       "           [0.02625453, 0.02882267, 0.05160959, ..., 0.02536222,\n",
       "            0.0256542 , 0.        ],\n",
       "           [0.02565738, 0.02781675, 0.05028712, ..., 0.02507905,\n",
       "            0.0254542 , 0.02492227]],\n",
       "  \n",
       "          [[1.        , 0.        , 0.        , ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           [0.43766302, 0.562337  , 0.        , ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           [0.26484033, 0.3568571 , 0.37830263, ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           ...,\n",
       "           [0.01386744, 0.02289416, 0.0264657 , ..., 0.02086384,\n",
       "            0.        , 0.        ],\n",
       "           [0.01361029, 0.02246614, 0.02546744, ..., 0.02038159,\n",
       "            0.02022726, 0.        ],\n",
       "           [0.01325001, 0.02224736, 0.0248613 , ..., 0.01986212,\n",
       "            0.0196534 , 0.0195916 ]]]], dtype=float32)>,\n",
       "  'decoder_layer5_block2': <tf.Tensor: shape=(1, 8, 40, 6), dtype=float32, numpy=\n",
       "  array([[[[0.16202869, 0.0992367 , 0.06957606, 0.09255358, 0.14319795,\n",
       "            0.433407  ],\n",
       "           [0.15854701, 0.09194845, 0.06707371, 0.1160778 , 0.15589316,\n",
       "            0.41045988],\n",
       "           [0.14886451, 0.10356005, 0.0807    , 0.13107817, 0.17549354,\n",
       "            0.36030373],\n",
       "           ...,\n",
       "           [0.11208136, 0.17628315, 0.11794075, 0.14681476, 0.18257228,\n",
       "            0.26430768],\n",
       "           [0.10914954, 0.1751132 , 0.11679087, 0.1476119 , 0.1817186 ,\n",
       "            0.2696159 ],\n",
       "           [0.10912329, 0.1745733 , 0.11586146, 0.14588577, 0.18146946,\n",
       "            0.27308676]],\n",
       "  \n",
       "          [[0.08416886, 0.24353863, 0.36804694, 0.13176215, 0.10552509,\n",
       "            0.06695838],\n",
       "           [0.09431879, 0.27993858, 0.3294514 , 0.11460669, 0.11126596,\n",
       "            0.07041863],\n",
       "           [0.08782504, 0.26840943, 0.34720764, 0.12640193, 0.10458872,\n",
       "            0.06556721],\n",
       "           ...,\n",
       "           [0.11194998, 0.27417374, 0.28392306, 0.1271207 , 0.15151897,\n",
       "            0.05131356],\n",
       "           [0.10881222, 0.27915266, 0.2888001 , 0.12429824, 0.14896321,\n",
       "            0.04997354],\n",
       "           [0.10585988, 0.28014848, 0.29329625, 0.12484986, 0.14683537,\n",
       "            0.04901019]],\n",
       "  \n",
       "          [[0.09272328, 0.16919139, 0.25913325, 0.2038534 , 0.14988953,\n",
       "            0.12520917],\n",
       "           [0.10878758, 0.15865006, 0.25797412, 0.18954772, 0.1628413 ,\n",
       "            0.12219924],\n",
       "           [0.10837342, 0.15927596, 0.26418304, 0.1829621 , 0.16168967,\n",
       "            0.12351583],\n",
       "           ...,\n",
       "           [0.14367057, 0.15919995, 0.17579174, 0.1625021 , 0.14678699,\n",
       "            0.21204871],\n",
       "           [0.14358465, 0.15400527, 0.17370486, 0.16591898, 0.14562444,\n",
       "            0.21716185],\n",
       "           [0.14382862, 0.15063994, 0.17374393, 0.1682318 , 0.14559732,\n",
       "            0.2179584 ]],\n",
       "  \n",
       "          ...,\n",
       "  \n",
       "          [[0.17958413, 0.17897473, 0.12063522, 0.11383421, 0.15867192,\n",
       "            0.24829973],\n",
       "           [0.18104762, 0.16993496, 0.11581011, 0.10465162, 0.18787502,\n",
       "            0.24068066],\n",
       "           [0.18120544, 0.16750726, 0.11129155, 0.11411985, 0.20347393,\n",
       "            0.22240199],\n",
       "           ...,\n",
       "           [0.12548009, 0.15283792, 0.17147252, 0.18181276, 0.20100932,\n",
       "            0.16738743],\n",
       "           [0.13157798, 0.15513805, 0.17010556, 0.17611384, 0.20939364,\n",
       "            0.15767089],\n",
       "           [0.13404316, 0.15776366, 0.16974488, 0.1721333 , 0.21613863,\n",
       "            0.15017636]],\n",
       "  \n",
       "          [[0.16467072, 0.15872   , 0.15074474, 0.21641676, 0.15940693,\n",
       "            0.15004078],\n",
       "           [0.1567926 , 0.1726939 , 0.1801797 , 0.18567905, 0.14422302,\n",
       "            0.16043174],\n",
       "           [0.17207943, 0.1778223 , 0.17968066, 0.17679492, 0.14698792,\n",
       "            0.14663477],\n",
       "           ...,\n",
       "           [0.1892452 , 0.14303054, 0.16927993, 0.21437845, 0.1298434 ,\n",
       "            0.15422249],\n",
       "           [0.19157997, 0.1477172 , 0.16569248, 0.21065605, 0.1291953 ,\n",
       "            0.15515898],\n",
       "           [0.19124714, 0.15313216, 0.1656656 , 0.21322353, 0.12846485,\n",
       "            0.14826672]],\n",
       "  \n",
       "          [[0.14018305, 0.3017479 , 0.21655914, 0.165037  , 0.1296043 ,\n",
       "            0.04686861],\n",
       "           [0.13156852, 0.30603862, 0.2293996 , 0.15963186, 0.11910548,\n",
       "            0.0542559 ],\n",
       "           [0.13775784, 0.3097659 , 0.20880064, 0.14822821, 0.12141377,\n",
       "            0.0740336 ],\n",
       "           ...,\n",
       "           [0.19603382, 0.25157237, 0.18248059, 0.16214328, 0.13515262,\n",
       "            0.07261727],\n",
       "           [0.19214028, 0.25900894, 0.18272664, 0.1581955 , 0.13657995,\n",
       "            0.0713487 ],\n",
       "           [0.18966025, 0.26768002, 0.18114075, 0.15305312, 0.13745704,\n",
       "            0.0710088 ]]]], dtype=float32)>,\n",
       "  'decoder_layer6_block1': <tf.Tensor: shape=(1, 8, 40, 40), dtype=float32, numpy=\n",
       "  array([[[[1.        , 0.        , 0.        , ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           [0.42806655, 0.5719335 , 0.        , ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           [0.22831795, 0.32132632, 0.4503557 , ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           ...,\n",
       "           [0.02168286, 0.02435597, 0.03211268, ..., 0.02528447,\n",
       "            0.        , 0.        ],\n",
       "           [0.02129088, 0.02360257, 0.03103147, ..., 0.0245363 ,\n",
       "            0.02407502, 0.        ],\n",
       "           [0.02035673, 0.02261777, 0.03000847, ..., 0.02398252,\n",
       "            0.02356382, 0.023589  ]],\n",
       "  \n",
       "          [[1.        , 0.        , 0.        , ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           [0.62562746, 0.37437254, 0.        , ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           [0.43707973, 0.26912126, 0.29379904, ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           ...,\n",
       "           [0.02766776, 0.01971383, 0.01915671, ..., 0.02121544,\n",
       "            0.        , 0.        ],\n",
       "           [0.0272272 , 0.019413  , 0.01894082, ..., 0.02081663,\n",
       "            0.02028525, 0.        ],\n",
       "           [0.02679153, 0.01900483, 0.01857827, ..., 0.02055762,\n",
       "            0.02002102, 0.01904302]],\n",
       "  \n",
       "          [[1.        , 0.        , 0.        , ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           [0.4499346 , 0.55006546, 0.        , ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           [0.29560974, 0.3655677 , 0.33882257, ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           ...,\n",
       "           [0.01582029, 0.01662367, 0.0174649 , ..., 0.02880255,\n",
       "            0.        , 0.        ],\n",
       "           [0.0151621 , 0.01608661, 0.0168296 , ..., 0.02815522,\n",
       "            0.02690656, 0.        ],\n",
       "           [0.01458954, 0.01557191, 0.0162349 , ..., 0.02753885,\n",
       "            0.02632432, 0.02573128]],\n",
       "  \n",
       "          ...,\n",
       "  \n",
       "          [[1.        , 0.        , 0.        , ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           [0.5337928 , 0.4662072 , 0.        , ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           [0.33479482, 0.31448397, 0.3507212 , ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           ...,\n",
       "           [0.00649152, 0.00723194, 0.00817017, ..., 0.03475008,\n",
       "            0.        , 0.        ],\n",
       "           [0.00648854, 0.00726827, 0.00819024, ..., 0.03343063,\n",
       "            0.03047409, 0.        ],\n",
       "           [0.00635351, 0.00713508, 0.00810669, ..., 0.03239919,\n",
       "            0.02959199, 0.02841197]],\n",
       "  \n",
       "          [[1.        , 0.        , 0.        , ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           [0.5778888 , 0.4221112 , 0.        , ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           [0.4192769 , 0.29152915, 0.28919396, ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           ...,\n",
       "           [0.04515929, 0.03529394, 0.03574451, ..., 0.02434781,\n",
       "            0.        , 0.        ],\n",
       "           [0.04353804, 0.03437039, 0.03478589, ..., 0.02404314,\n",
       "            0.02328285, 0.        ],\n",
       "           [0.04207307, 0.03363517, 0.03405386, ..., 0.02389983,\n",
       "            0.02318227, 0.02220704]],\n",
       "  \n",
       "          [[1.        , 0.        , 0.        , ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           [0.4757381 , 0.52426183, 0.        , ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           [0.2929092 , 0.32862803, 0.3784628 , ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           ...,\n",
       "           [0.00852479, 0.00907582, 0.00949559, ..., 0.03669571,\n",
       "            0.        , 0.        ],\n",
       "           [0.008289  , 0.00870748, 0.00906132, ..., 0.03588992,\n",
       "            0.03490293, 0.        ],\n",
       "           [0.0080615 , 0.0083726 , 0.00866656, ..., 0.03509757,\n",
       "            0.03429994, 0.0331841 ]]]], dtype=float32)>,\n",
       "  'decoder_layer6_block2': <tf.Tensor: shape=(1, 8, 40, 6), dtype=float32, numpy=\n",
       "  array([[[[0.23885186, 0.12458402, 0.08984716, 0.23503524, 0.14192285,\n",
       "            0.16975883],\n",
       "           [0.22927482, 0.11871142, 0.08668144, 0.22111689, 0.14043035,\n",
       "            0.20378506],\n",
       "           [0.2294365 , 0.11724978, 0.09373742, 0.20717245, 0.15017864,\n",
       "            0.20222521],\n",
       "           ...,\n",
       "           [0.22529757, 0.11382868, 0.08217304, 0.14794163, 0.17457591,\n",
       "            0.25618315],\n",
       "           [0.2225059 , 0.11532306, 0.08226497, 0.14812975, 0.17508273,\n",
       "            0.2566936 ],\n",
       "           [0.22121862, 0.11788165, 0.08202191, 0.14766435, 0.17594188,\n",
       "            0.2552716 ]],\n",
       "  \n",
       "          [[0.18927242, 0.13406421, 0.11057849, 0.08885121, 0.14376478,\n",
       "            0.33346888],\n",
       "           [0.18069214, 0.13322976, 0.09262937, 0.08630993, 0.1331305 ,\n",
       "            0.37400833],\n",
       "           [0.17634964, 0.12866972, 0.09989376, 0.08542594, 0.1385302 ,\n",
       "            0.3711307 ],\n",
       "           ...,\n",
       "           [0.20530953, 0.14165576, 0.07586642, 0.08152939, 0.16284108,\n",
       "            0.3327978 ],\n",
       "           [0.20452827, 0.13979453, 0.07602882, 0.08089671, 0.16279079,\n",
       "            0.33596092],\n",
       "           [0.20467682, 0.13909242, 0.07651027, 0.08046552, 0.16371538,\n",
       "            0.3355396 ]],\n",
       "  \n",
       "          [[0.08698685, 0.1342666 , 0.3737433 , 0.18831724, 0.162313  ,\n",
       "            0.05437298],\n",
       "           [0.0694292 , 0.12191056, 0.397255  , 0.179736  , 0.1728203 ,\n",
       "            0.05884897],\n",
       "           [0.07137389, 0.12524614, 0.3788011 , 0.18676966, 0.17609522,\n",
       "            0.06171402],\n",
       "           ...,\n",
       "           [0.09571131, 0.13088338, 0.34656945, 0.18752113, 0.19897322,\n",
       "            0.04034152],\n",
       "           [0.09402572, 0.130403  , 0.34827736, 0.18699121, 0.20015126,\n",
       "            0.04015139],\n",
       "           [0.09398918, 0.1304297 , 0.34815574, 0.18763618, 0.20006989,\n",
       "            0.03971932]],\n",
       "  \n",
       "          ...,\n",
       "  \n",
       "          [[0.11751188, 0.26937765, 0.18247421, 0.11470649, 0.11710327,\n",
       "            0.19882654],\n",
       "           [0.11837944, 0.24882591, 0.19905506, 0.12525459, 0.12363806,\n",
       "            0.18484692],\n",
       "           [0.11784551, 0.24378733, 0.19117458, 0.12221522, 0.11957066,\n",
       "            0.20540671],\n",
       "           ...,\n",
       "           [0.09840338, 0.18503562, 0.14700294, 0.11084667, 0.10111608,\n",
       "            0.35759532],\n",
       "           [0.09699216, 0.18140584, 0.14418125, 0.10784929, 0.09885538,\n",
       "            0.37071604],\n",
       "           [0.09524893, 0.17852753, 0.1400901 , 0.10510062, 0.09741556,\n",
       "            0.38361728]],\n",
       "  \n",
       "          [[0.2251153 , 0.15101133, 0.15401971, 0.19341674, 0.16226235,\n",
       "            0.11417448],\n",
       "           [0.18884905, 0.1585885 , 0.17753847, 0.19113404, 0.1543158 ,\n",
       "            0.12957409],\n",
       "           [0.19632027, 0.15222976, 0.17448096, 0.17608619, 0.15639909,\n",
       "            0.14448367],\n",
       "           ...,\n",
       "           [0.15115274, 0.18010297, 0.21266805, 0.16672444, 0.12537597,\n",
       "            0.1639758 ],\n",
       "           [0.14899804, 0.17733431, 0.21427582, 0.16644353, 0.12559208,\n",
       "            0.16735624],\n",
       "           [0.14796717, 0.17356585, 0.21542273, 0.16801716, 0.12717396,\n",
       "            0.16785307]],\n",
       "  \n",
       "          [[0.15187049, 0.1523843 , 0.1963923 , 0.09207437, 0.09438473,\n",
       "            0.3128938 ],\n",
       "           [0.15115875, 0.16316912, 0.22011477, 0.10275852, 0.11071595,\n",
       "            0.252083  ],\n",
       "           [0.13452561, 0.1455589 , 0.22665425, 0.10656091, 0.09551313,\n",
       "            0.29118714],\n",
       "           ...,\n",
       "           [0.13449918, 0.18803786, 0.23270339, 0.13962832, 0.09934482,\n",
       "            0.20578647],\n",
       "           [0.13368817, 0.18901119, 0.23330863, 0.13923651, 0.09930419,\n",
       "            0.20545138],\n",
       "           [0.13100745, 0.18836603, 0.23445696, 0.13892126, 0.09939884,\n",
       "            0.20784944]]]], dtype=float32)>})"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_chi_to_eng(transformer_1, \"你好嗎?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_chi_to_eng(transformer, sentence):\n",
    "    result, attention_weights = evaluate_chi_to_eng(transformer, sentence)\n",
    "\n",
    "    predicted_sentence = tokenizer_en_1.decode([i for i in result\n",
    "                                              if i < tokenizer_en_1.vocab_size])\n",
    "\n",
    "#     print('Input: {}'.format(sentence))\n",
    "#     print('Predicted Translation: {}'.format(predicted_sentence))\n",
    "\n",
    "    \n",
    "    return predicted_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'carried carried carried carried carried carried carried carried carried carried carried carried carried carried carried carried carried carried carried carried carried carried carried carried carried carried carried carried carried carried carried carried carried carried carried carried carried carried carried carried '"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate_chi_to_eng(transformer_1, \"你好嗎?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x23cc00f0048>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer_chi_to_eng = Transformer(config=config,\n",
    "                          target_vocab_size=target_vocab_size_1,\n",
    "                          bert_config_file=bert_config_file)\n",
    "  \n",
    "fn_out, _ = transformer_chi_to_eng(inp, tar_inp, \n",
    "                        True,\n",
    "                        look_ahead_mask=None,\n",
    "                        dec_padding_mask=None)\n",
    "transformer_chi_to_eng.load_weights('/weights/nmt_chi_2_en_bert_ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'were you done ?'"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate_chi_to_eng(transformer_chi_to_eng, \"你好嗎?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'where are you going to go ?'"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate_chi_to_eng(transformer_chi_to_eng,  '你要去哪裡？')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mary knew mary s happy .'"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate_chi_to_eng(transformer_chi_to_eng, '很高興認識你')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_ques(en):\n",
    "    tokens_en = tokenizer_ques.tokenize(en)\n",
    "    lang1 = tokenizer_ques.convert_tokens_to_ids(['[CLS]'] + tokens_en + ['[SEP]'])\n",
    "    return lang1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_chatbot(transformer, inp_sentence):\n",
    "    # normalize input sentence\n",
    "    inp_sentence = encode_ques(inp_sentence)\n",
    "    encoder_input = tf.expand_dims(inp_sentence, 0)\n",
    "\n",
    "    # as the target is english, the first word to the transformer should be the\n",
    "    # english start token.\n",
    "    decoder_input = [tokenizer_ans.vocab_size]\n",
    "    output = tf.expand_dims(decoder_input, 0)\n",
    "\n",
    "    for i in range(MAX_SEQ_LENGTH):\n",
    "        combined_mask, dec_padding_mask = create_masks(\n",
    "            encoder_input, output)\n",
    "\n",
    "        # predictions.shape == (batch_size, seq_len, vocab_size)\n",
    "        predictions, attention_weights = transformer(encoder_input,\n",
    "                                                     output,\n",
    "                                                     False,\n",
    "                                                     combined_mask,\n",
    "                                                     dec_padding_mask)\n",
    "\n",
    "        # select the last word from the seq_len dimension\n",
    "        predictions = predictions[:, -1:, :]  # (batch_size, 1, vocab_size)\n",
    "\n",
    "        predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
    "\n",
    "        # return the result if the predicted_id is equal to the end token\n",
    "        if tf.equal(predicted_id, tokenizer_ans.vocab_size + 1):\n",
    "            return tf.squeeze(output, axis=0), attention_weights\n",
    "\n",
    "        # concatentate the predicted_id to the output which is given to the decoder\n",
    "        # as its input.\n",
    "        output = tf.concat([output, predicted_id], axis=-1)\n",
    "\n",
    "    return tf.squeeze(output, axis=0), attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(41,), dtype=int32, numpy=\n",
       " array([8025, 2413, 2413, 2413, 2413, 2413, 2413, 2413, 2413, 2413, 2413,\n",
       "        2413, 2413, 2413, 2413, 2413, 2413, 2413, 2413, 2413, 2413, 2413,\n",
       "        2413, 2413, 2413, 2413, 2413, 2413, 2413, 2413, 2413, 2413, 2413,\n",
       "        2413, 2413, 2413, 2413, 2413, 2413, 2413, 2413])>,\n",
       " {'decoder_layer1_block1': <tf.Tensor: shape=(1, 8, 40, 40), dtype=float32, numpy=\n",
       "  array([[[[1.        , 0.        , 0.        , ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           [0.92084986, 0.0791501 , 0.        , ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           [0.8569223 , 0.0721079 , 0.07096978, ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           ...,\n",
       "           [0.5055334 , 0.02431858, 0.02304769, ..., 0.01422325,\n",
       "            0.        , 0.        ],\n",
       "           [0.4639414 , 0.02467237, 0.02462771, ..., 0.01487065,\n",
       "            0.01812332, 0.        ],\n",
       "           [0.4304367 , 0.02425041, 0.02474623, ..., 0.01604094,\n",
       "            0.01916436, 0.02274786]],\n",
       "  \n",
       "          [[1.        , 0.        , 0.        , ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           [0.37935105, 0.6206489 , 0.        , ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           [0.270447  , 0.37687042, 0.3526826 , ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           ...,\n",
       "           [0.0411268 , 0.02987323, 0.03061915, ..., 0.0345088 ,\n",
       "            0.        , 0.        ],\n",
       "           [0.03737989, 0.03149939, 0.03214709, ..., 0.03429056,\n",
       "            0.03388229, 0.        ],\n",
       "           [0.0349775 , 0.03142844, 0.03205159, ..., 0.03447931,\n",
       "            0.03393577, 0.03581005]],\n",
       "  \n",
       "          [[1.        , 0.        , 0.        , ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           [0.7086665 , 0.2913335 , 0.        , ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           [0.57207286, 0.20375662, 0.22417048, ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           ...,\n",
       "           [0.05912087, 0.02289424, 0.02369387, ..., 0.02558713,\n",
       "            0.        , 0.        ],\n",
       "           [0.07430308, 0.0237986 , 0.0258279 , ..., 0.02293981,\n",
       "            0.02215661, 0.        ],\n",
       "           [0.09343053, 0.02409482, 0.02719301, ..., 0.02044433,\n",
       "            0.01950715, 0.02125471]],\n",
       "  \n",
       "          ...,\n",
       "  \n",
       "          [[1.        , 0.        , 0.        , ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           [0.4181365 , 0.58186346, 0.        , ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           [0.26006174, 0.34306   , 0.39687827, ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           ...,\n",
       "           [0.01030847, 0.01296801, 0.01373474, ..., 0.02131412,\n",
       "            0.        , 0.        ],\n",
       "           [0.00877194, 0.01291197, 0.01403626, ..., 0.02143938,\n",
       "            0.02035003, 0.        ],\n",
       "           [0.00694129, 0.01273359, 0.0143916 , ..., 0.02152432,\n",
       "            0.02039329, 0.01747281]],\n",
       "  \n",
       "          [[1.        , 0.        , 0.        , ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           [0.3877124 , 0.6122876 , 0.        , ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           [0.27683607, 0.3461202 , 0.37704372, ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           ...,\n",
       "           [0.02369476, 0.02196861, 0.02101121, ..., 0.02396734,\n",
       "            0.        , 0.        ],\n",
       "           [0.02700953, 0.01977792, 0.01873677, ..., 0.02415238,\n",
       "            0.01800187, 0.        ],\n",
       "           [0.03167893, 0.01794405, 0.01666416, ..., 0.02486706,\n",
       "            0.01935799, 0.01543827]],\n",
       "  \n",
       "          [[1.        , 0.        , 0.        , ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           [0.26392457, 0.73607546, 0.        , ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           [0.13459484, 0.42526245, 0.44014272, ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           ...,\n",
       "           [0.01165597, 0.0602513 , 0.06077321, ..., 0.02504868,\n",
       "            0.        , 0.        ],\n",
       "           [0.0106789 , 0.05586963, 0.05414848, ..., 0.02661532,\n",
       "            0.02718087, 0.        ],\n",
       "           [0.00985619, 0.04941614, 0.04698335, ..., 0.02848887,\n",
       "            0.02961715, 0.02791731]]]], dtype=float32)>,\n",
       "  'decoder_layer1_block2': <tf.Tensor: shape=(1, 8, 40, 6), dtype=float32, numpy=\n",
       "  array([[[[0.07261358, 0.15682206, 0.2188563 , 0.30718875, 0.07519238,\n",
       "            0.16932687],\n",
       "           [0.14490268, 0.1354002 , 0.22094539, 0.23122908, 0.05845552,\n",
       "            0.20906718],\n",
       "           [0.16095416, 0.13802694, 0.23344941, 0.21242778, 0.06134241,\n",
       "            0.19379929],\n",
       "           ...,\n",
       "           [0.1998139 , 0.10883117, 0.16360481, 0.17966962, 0.08306315,\n",
       "            0.26501727],\n",
       "           [0.18889359, 0.11771721, 0.16601497, 0.18504743, 0.07486264,\n",
       "            0.26746422],\n",
       "           [0.1842785 , 0.12852992, 0.16156939, 0.18731165, 0.06963349,\n",
       "            0.26867703]],\n",
       "  \n",
       "          [[0.1830722 , 0.14877263, 0.24804752, 0.1714358 , 0.10984731,\n",
       "            0.13882458],\n",
       "           [0.31516623, 0.07793567, 0.12191978, 0.10491516, 0.22216365,\n",
       "            0.15789959],\n",
       "           [0.30840492, 0.08304604, 0.12520497, 0.10401089, 0.21313448,\n",
       "            0.16619863],\n",
       "           ...,\n",
       "           [0.1535585 , 0.06741308, 0.14118147, 0.15459181, 0.23265436,\n",
       "            0.25060079],\n",
       "           [0.16086417, 0.06747349, 0.12413333, 0.13776465, 0.24090074,\n",
       "            0.26886365],\n",
       "           [0.17542869, 0.06768923, 0.1066278 , 0.12158357, 0.25086161,\n",
       "            0.2778091 ]],\n",
       "  \n",
       "          [[0.21280833, 0.06675351, 0.07445929, 0.16005813, 0.13724262,\n",
       "            0.34867814],\n",
       "           [0.3629007 , 0.08649785, 0.09699722, 0.16737986, 0.07819701,\n",
       "            0.20802733],\n",
       "           [0.37993634, 0.09615341, 0.09547889, 0.16540551, 0.07857733,\n",
       "            0.18444851],\n",
       "           ...,\n",
       "           [0.33638096, 0.13478434, 0.09507382, 0.13388309, 0.07993968,\n",
       "            0.21993805],\n",
       "           [0.35493514, 0.12434488, 0.08437467, 0.11552414, 0.08341105,\n",
       "            0.23741016],\n",
       "           [0.37742564, 0.11550893, 0.07716972, 0.10528062, 0.08628763,\n",
       "            0.23832747]],\n",
       "  \n",
       "          ...,\n",
       "  \n",
       "          [[0.1953652 , 0.09156018, 0.11413249, 0.13350797, 0.30822423,\n",
       "            0.15720995],\n",
       "           [0.19298846, 0.08370891, 0.11077807, 0.16916288, 0.16251351,\n",
       "            0.28084815],\n",
       "           [0.2092247 , 0.07680427, 0.10726046, 0.17319492, 0.15580645,\n",
       "            0.27770925],\n",
       "           ...,\n",
       "           [0.13720356, 0.06714148, 0.12225852, 0.216957  , 0.13546643,\n",
       "            0.32097307],\n",
       "           [0.12779295, 0.07362518, 0.12595695, 0.2095351 , 0.12005974,\n",
       "            0.34303012],\n",
       "           [0.12739295, 0.08012814, 0.13214302, 0.20079568, 0.11876589,\n",
       "            0.34077436]],\n",
       "  \n",
       "          [[0.04657566, 0.14351021, 0.24345419, 0.27363428, 0.18712646,\n",
       "            0.10569923],\n",
       "           [0.04167708, 0.30999905, 0.25446695, 0.24306487, 0.0926538 ,\n",
       "            0.05813821],\n",
       "           [0.0483669 , 0.31234807, 0.24769562, 0.24256057, 0.09207486,\n",
       "            0.05695391],\n",
       "           ...,\n",
       "           [0.05762887, 0.32622305, 0.2301818 , 0.23148881, 0.08927588,\n",
       "            0.06520159],\n",
       "           [0.05250946, 0.3385437 , 0.23130617, 0.2350467 , 0.08156676,\n",
       "            0.06102726],\n",
       "           [0.04773923, 0.32807162, 0.2412294 , 0.24560568, 0.07453992,\n",
       "            0.06281417]],\n",
       "  \n",
       "          [[0.26955852, 0.13873005, 0.1051959 , 0.18897083, 0.21501566,\n",
       "            0.08252897],\n",
       "           [0.13571435, 0.16019776, 0.1857674 , 0.26104543, 0.10824327,\n",
       "            0.14903173],\n",
       "           [0.13069908, 0.16533755, 0.18598433, 0.2577939 , 0.11282484,\n",
       "            0.1473603 ],\n",
       "           ...,\n",
       "           [0.15014416, 0.29644707, 0.18930781, 0.19811355, 0.06606958,\n",
       "            0.09991791],\n",
       "           [0.15478918, 0.26427662, 0.19809923, 0.22105151, 0.06738747,\n",
       "            0.09439602],\n",
       "           [0.17803408, 0.21578763, 0.19438371, 0.23675689, 0.0770483 ,\n",
       "            0.0979894 ]]]], dtype=float32)>,\n",
       "  'decoder_layer2_block1': <tf.Tensor: shape=(1, 8, 40, 40), dtype=float32, numpy=\n",
       "  array([[[[1.        , 0.        , 0.        , ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           [0.36489803, 0.635102  , 0.        , ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           [0.23662896, 0.37392694, 0.3894441 , ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           ...,\n",
       "           [0.00968569, 0.01967502, 0.02311791, ..., 0.01705412,\n",
       "            0.        , 0.        ],\n",
       "           [0.01098331, 0.02056479, 0.02330107, ..., 0.01659334,\n",
       "            0.0184434 , 0.        ],\n",
       "           [0.01388049, 0.02176938, 0.02417458, ..., 0.01608212,\n",
       "            0.01765374, 0.02018654]],\n",
       "  \n",
       "          [[1.        , 0.        , 0.        , ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           [0.6114731 , 0.38852692, 0.        , ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           [0.43786275, 0.2701381 , 0.2919991 , ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           ...,\n",
       "           [0.02092286, 0.02417329, 0.02508811, ..., 0.0179828 ,\n",
       "            0.        , 0.        ],\n",
       "           [0.02169686, 0.02266288, 0.02347521, ..., 0.01866505,\n",
       "            0.02326439, 0.        ],\n",
       "           [0.02390841, 0.02112077, 0.02156577, ..., 0.01845269,\n",
       "            0.0226423 , 0.02741869]],\n",
       "  \n",
       "          [[1.        , 0.        , 0.        , ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           [0.25177467, 0.74822533, 0.        , ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           [0.14434527, 0.37049115, 0.48516366, ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           ...,\n",
       "           [0.004017  , 0.01845535, 0.02424694, ..., 0.02551927,\n",
       "            0.        , 0.        ],\n",
       "           [0.00374255, 0.01730549, 0.02255103, ..., 0.02510295,\n",
       "            0.02704682, 0.        ],\n",
       "           [0.00424162, 0.01774193, 0.02245055, ..., 0.0259132 ,\n",
       "            0.02793822, 0.02815401]],\n",
       "  \n",
       "          ...,\n",
       "  \n",
       "          [[1.        , 0.        , 0.        , ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           [0.74561965, 0.25438038, 0.        , ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           [0.57075316, 0.23468469, 0.19456214, ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           ...,\n",
       "           [0.01429989, 0.0138073 , 0.01379861, ..., 0.01687483,\n",
       "            0.        , 0.        ],\n",
       "           [0.01587737, 0.01291208, 0.01250602, ..., 0.01705302,\n",
       "            0.02034889, 0.        ],\n",
       "           [0.01760242, 0.01230867, 0.01168507, ..., 0.018074  ,\n",
       "            0.02124145, 0.02351553]],\n",
       "  \n",
       "          [[1.        , 0.        , 0.        , ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           [0.5711833 , 0.42881668, 0.        , ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           [0.46562558, 0.281778  , 0.2525965 , ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           ...,\n",
       "           [0.02586427, 0.01717671, 0.01518157, ..., 0.01082397,\n",
       "            0.        , 0.        ],\n",
       "           [0.02554184, 0.01639493, 0.01431725, ..., 0.01091762,\n",
       "            0.01214253, 0.        ],\n",
       "           [0.02606446, 0.01548991, 0.01392357, ..., 0.01080372,\n",
       "            0.01187382, 0.01279909]],\n",
       "  \n",
       "          [[1.        , 0.        , 0.        , ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           [0.2235647 , 0.7764353 , 0.        , ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           [0.13767774, 0.39049968, 0.47182262, ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           ...,\n",
       "           [0.01741357, 0.03135567, 0.03693154, ..., 0.01565614,\n",
       "            0.        , 0.        ],\n",
       "           [0.01456416, 0.03062372, 0.0366586 , ..., 0.01442572,\n",
       "            0.01571888, 0.        ],\n",
       "           [0.01263172, 0.0312768 , 0.03768544, ..., 0.0125747 ,\n",
       "            0.01419217, 0.01925745]]]], dtype=float32)>,\n",
       "  'decoder_layer2_block2': <tf.Tensor: shape=(1, 8, 40, 6), dtype=float32, numpy=\n",
       "  array([[[[0.35437292, 0.20635927, 0.09319396, 0.09536935, 0.10426898,\n",
       "            0.14643553],\n",
       "           [0.21507327, 0.30642197, 0.09301995, 0.11058627, 0.13281924,\n",
       "            0.14207931],\n",
       "           [0.17956206, 0.3495417 , 0.09332657, 0.11861846, 0.1186415 ,\n",
       "            0.14030969],\n",
       "           ...,\n",
       "           [0.24126601, 0.36725816, 0.08416279, 0.12117017, 0.11647271,\n",
       "            0.06967017],\n",
       "           [0.2418525 , 0.37480626, 0.08129884, 0.11809631, 0.11078615,\n",
       "            0.07315985],\n",
       "           [0.23591389, 0.36690965, 0.0811829 , 0.11851568, 0.11515894,\n",
       "            0.08231889]],\n",
       "  \n",
       "          [[0.24710025, 0.31145647, 0.13312948, 0.07840901, 0.16260108,\n",
       "            0.06730373],\n",
       "           [0.22417216, 0.27649727, 0.16339664, 0.10780232, 0.08929692,\n",
       "            0.13883464],\n",
       "           [0.21410587, 0.271234  , 0.18381555, 0.11534902, 0.08249408,\n",
       "            0.1330014 ],\n",
       "           ...,\n",
       "           [0.171054  , 0.15675189, 0.1503145 , 0.18244277, 0.07349154,\n",
       "            0.26594532],\n",
       "           [0.15926746, 0.16359235, 0.15499902, 0.18029188, 0.07207827,\n",
       "            0.26977104],\n",
       "           [0.14900334, 0.16856109, 0.1607795 , 0.17219603, 0.07565809,\n",
       "            0.27380198]],\n",
       "  \n",
       "          [[0.23862709, 0.19996947, 0.23100905, 0.11080064, 0.1314369 ,\n",
       "            0.08815684],\n",
       "           [0.29047728, 0.19001162, 0.14639305, 0.09130774, 0.12989311,\n",
       "            0.15191716],\n",
       "           [0.33148393, 0.19236052, 0.12966013, 0.0821739 , 0.12139091,\n",
       "            0.14293054],\n",
       "           ...,\n",
       "           [0.37835637, 0.2616005 , 0.10223609, 0.08008508, 0.08585776,\n",
       "            0.09186419],\n",
       "           [0.35689718, 0.26850688, 0.11253126, 0.0855517 , 0.08304854,\n",
       "            0.09346452],\n",
       "           [0.34201044, 0.2703846 , 0.12120365, 0.09326531, 0.08386807,\n",
       "            0.08926792]],\n",
       "  \n",
       "          ...,\n",
       "  \n",
       "          [[0.1807873 , 0.20627645, 0.14412276, 0.21570514, 0.07043802,\n",
       "            0.18267037],\n",
       "           [0.15561055, 0.15654224, 0.11382232, 0.16072524, 0.13805537,\n",
       "            0.2752443 ],\n",
       "           [0.18350898, 0.1539021 , 0.11960765, 0.16703123, 0.1557021 ,\n",
       "            0.22024785],\n",
       "           ...,\n",
       "           [0.14547196, 0.1562005 , 0.13438472, 0.20307955, 0.18201117,\n",
       "            0.1788521 ],\n",
       "           [0.16223599, 0.1561939 , 0.13848108, 0.20839715, 0.16018775,\n",
       "            0.17450413],\n",
       "           [0.16990106, 0.1569852 , 0.1408006 , 0.21416639, 0.14547344,\n",
       "            0.17267334]],\n",
       "  \n",
       "          [[0.32060888, 0.09823455, 0.07753961, 0.14850298, 0.14339758,\n",
       "            0.21171643],\n",
       "           [0.23756254, 0.09040222, 0.10444674, 0.18245202, 0.16146766,\n",
       "            0.22366884],\n",
       "           [0.24880755, 0.09038463, 0.10349198, 0.18919069, 0.1528124 ,\n",
       "            0.21531278],\n",
       "           ...,\n",
       "           [0.17503323, 0.13067879, 0.17603156, 0.257503  , 0.1259017 ,\n",
       "            0.13485168],\n",
       "           [0.17518243, 0.13581656, 0.17766696, 0.27009103, 0.11822145,\n",
       "            0.12302154],\n",
       "           [0.18294948, 0.13338155, 0.17217754, 0.2760157 , 0.11434808,\n",
       "            0.12112772]],\n",
       "  \n",
       "          [[0.39646062, 0.1677146 , 0.07021182, 0.0939713 , 0.14564854,\n",
       "            0.12599316],\n",
       "           [0.22821127, 0.16081594, 0.12006672, 0.10598291, 0.14910172,\n",
       "            0.23582138],\n",
       "           [0.20873208, 0.1588444 , 0.12582155, 0.10777369, 0.15000848,\n",
       "            0.24881977],\n",
       "           ...,\n",
       "           [0.15848671, 0.11476539, 0.15714122, 0.10913188, 0.19842485,\n",
       "            0.26205   ],\n",
       "           [0.1752948 , 0.11310475, 0.15836976, 0.11061404, 0.19437844,\n",
       "            0.24823816],\n",
       "           [0.19121851, 0.11078811, 0.15667853, 0.10854333, 0.19424778,\n",
       "            0.2385237 ]]]], dtype=float32)>,\n",
       "  'decoder_layer3_block1': <tf.Tensor: shape=(1, 8, 40, 40), dtype=float32, numpy=\n",
       "  array([[[[1.        , 0.        , 0.        , ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           [0.50062543, 0.49937457, 0.        , ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           [0.30583385, 0.36543053, 0.3287356 , ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           ...,\n",
       "           [0.05376604, 0.04872986, 0.0411975 , ..., 0.02410055,\n",
       "            0.        , 0.        ],\n",
       "           [0.05394352, 0.04799523, 0.04055277, ..., 0.02304234,\n",
       "            0.02526042, 0.        ],\n",
       "           [0.05094231, 0.04755048, 0.04025407, ..., 0.02238891,\n",
       "            0.02484453, 0.0243674 ]],\n",
       "  \n",
       "          [[1.        , 0.        , 0.        , ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           [0.6335881 , 0.36641192, 0.        , ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           [0.45859274, 0.28639787, 0.2550094 , ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           ...,\n",
       "           [0.03971174, 0.0235361 , 0.02251588, ..., 0.03103215,\n",
       "            0.        , 0.        ],\n",
       "           [0.03948054, 0.02257791, 0.02172348, ..., 0.03053657,\n",
       "            0.02899979, 0.        ],\n",
       "           [0.04011384, 0.02338963, 0.02255336, ..., 0.02955558,\n",
       "            0.02801648, 0.0273204 ]],\n",
       "  \n",
       "          [[1.        , 0.        , 0.        , ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           [0.32087445, 0.67912555, 0.        , ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           [0.16778073, 0.38961613, 0.44260314, ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           ...,\n",
       "           [0.00356891, 0.01210995, 0.01573491, ..., 0.02046225,\n",
       "            0.        , 0.        ],\n",
       "           [0.00387199, 0.0119715 , 0.01512761, ..., 0.02062547,\n",
       "            0.02058305, 0.        ],\n",
       "           [0.00422122, 0.01178699, 0.0145375 , ..., 0.02006885,\n",
       "            0.02018022, 0.02003717]],\n",
       "  \n",
       "          ...,\n",
       "  \n",
       "          [[1.        , 0.        , 0.        , ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           [0.5550086 , 0.4449914 , 0.        , ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           [0.41013637, 0.2813773 , 0.3084864 , ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           ...,\n",
       "           [0.03487955, 0.02041335, 0.02290386, ..., 0.02605335,\n",
       "            0.        , 0.        ],\n",
       "           [0.02995818, 0.01934997, 0.02224117, ..., 0.02534731,\n",
       "            0.02492435, 0.        ],\n",
       "           [0.02578489, 0.01830289, 0.02151617, ..., 0.02490281,\n",
       "            0.02472093, 0.02260113]],\n",
       "  \n",
       "          [[1.        , 0.        , 0.        , ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           [0.6084807 , 0.3915193 , 0.        , ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           [0.43413535, 0.2648642 , 0.30100042, ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           ...,\n",
       "           [0.05601383, 0.03353283, 0.03143233, ..., 0.02692126,\n",
       "            0.        , 0.        ],\n",
       "           [0.05985284, 0.03352219, 0.03073541, ..., 0.02589755,\n",
       "            0.02915274, 0.        ],\n",
       "           [0.06124723, 0.03266433, 0.02979172, ..., 0.02488329,\n",
       "            0.02770262, 0.02993997]],\n",
       "  \n",
       "          [[1.        , 0.        , 0.        , ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           [0.5659958 , 0.4340042 , 0.        , ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           [0.3572822 , 0.31777683, 0.32494095, ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           ...,\n",
       "           [0.03031593, 0.02220804, 0.02023678, ..., 0.02499734,\n",
       "            0.        , 0.        ],\n",
       "           [0.03156585, 0.02233068, 0.02023863, ..., 0.02401455,\n",
       "            0.02274801, 0.        ],\n",
       "           [0.02995585, 0.02150126, 0.01965375, ..., 0.02366344,\n",
       "            0.02227592, 0.0206639 ]]]], dtype=float32)>,\n",
       "  'decoder_layer3_block2': <tf.Tensor: shape=(1, 8, 40, 6), dtype=float32, numpy=\n",
       "  array([[[[0.1088609 , 0.24899058, 0.21495432, 0.2078708 , 0.10846512,\n",
       "            0.11085827],\n",
       "           [0.10418849, 0.2147185 , 0.19389442, 0.21399544, 0.10542803,\n",
       "            0.16777508],\n",
       "           [0.09463733, 0.20273952, 0.19703591, 0.22269866, 0.1111866 ,\n",
       "            0.17170203],\n",
       "           ...,\n",
       "           [0.1196428 , 0.18863162, 0.1771243 , 0.21900785, 0.12655312,\n",
       "            0.16904034],\n",
       "           [0.12189993, 0.1761332 , 0.1755683 , 0.22106023, 0.12789531,\n",
       "            0.17744301],\n",
       "           [0.12507841, 0.16672614, 0.17136207, 0.21728745, 0.13031225,\n",
       "            0.18923372]],\n",
       "  \n",
       "          [[0.22185725, 0.10156874, 0.08312685, 0.10656986, 0.36676693,\n",
       "            0.12011039],\n",
       "           [0.24047586, 0.08798929, 0.09579977, 0.11305049, 0.34850973,\n",
       "            0.11417489],\n",
       "           [0.24355577, 0.08602881, 0.09724833, 0.11599012, 0.33164483,\n",
       "            0.12553212],\n",
       "           ...,\n",
       "           [0.26922643, 0.08832059, 0.10737577, 0.16825253, 0.25029245,\n",
       "            0.1165322 ],\n",
       "           [0.27036673, 0.09292538, 0.10655308, 0.15883732, 0.25765973,\n",
       "            0.11365771],\n",
       "           [0.26761112, 0.09532799, 0.10566674, 0.15247923, 0.26748043,\n",
       "            0.11143441]],\n",
       "  \n",
       "          [[0.17216583, 0.10628304, 0.13965972, 0.278327  , 0.24263443,\n",
       "            0.06093   ],\n",
       "           [0.12783514, 0.11505153, 0.16929492, 0.29086012, 0.21869668,\n",
       "            0.07826166],\n",
       "           [0.12157387, 0.11923075, 0.17720583, 0.27594534, 0.2284337 ,\n",
       "            0.0776105 ],\n",
       "           ...,\n",
       "           [0.09126829, 0.10972004, 0.15619247, 0.24958313, 0.28861007,\n",
       "            0.10462594],\n",
       "           [0.09146523, 0.11195787, 0.15618584, 0.250397  , 0.28554845,\n",
       "            0.10444552],\n",
       "           [0.09027272, 0.11531823, 0.16235253, 0.25348604, 0.27208284,\n",
       "            0.10648759]],\n",
       "  \n",
       "          ...,\n",
       "  \n",
       "          [[0.1868739 , 0.11791421, 0.2237669 , 0.20959392, 0.14681296,\n",
       "            0.11503813],\n",
       "           [0.1343322 , 0.12194238, 0.29547513, 0.22306514, 0.0972692 ,\n",
       "            0.12791592],\n",
       "           [0.12836295, 0.12763453, 0.29953635, 0.21764943, 0.10106681,\n",
       "            0.12574993],\n",
       "           ...,\n",
       "           [0.1139182 , 0.16024171, 0.31466848, 0.18934569, 0.10795374,\n",
       "            0.11387216],\n",
       "           [0.11300224, 0.15939319, 0.31383815, 0.18872878, 0.10738244,\n",
       "            0.11765525],\n",
       "           [0.11323424, 0.1569175 , 0.31040964, 0.19133468, 0.10717323,\n",
       "            0.12093069]],\n",
       "  \n",
       "          [[0.1016657 , 0.1733303 , 0.15933591, 0.28729138, 0.20627919,\n",
       "            0.07209752],\n",
       "           [0.10682781, 0.15287003, 0.1694205 , 0.3176413 , 0.1809857 ,\n",
       "            0.07225465],\n",
       "           [0.12053034, 0.1441574 , 0.15937027, 0.29725394, 0.19752526,\n",
       "            0.0811628 ],\n",
       "           ...,\n",
       "           [0.08886661, 0.1007774 , 0.16726269, 0.25437257, 0.30855182,\n",
       "            0.08016898],\n",
       "           [0.0911108 , 0.10185397, 0.16659388, 0.25146323, 0.30779827,\n",
       "            0.08117983],\n",
       "           [0.09184031, 0.10523559, 0.16597421, 0.25021142, 0.30247918,\n",
       "            0.08425931]],\n",
       "  \n",
       "          [[0.20997566, 0.18468572, 0.24404348, 0.21963522, 0.08899286,\n",
       "            0.05266714],\n",
       "           [0.1661985 , 0.20223811, 0.13075954, 0.23724094, 0.17717524,\n",
       "            0.08638763],\n",
       "           [0.15505618, 0.20572703, 0.11940348, 0.24040212, 0.19336563,\n",
       "            0.08604553],\n",
       "           ...,\n",
       "           [0.14757593, 0.20456952, 0.10369465, 0.28429124, 0.16372547,\n",
       "            0.0961432 ],\n",
       "           [0.1379828 , 0.21907513, 0.10884092, 0.29335594, 0.15005025,\n",
       "            0.09069492],\n",
       "           [0.13432439, 0.23273386, 0.11136773, 0.28872785, 0.14311221,\n",
       "            0.08973396]]]], dtype=float32)>,\n",
       "  'decoder_layer4_block1': <tf.Tensor: shape=(1, 8, 40, 40), dtype=float32, numpy=\n",
       "  array([[[[1.        , 0.        , 0.        , ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           [0.53972226, 0.46027777, 0.        , ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           [0.41102642, 0.33579135, 0.25318223, ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           ...,\n",
       "           [0.08461299, 0.06302813, 0.04584467, ..., 0.01801339,\n",
       "            0.        , 0.        ],\n",
       "           [0.08187894, 0.06065882, 0.04459812, ..., 0.01756823,\n",
       "            0.01751167, 0.        ],\n",
       "           [0.08079798, 0.05744699, 0.04292464, ..., 0.01711813,\n",
       "            0.01718863, 0.01694595]],\n",
       "  \n",
       "          [[1.        , 0.        , 0.        , ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           [0.6278833 , 0.37211663, 0.        , ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           [0.46172294, 0.2620896 , 0.27618748, ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           ...,\n",
       "           [0.03204861, 0.01822922, 0.01967308, ..., 0.0255132 ,\n",
       "            0.        , 0.        ],\n",
       "           [0.03238508, 0.01783867, 0.0191666 , ..., 0.02502234,\n",
       "            0.02617196, 0.        ],\n",
       "           [0.03257726, 0.01741393, 0.01871703, ..., 0.02470248,\n",
       "            0.02579744, 0.02556943]],\n",
       "  \n",
       "          [[1.        , 0.        , 0.        , ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           [0.4136733 , 0.5863267 , 0.        , ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           [0.28050575, 0.36934045, 0.3501538 , ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           ...,\n",
       "           [0.02051702, 0.02575967, 0.02408249, ..., 0.03330025,\n",
       "            0.        , 0.        ],\n",
       "           [0.02035946, 0.02537666, 0.02374676, ..., 0.0319152 ,\n",
       "            0.03210764, 0.        ],\n",
       "           [0.02026064, 0.02535538, 0.02360717, ..., 0.03065169,\n",
       "            0.03081035, 0.03054244]],\n",
       "  \n",
       "          ...,\n",
       "  \n",
       "          [[1.        , 0.        , 0.        , ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           [0.5307048 , 0.4692952 , 0.        , ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           [0.3620346 , 0.3094965 , 0.32846886, ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           ...,\n",
       "           [0.02438487, 0.02598311, 0.02514944, ..., 0.02724979,\n",
       "            0.        , 0.        ],\n",
       "           [0.02434876, 0.02519272, 0.02411777, ..., 0.02644772,\n",
       "            0.02493599, 0.        ],\n",
       "           [0.024779  , 0.0245409 , 0.02323384, ..., 0.02570519,\n",
       "            0.02436982, 0.02253422]],\n",
       "  \n",
       "          [[1.        , 0.        , 0.        , ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           [0.58914363, 0.4108563 , 0.        , ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           [0.3941526 , 0.31168348, 0.29416397, ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           ...,\n",
       "           [0.02741456, 0.02993399, 0.02801536, ..., 0.02391701,\n",
       "            0.        , 0.        ],\n",
       "           [0.02735822, 0.02881057, 0.02707512, ..., 0.0235134 ,\n",
       "            0.02515517, 0.        ],\n",
       "           [0.02776012, 0.02757733, 0.02611064, ..., 0.02286199,\n",
       "            0.02450564, 0.02641511]],\n",
       "  \n",
       "          [[1.        , 0.        , 0.        , ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           [0.5682063 , 0.43179366, 0.        , ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           [0.3770929 , 0.3085854 , 0.31432167, ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           ...,\n",
       "           [0.02502966, 0.02592734, 0.02654121, ..., 0.03173676,\n",
       "            0.        , 0.        ],\n",
       "           [0.02435826, 0.02562499, 0.02604978, ..., 0.03045637,\n",
       "            0.03074351, 0.        ],\n",
       "           [0.02378755, 0.02549016, 0.02581449, ..., 0.02918047,\n",
       "            0.02952761, 0.03019887]]]], dtype=float32)>,\n",
       "  'decoder_layer4_block2': <tf.Tensor: shape=(1, 8, 40, 6), dtype=float32, numpy=\n",
       "  array([[[[0.14491911, 0.07591163, 0.13601865, 0.07758997, 0.27831978,\n",
       "            0.28724086],\n",
       "           [0.1439707 , 0.06216945, 0.12061325, 0.08746706, 0.30751044,\n",
       "            0.27826905],\n",
       "           [0.14724253, 0.06163056, 0.12242575, 0.08584017, 0.27407146,\n",
       "            0.30878955],\n",
       "           ...,\n",
       "           [0.17352921, 0.07422378, 0.10688757, 0.07189893, 0.27765337,\n",
       "            0.2958071 ],\n",
       "           [0.18013453, 0.07242799, 0.10656891, 0.07148448, 0.27847636,\n",
       "            0.2909077 ],\n",
       "           [0.18809523, 0.07013827, 0.10546582, 0.07146291, 0.2781383 ,\n",
       "            0.28669944]],\n",
       "  \n",
       "          [[0.20186125, 0.12893073, 0.20026958, 0.10552842, 0.25687078,\n",
       "            0.10653924],\n",
       "           [0.18112886, 0.09317507, 0.21822488, 0.12089407, 0.29552737,\n",
       "            0.09104969],\n",
       "           [0.17442547, 0.08493349, 0.23586184, 0.12357228, 0.29356793,\n",
       "            0.08763894],\n",
       "           ...,\n",
       "           [0.16695948, 0.06026239, 0.26299357, 0.12454721, 0.29051343,\n",
       "            0.09472392],\n",
       "           [0.1643584 , 0.06083306, 0.25290626, 0.12713212, 0.29905695,\n",
       "            0.0957132 ],\n",
       "           [0.16153061, 0.06204269, 0.24668738, 0.13049768, 0.30592278,\n",
       "            0.0933189 ]],\n",
       "  \n",
       "          [[0.12123303, 0.47154158, 0.13860895, 0.18692032, 0.05220906,\n",
       "            0.02948699],\n",
       "           [0.14778058, 0.47136328, 0.13456285, 0.16771597, 0.05413818,\n",
       "            0.02443911],\n",
       "           [0.16209902, 0.45258388, 0.1326473 , 0.17136198, 0.0570908 ,\n",
       "            0.02421703],\n",
       "           ...,\n",
       "           [0.24745597, 0.37566367, 0.1138126 , 0.1717649 , 0.06133596,\n",
       "            0.02996687],\n",
       "           [0.247467  , 0.373407  , 0.11519801, 0.17452283, 0.06101765,\n",
       "            0.02838754],\n",
       "           [0.2516777 , 0.36622182, 0.11510377, 0.17819418, 0.06175439,\n",
       "            0.02704815]],\n",
       "  \n",
       "          ...,\n",
       "  \n",
       "          [[0.1356658 , 0.09220946, 0.2593619 , 0.21803655, 0.08832566,\n",
       "            0.20640068],\n",
       "           [0.14462718, 0.12019841, 0.25246683, 0.18328983, 0.10240678,\n",
       "            0.19701092],\n",
       "           [0.15757303, 0.1166584 , 0.24004915, 0.17127274, 0.11288663,\n",
       "            0.20156   ],\n",
       "           ...,\n",
       "           [0.18082665, 0.14264417, 0.22670954, 0.16808087, 0.11289784,\n",
       "            0.16884096],\n",
       "           [0.17841627, 0.147177  , 0.22721872, 0.170519  , 0.11002231,\n",
       "            0.16664669],\n",
       "           [0.17917673, 0.14913037, 0.22707433, 0.17050464, 0.10786094,\n",
       "            0.16625294]],\n",
       "  \n",
       "          [[0.17129646, 0.11435341, 0.08679523, 0.11072993, 0.1676425 ,\n",
       "            0.34918243],\n",
       "           [0.2187642 , 0.14158686, 0.09907088, 0.11060898, 0.17133035,\n",
       "            0.25863874],\n",
       "           [0.22413732, 0.14856537, 0.09935162, 0.10265998, 0.17163412,\n",
       "            0.25365156],\n",
       "           ...,\n",
       "           [0.22032915, 0.10706898, 0.10655515, 0.11963854, 0.20764475,\n",
       "            0.23876348],\n",
       "           [0.21408731, 0.10918504, 0.10858705, 0.1246206 , 0.20297478,\n",
       "            0.24054523],\n",
       "           [0.20791532, 0.11244029, 0.11101285, 0.13043061, 0.19855614,\n",
       "            0.23964483]],\n",
       "  \n",
       "          [[0.14390504, 0.11869728, 0.074631  , 0.08311703, 0.40683648,\n",
       "            0.17281315],\n",
       "           [0.17112212, 0.0916714 , 0.06595588, 0.09050035, 0.42557377,\n",
       "            0.15517646],\n",
       "           [0.18985665, 0.09807589, 0.06919941, 0.09276695, 0.39478615,\n",
       "            0.15531498],\n",
       "           ...,\n",
       "           [0.24210483, 0.08432082, 0.06278784, 0.10501727, 0.3507637 ,\n",
       "            0.15500551],\n",
       "           [0.24937664, 0.08082553, 0.05991384, 0.09935905, 0.36020815,\n",
       "            0.15031679],\n",
       "           [0.25640455, 0.07747369, 0.05748463, 0.09640791, 0.36525676,\n",
       "            0.14697242]]]], dtype=float32)>,\n",
       "  'decoder_layer5_block1': <tf.Tensor: shape=(1, 8, 40, 40), dtype=float32, numpy=\n",
       "  array([[[[1.        , 0.        , 0.        , ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           [0.49928603, 0.500714  , 0.        , ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           [0.33097756, 0.3399231 , 0.32909933, ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           ...,\n",
       "           [0.03227851, 0.0292615 , 0.02839727, ..., 0.02269328,\n",
       "            0.        , 0.        ],\n",
       "           [0.03101012, 0.02839201, 0.02768807, ..., 0.02225691,\n",
       "            0.0212907 , 0.        ],\n",
       "           [0.03011923, 0.02795452, 0.02731788, ..., 0.0217297 ,\n",
       "            0.02083242, 0.02083471]],\n",
       "  \n",
       "          [[1.        , 0.        , 0.        , ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           [0.3808862 , 0.6191138 , 0.        , ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           [0.2235054 , 0.36836502, 0.40812957, ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           ...,\n",
       "           [0.02340592, 0.03818326, 0.03893866, ..., 0.02339446,\n",
       "            0.        , 0.        ],\n",
       "           [0.02317923, 0.03760185, 0.03834785, ..., 0.02264838,\n",
       "            0.02329597, 0.        ],\n",
       "           [0.02299982, 0.03718369, 0.03785922, ..., 0.02197603,\n",
       "            0.02255145, 0.02396847]],\n",
       "  \n",
       "          [[1.        , 0.        , 0.        , ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           [0.4722946 , 0.5277054 , 0.        , ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           [0.30997515, 0.34397933, 0.3460455 , ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           ...,\n",
       "           [0.04602667, 0.03834276, 0.03810705, ..., 0.02153945,\n",
       "            0.        , 0.        ],\n",
       "           [0.04498751, 0.03781261, 0.03758508, ..., 0.02100621,\n",
       "            0.02138582, 0.        ],\n",
       "           [0.0439555 , 0.03728154, 0.03707316, ..., 0.02043669,\n",
       "            0.02079045, 0.02184889]],\n",
       "  \n",
       "          ...,\n",
       "  \n",
       "          [[1.        , 0.        , 0.        , ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           [0.37712318, 0.62287676, 0.        , ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           [0.2426859 , 0.39369595, 0.36361814, ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           ...,\n",
       "           [0.02049294, 0.03011231, 0.02745385, ..., 0.02313287,\n",
       "            0.        , 0.        ],\n",
       "           [0.02109802, 0.02999584, 0.02712989, ..., 0.02243329,\n",
       "            0.02105612, 0.        ],\n",
       "           [0.02146426, 0.02969699, 0.0267204 , ..., 0.02184956,\n",
       "            0.02053688, 0.02050595]],\n",
       "  \n",
       "          [[1.        , 0.        , 0.        , ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           [0.5411821 , 0.45881787, 0.        , ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           [0.37637028, 0.32242668, 0.30120304, ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           ...,\n",
       "           [0.0257248 , 0.0233559 , 0.02353831, ..., 0.0278777 ,\n",
       "            0.        , 0.        ],\n",
       "           [0.02555777, 0.02310155, 0.02322686, ..., 0.02688429,\n",
       "            0.02506351, 0.        ],\n",
       "           [0.02504954, 0.02276355, 0.0229315 , ..., 0.02600385,\n",
       "            0.02422544, 0.02288664]],\n",
       "  \n",
       "          [[1.        , 0.        , 0.        , ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           [0.33285332, 0.6671467 , 0.        , ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           [0.19306831, 0.39133298, 0.41559875, ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           ...,\n",
       "           [0.00735598, 0.01644836, 0.01753373, ..., 0.02854642,\n",
       "            0.        , 0.        ],\n",
       "           [0.00700754, 0.01567789, 0.01677761, ..., 0.02770589,\n",
       "            0.02963487, 0.        ],\n",
       "           [0.00679356, 0.01500835, 0.0161273 , ..., 0.02691422,\n",
       "            0.02874782, 0.02925279]]]], dtype=float32)>,\n",
       "  'decoder_layer5_block2': <tf.Tensor: shape=(1, 8, 40, 6), dtype=float32, numpy=\n",
       "  array([[[[0.09810452, 0.20587672, 0.2761077 , 0.23437706, 0.09547383,\n",
       "            0.09006018],\n",
       "           [0.10111443, 0.17797333, 0.2667088 , 0.23528327, 0.11179736,\n",
       "            0.10712282],\n",
       "           [0.10244691, 0.17027643, 0.26380977, 0.23121421, 0.1182811 ,\n",
       "            0.11397157],\n",
       "           ...,\n",
       "           [0.11220843, 0.20103341, 0.2310548 , 0.2048335 , 0.13388565,\n",
       "            0.11698418],\n",
       "           [0.11168402, 0.20306835, 0.2327801 , 0.20484465, 0.13431084,\n",
       "            0.11331206],\n",
       "           [0.11059172, 0.20463304, 0.23472191, 0.20494871, 0.13389756,\n",
       "            0.11120709]],\n",
       "  \n",
       "          [[0.26881623, 0.11563648, 0.12077765, 0.11718282, 0.11259099,\n",
       "            0.26499584],\n",
       "           [0.25872993, 0.15404809, 0.12216084, 0.11710792, 0.12365336,\n",
       "            0.22429982],\n",
       "           [0.26046088, 0.1542485 , 0.12311196, 0.12186535, 0.1301823 ,\n",
       "            0.21013103],\n",
       "           ...,\n",
       "           [0.35314256, 0.13213752, 0.10693587, 0.12109873, 0.13898924,\n",
       "            0.14769614],\n",
       "           [0.35500482, 0.1316271 , 0.1085286 , 0.12378155, 0.13612115,\n",
       "            0.1449368 ],\n",
       "           [0.35978472, 0.13013767, 0.10951374, 0.12537843, 0.13345356,\n",
       "            0.1417318 ]],\n",
       "  \n",
       "          [[0.21177174, 0.18508181, 0.11048004, 0.10397031, 0.17636871,\n",
       "            0.21232742],\n",
       "           [0.24935688, 0.21259655, 0.09802017, 0.09560101, 0.16875747,\n",
       "            0.17566791],\n",
       "           [0.23904382, 0.21933313, 0.09679513, 0.09459604, 0.17113209,\n",
       "            0.1790997 ],\n",
       "           ...,\n",
       "           [0.21100825, 0.22537097, 0.09762084, 0.10970188, 0.13921064,\n",
       "            0.2170874 ],\n",
       "           [0.21058892, 0.22624063, 0.09674091, 0.10959738, 0.1398349 ,\n",
       "            0.21699725],\n",
       "           [0.21062292, 0.2268171 , 0.09573529, 0.10873342, 0.13858539,\n",
       "            0.21950589]],\n",
       "  \n",
       "          ...,\n",
       "  \n",
       "          [[0.0642141 , 0.12083664, 0.20564534, 0.12489165, 0.19160543,\n",
       "            0.29280683],\n",
       "           [0.07683557, 0.11249444, 0.17354938, 0.10456792, 0.16913   ,\n",
       "            0.3634227 ],\n",
       "           [0.07741994, 0.11359939, 0.17384475, 0.10461209, 0.1554218 ,\n",
       "            0.37510204],\n",
       "           ...,\n",
       "           [0.06392825, 0.09753544, 0.1784479 , 0.09688646, 0.11047003,\n",
       "            0.45273197],\n",
       "           [0.06298359, 0.09986141, 0.17902106, 0.09631824, 0.10770265,\n",
       "            0.45411307],\n",
       "           [0.0618038 , 0.10055789, 0.18018752, 0.09629609, 0.10507176,\n",
       "            0.45608297]],\n",
       "  \n",
       "          [[0.15835808, 0.11171632, 0.1312498 , 0.2676994 , 0.14556517,\n",
       "            0.18541129],\n",
       "           [0.16242664, 0.13233012, 0.1404961 , 0.2195824 , 0.13795717,\n",
       "            0.2072076 ],\n",
       "           [0.16054298, 0.13590701, 0.14158899, 0.2073677 , 0.13517591,\n",
       "            0.2194174 ],\n",
       "           ...,\n",
       "           [0.17134139, 0.15128195, 0.1428186 , 0.16649097, 0.12542106,\n",
       "            0.24264605],\n",
       "           [0.17311884, 0.15098509, 0.14290877, 0.16485147, 0.1231924 ,\n",
       "            0.24494345],\n",
       "           [0.17053628, 0.15438727, 0.14572594, 0.16363423, 0.12048262,\n",
       "            0.24523361]],\n",
       "  \n",
       "          [[0.18468963, 0.13098414, 0.09701663, 0.17068028, 0.23610115,\n",
       "            0.18052824],\n",
       "           [0.16165872, 0.13983415, 0.09787765, 0.17822096, 0.19873774,\n",
       "            0.22367078],\n",
       "           [0.15498205, 0.14422746, 0.09909222, 0.1761952 , 0.18706691,\n",
       "            0.23843618],\n",
       "           ...,\n",
       "           [0.14385304, 0.1725404 , 0.11861195, 0.17871791, 0.14943498,\n",
       "            0.23684175],\n",
       "           [0.14331678, 0.1712007 , 0.11786915, 0.17890283, 0.15008324,\n",
       "            0.23862723],\n",
       "           [0.14180207, 0.17125678, 0.11785702, 0.18062493, 0.15002504,\n",
       "            0.23843406]]]], dtype=float32)>,\n",
       "  'decoder_layer6_block1': <tf.Tensor: shape=(1, 8, 40, 40), dtype=float32, numpy=\n",
       "  array([[[[1.        , 0.        , 0.        , ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           [0.5376598 , 0.4623402 , 0.        , ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           [0.37432376, 0.31970662, 0.30596966, ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           ...,\n",
       "           [0.0481246 , 0.03653573, 0.03423942, ..., 0.02128776,\n",
       "            0.        , 0.        ],\n",
       "           [0.04686957, 0.03580252, 0.03349869, ..., 0.02080293,\n",
       "            0.02068268, 0.        ],\n",
       "           [0.04538009, 0.03485845, 0.03260233, ..., 0.02041916,\n",
       "            0.02029024, 0.02074043]],\n",
       "  \n",
       "          [[1.        , 0.        , 0.        , ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           [0.5111636 , 0.4888364 , 0.        , ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           [0.3541777 , 0.33266088, 0.31316134, ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           ...,\n",
       "           [0.04224368, 0.03466052, 0.03098952, ..., 0.02731234,\n",
       "            0.        , 0.        ],\n",
       "           [0.04168436, 0.03390154, 0.03026461, ..., 0.02652369,\n",
       "            0.02634895, 0.        ],\n",
       "           [0.04093973, 0.03317421, 0.02958615, ..., 0.02587265,\n",
       "            0.02570082, 0.02527542]],\n",
       "  \n",
       "          [[1.        , 0.        , 0.        , ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           [0.5837988 , 0.41620114, 0.        , ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           [0.40694547, 0.29143083, 0.30162373, ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           ...,\n",
       "           [0.03053198, 0.02314371, 0.02403955, ..., 0.02607889,\n",
       "            0.        , 0.        ],\n",
       "           [0.02994915, 0.02265484, 0.0235587 , ..., 0.02536949,\n",
       "            0.02538973, 0.        ],\n",
       "           [0.02939446, 0.02223139, 0.0231254 , ..., 0.02469853,\n",
       "            0.02473038, 0.02485502]],\n",
       "  \n",
       "          ...,\n",
       "  \n",
       "          [[1.        , 0.        , 0.        , ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           [0.5993159 , 0.40068415, 0.        , ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           [0.45541838, 0.29783794, 0.24674365, ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           ...,\n",
       "           [0.08840384, 0.0532219 , 0.04143737, ..., 0.01944281,\n",
       "            0.        , 0.        ],\n",
       "           [0.08668984, 0.05241622, 0.04087181, ..., 0.01900509,\n",
       "            0.01924808, 0.        ],\n",
       "           [0.08455413, 0.05133662, 0.04014229, ..., 0.0186721 ,\n",
       "            0.0189015 , 0.0191049 ]],\n",
       "  \n",
       "          [[1.        , 0.        , 0.        , ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           [0.5205282 , 0.47947174, 0.        , ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           [0.37037706, 0.34096456, 0.2886584 , ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           ...,\n",
       "           [0.04778506, 0.04228293, 0.0346179 , ..., 0.02548436,\n",
       "            0.        , 0.        ],\n",
       "           [0.04615362, 0.04093658, 0.0335425 , ..., 0.02498856,\n",
       "            0.02485259, 0.        ],\n",
       "           [0.04445802, 0.03959938, 0.03249412, ..., 0.02462834,\n",
       "            0.02449219, 0.02382073]],\n",
       "  \n",
       "          [[1.        , 0.        , 0.        , ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           [0.49069697, 0.50930303, 0.        , ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           [0.31728232, 0.34243566, 0.34028196, ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           ...,\n",
       "           [0.01774864, 0.02173678, 0.02258446, ..., 0.02947132,\n",
       "            0.        , 0.        ],\n",
       "           [0.01688133, 0.02081931, 0.02172896, ..., 0.02874285,\n",
       "            0.02945786, 0.        ],\n",
       "           [0.01609856, 0.02003798, 0.02099447, ..., 0.02797731,\n",
       "            0.02869292, 0.02854566]]]], dtype=float32)>,\n",
       "  'decoder_layer6_block2': <tf.Tensor: shape=(1, 8, 40, 6), dtype=float32, numpy=\n",
       "  array([[[[0.18182206, 0.15487409, 0.27677122, 0.11955051, 0.06850246,\n",
       "            0.19847968],\n",
       "           [0.22302133, 0.14005528, 0.23302367, 0.11558573, 0.07424308,\n",
       "            0.2140709 ],\n",
       "           [0.229135  , 0.12999287, 0.21924992, 0.11349256, 0.07682266,\n",
       "            0.23130701],\n",
       "           ...,\n",
       "           [0.20375516, 0.09338798, 0.19205327, 0.10667604, 0.07877703,\n",
       "            0.32535052],\n",
       "           [0.20397334, 0.09427562, 0.19365038, 0.10749996, 0.07919108,\n",
       "            0.3214096 ],\n",
       "           [0.20378153, 0.09473269, 0.19458997, 0.10779191, 0.08016218,\n",
       "            0.31894174]],\n",
       "  \n",
       "          [[0.19332303, 0.1398305 , 0.07983343, 0.09246679, 0.25693744,\n",
       "            0.23760878],\n",
       "           [0.2110188 , 0.12574041, 0.07165166, 0.0900052 , 0.28712228,\n",
       "            0.21446168],\n",
       "           [0.2160296 , 0.12180498, 0.06891289, 0.08943714, 0.292749  ,\n",
       "            0.21106641],\n",
       "           ...,\n",
       "           [0.24073398, 0.1157289 , 0.06475747, 0.08726925, 0.28755233,\n",
       "            0.203958  ],\n",
       "           [0.24001214, 0.11705987, 0.06489991, 0.08720063, 0.28542283,\n",
       "            0.2054046 ],\n",
       "           [0.24016866, 0.11852013, 0.065148  , 0.08697588, 0.2832607 ,\n",
       "            0.20592661]],\n",
       "  \n",
       "          [[0.15052061, 0.2329318 , 0.29511696, 0.12082461, 0.09082825,\n",
       "            0.10977776],\n",
       "           [0.17721757, 0.22130518, 0.29609764, 0.12085105, 0.08875858,\n",
       "            0.09576999],\n",
       "           [0.18136834, 0.22071877, 0.29334602, 0.12202911, 0.09187523,\n",
       "            0.09066252],\n",
       "           ...,\n",
       "           [0.1768901 , 0.18804878, 0.29517   , 0.1401169 , 0.12287395,\n",
       "            0.07690027],\n",
       "           [0.17688654, 0.1857917 , 0.29738894, 0.13931926, 0.12337183,\n",
       "            0.07724172],\n",
       "           [0.17853424, 0.18333647, 0.2991984 , 0.13917525, 0.12314517,\n",
       "            0.07661048]],\n",
       "  \n",
       "          ...,\n",
       "  \n",
       "          [[0.22337732, 0.18100393, 0.2053186 , 0.2233621 , 0.1009432 ,\n",
       "            0.06599479],\n",
       "           [0.23847446, 0.18043132, 0.21269664, 0.20681134, 0.09932604,\n",
       "            0.06226023],\n",
       "           [0.24409376, 0.17870425, 0.21335198, 0.20394644, 0.0990361 ,\n",
       "            0.06086745],\n",
       "           ...,\n",
       "           [0.2222819 , 0.18267727, 0.20487136, 0.17068258, 0.13100734,\n",
       "            0.08847951],\n",
       "           [0.22224587, 0.18364088, 0.20418008, 0.17115444, 0.1298962 ,\n",
       "            0.08888252],\n",
       "           [0.2248871 , 0.18344364, 0.20407966, 0.17029792, 0.12821421,\n",
       "            0.08907751]],\n",
       "  \n",
       "          [[0.05319715, 0.10159186, 0.20755847, 0.19331488, 0.05025604,\n",
       "            0.39408156],\n",
       "           [0.04291812, 0.09268197, 0.20323433, 0.18857259, 0.05108994,\n",
       "            0.42150307],\n",
       "           [0.03951754, 0.09214679, 0.20511833, 0.17889436, 0.05021957,\n",
       "            0.4341034 ],\n",
       "           ...,\n",
       "           [0.04886673, 0.11564311, 0.22517146, 0.14313537, 0.05155281,\n",
       "            0.41563055],\n",
       "           [0.04817775, 0.11620569, 0.22548495, 0.14329743, 0.05203235,\n",
       "            0.41480184],\n",
       "           [0.04730111, 0.11527035, 0.22626656, 0.14426151, 0.05256094,\n",
       "            0.41433954]],\n",
       "  \n",
       "          [[0.3762992 , 0.06820031, 0.09106874, 0.12289561, 0.25610963,\n",
       "            0.08542652],\n",
       "           [0.3477959 , 0.07610758, 0.10903434, 0.13371284, 0.22896592,\n",
       "            0.10438341],\n",
       "           [0.331322  , 0.07884049, 0.11554576, 0.14220572, 0.22498761,\n",
       "            0.10709836],\n",
       "           ...,\n",
       "           [0.32935694, 0.09772646, 0.13079134, 0.15633753, 0.1700472 ,\n",
       "            0.11574046],\n",
       "           [0.32845342, 0.09919687, 0.13182831, 0.15667629, 0.1683807 ,\n",
       "            0.11546442],\n",
       "           [0.32507342, 0.10151954, 0.13400558, 0.1581046 , 0.16674982,\n",
       "            0.11454709]]]], dtype=float32)>})"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_chatbot(transformer_2, \"how are you?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chatbot(transformer, sentence):\n",
    "    result, attention_weights = evaluate_chatbot(transformer, sentence)\n",
    "\n",
    "    predicted_sentence = tokenizer_ans.decode([i for i in result\n",
    "                                              if i < tokenizer_ans.vocab_size])\n",
    "\n",
    "#     print('Input: {}'.format(sentence))\n",
    "#     print('Predicted Translation: {}'.format(predicted_sentence))\n",
    "    \n",
    "    return predicted_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'rmrmrmrmrmrmrmrmrmrmrmrmrmrmrmrmrmrmrmrmrmrmrmrmrmrmrmrmrmrmrmrmrmrmrmrmrmrmrmrm'"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chatbot(transformer_2, \"how are you?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x23cd1166a48>"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer_chatbot = Transformer(config=config,\n",
    "                          target_vocab_size=target_vocab_size_2,\n",
    "                          bert_config_file=bert_config_file)\n",
    "  \n",
    "# fn_out, _ = transformer_chatbot(inp, tar_inp, \n",
    "#                         True,\n",
    "#                         look_ahead_mask=None,\n",
    "#                         dec_padding_mask=None)\n",
    "transformer_chatbot.load_weights('/weights/chatbot_bert_ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'nice very uh , thank you . thank you .'"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chatbot(transformer_chatbot, \"how are you doing?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate eng to chi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_en(en):\n",
    "    tokens_en = tokenizer_en_2.tokenize(en)\n",
    "    lang1 = tokenizer_en_2.convert_tokens_to_ids(['[CLS]'] + tokens_en + ['[SEP]'])\n",
    "    return lang1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_eng_to_chi(transformer, inp_sentence):\n",
    "    # normalize input sentence\n",
    "    inp_sentence = encode_en(inp_sentence)\n",
    "    encoder_input = tf.expand_dims(inp_sentence, 0)\n",
    "\n",
    "    # as the target is english, the first word to the transformer should be the\n",
    "    # english start token.\n",
    "    decoder_input = [tokenizer_chi_2.vocab_size]\n",
    "    output = tf.expand_dims(decoder_input, 0)\n",
    "\n",
    "    for i in range(MAX_SEQ_LENGTH):\n",
    "        combined_mask, dec_padding_mask = create_masks(\n",
    "            encoder_input, output)\n",
    "\n",
    "        # predictions.shape == (batch_size, seq_len, vocab_size)\n",
    "        predictions, attention_weights = transformer(encoder_input,\n",
    "                                                     output,\n",
    "                                                     False,\n",
    "                                                     combined_mask,\n",
    "                                                     dec_padding_mask)\n",
    "\n",
    "        # select the last word from the seq_len dimension\n",
    "        predictions = predictions[:, -1:, :]  # (batch_size, 1, vocab_size)\n",
    "\n",
    "        predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
    "\n",
    "        # return the result if the predicted_id is equal to the end token\n",
    "        if tf.equal(predicted_id, tokenizer_chi_2.vocab_size + 1):\n",
    "            return tf.squeeze(output, axis=0), attention_weights\n",
    "\n",
    "        # concatentate the predicted_id to the output which is given to the decoder\n",
    "        # as its input.\n",
    "        output = tf.concat([output, predicted_id], axis=-1)\n",
    "\n",
    "    return tf.squeeze(output, axis=0), attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(41,), dtype=int32, numpy=\n",
       " array([7318, 4084, 4084, 5153, 5153, 5153, 5153, 5153, 5153, 5153, 5153,\n",
       "        5153, 5153, 5153, 5153, 5153, 5153, 5153, 5153, 5153, 5153, 5153,\n",
       "        5153, 5153, 5153, 5153, 5153, 5153, 5153, 5153, 5153, 5153, 5153,\n",
       "        5153, 5153, 5153, 5153, 5153, 5153, 5153, 5153])>,\n",
       " {'decoder_layer1_block1': <tf.Tensor: shape=(1, 8, 40, 40), dtype=float32, numpy=\n",
       "  array([[[[1.        , 0.        , 0.        , ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           [0.47573826, 0.5242618 , 0.        , ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           [0.27310273, 0.29794422, 0.42895308, ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           ...,\n",
       "           [0.00719214, 0.01817749, 0.0278219 , ..., 0.02544523,\n",
       "            0.        , 0.        ],\n",
       "           [0.00589785, 0.01770682, 0.02639655, ..., 0.02191273,\n",
       "            0.02513941, 0.        ],\n",
       "           [0.00597074, 0.01850038, 0.02735454, ..., 0.01867624,\n",
       "            0.02082236, 0.02269026]],\n",
       "  \n",
       "          [[1.        , 0.        , 0.        , ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           [0.3109133 , 0.6890867 , 0.        , ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           [0.16437519, 0.40204653, 0.43357822, ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           ...,\n",
       "           [0.05099736, 0.00614443, 0.00684732, ..., 0.01646925,\n",
       "            0.        , 0.        ],\n",
       "           [0.06607062, 0.00637449, 0.00679067, ..., 0.01454675,\n",
       "            0.01106062, 0.        ],\n",
       "           [0.07818402, 0.00712807, 0.0073636 , ..., 0.01337169,\n",
       "            0.01063632, 0.00842791]],\n",
       "  \n",
       "          [[1.        , 0.        , 0.        , ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           [0.1475453 , 0.8524547 , 0.        , ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           [0.07660351, 0.47296193, 0.45043454, ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           ...,\n",
       "           [0.02256538, 0.01550523, 0.01450801, ..., 0.02617216,\n",
       "            0.        , 0.        ],\n",
       "           [0.0166641 , 0.01344264, 0.01277496, ..., 0.02611868,\n",
       "            0.02772287, 0.        ],\n",
       "           [0.01441886, 0.01196482, 0.01131437, ..., 0.02534162,\n",
       "            0.02629789, 0.02981457]],\n",
       "  \n",
       "          ...,\n",
       "  \n",
       "          [[1.        , 0.        , 0.        , ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           [0.21153072, 0.78846925, 0.        , ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           [0.14216873, 0.3983578 , 0.45947343, ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           ...,\n",
       "           [0.01071138, 0.01813262, 0.02208853, ..., 0.03794208,\n",
       "            0.        , 0.        ],\n",
       "           [0.009485  , 0.01702671, 0.02090792, ..., 0.0355901 ,\n",
       "            0.03480171, 0.        ],\n",
       "           [0.00794762, 0.01588413, 0.01947594, ..., 0.0339121 ,\n",
       "            0.03299609, 0.03340883]],\n",
       "  \n",
       "          [[1.        , 0.        , 0.        , ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           [0.75222725, 0.24777275, 0.        , ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           [0.67516464, 0.1682837 , 0.15655166, ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           ...,\n",
       "           [0.02591558, 0.03730247, 0.0301881 , ..., 0.05620558,\n",
       "            0.        , 0.        ],\n",
       "           [0.02170947, 0.02710138, 0.02158066, ..., 0.05980192,\n",
       "            0.05342596, 0.        ],\n",
       "           [0.01809949, 0.02152684, 0.0171501 , ..., 0.06185063,\n",
       "            0.05500456, 0.05159101]],\n",
       "  \n",
       "          [[1.        , 0.        , 0.        , ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           [0.7608598 , 0.23914018, 0.        , ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           [0.6439228 , 0.17032346, 0.18575372, ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           ...,\n",
       "           [0.05012777, 0.06987036, 0.08138134, ..., 0.01767126,\n",
       "            0.        , 0.        ],\n",
       "           [0.04897516, 0.07249501, 0.08311068, ..., 0.01696552,\n",
       "            0.01668837, 0.        ],\n",
       "           [0.05229821, 0.08088836, 0.0883727 , ..., 0.01502485,\n",
       "            0.01511434, 0.01521713]]]], dtype=float32)>,\n",
       "  'decoder_layer1_block2': <tf.Tensor: shape=(1, 8, 40, 6), dtype=float32, numpy=\n",
       "  array([[[[0.09795935, 0.19196814, 0.2392905 , 0.31599206, 0.11846125,\n",
       "            0.03632869],\n",
       "           [0.21930878, 0.15117452, 0.21909772, 0.22754988, 0.10335433,\n",
       "            0.07951476],\n",
       "           [0.23365349, 0.14763896, 0.21689856, 0.20822471, 0.10741232,\n",
       "            0.08617198],\n",
       "           ...,\n",
       "           [0.52464885, 0.07386623, 0.07452007, 0.07561515, 0.16028364,\n",
       "            0.09106604],\n",
       "           [0.49380556, 0.08684069, 0.08634417, 0.08731919, 0.1557569 ,\n",
       "            0.08993347],\n",
       "           [0.44413388, 0.1070415 , 0.10066455, 0.1007165 , 0.15211746,\n",
       "            0.0953261 ]],\n",
       "  \n",
       "          [[0.20314603, 0.23159382, 0.19137928, 0.12416458, 0.11595415,\n",
       "            0.13376221],\n",
       "           [0.11348547, 0.21446496, 0.2443344 , 0.19959098, 0.16051704,\n",
       "            0.06760723],\n",
       "           [0.13602148, 0.21418916, 0.22653948, 0.18974793, 0.1673714 ,\n",
       "            0.06613056],\n",
       "           ...,\n",
       "           [0.1796385 , 0.2642653 , 0.2316503 , 0.22352542, 0.03351282,\n",
       "            0.06740768],\n",
       "           [0.1858887 , 0.278364  , 0.21713637, 0.21233653, 0.03434572,\n",
       "            0.07192874],\n",
       "           [0.18916453, 0.28035617, 0.20391704, 0.20674811, 0.03772731,\n",
       "            0.08208681]],\n",
       "  \n",
       "          [[0.26638678, 0.07969514, 0.16835442, 0.11248854, 0.15304749,\n",
       "            0.22002766],\n",
       "           [0.20941994, 0.06506745, 0.1366582 , 0.15773098, 0.18181314,\n",
       "            0.24931034],\n",
       "           [0.1877646 , 0.06631804, 0.13808627, 0.16279939, 0.20172435,\n",
       "            0.24330732],\n",
       "           ...,\n",
       "           [0.09622899, 0.09252498, 0.1506052 , 0.14262548, 0.22623059,\n",
       "            0.2917847 ],\n",
       "           [0.09351375, 0.08688255, 0.14785056, 0.15533541, 0.23402931,\n",
       "            0.2823884 ],\n",
       "           [0.09603519, 0.08498257, 0.1450251 , 0.17499943, 0.23523246,\n",
       "            0.26372525]],\n",
       "  \n",
       "          ...,\n",
       "  \n",
       "          [[0.17807235, 0.16343305, 0.13556984, 0.2075017 , 0.18168826,\n",
       "            0.13373479],\n",
       "           [0.22048603, 0.13015509, 0.10155272, 0.23637421, 0.1689635 ,\n",
       "            0.14246844],\n",
       "           [0.20992383, 0.12853026, 0.10766776, 0.24599884, 0.16385502,\n",
       "            0.14402431],\n",
       "           ...,\n",
       "           [0.3799549 , 0.09985736, 0.13897906, 0.22223498, 0.06574235,\n",
       "            0.09323139],\n",
       "           [0.41773218, 0.09463952, 0.12583637, 0.20291165, 0.07243746,\n",
       "            0.08644284],\n",
       "           [0.44381717, 0.09073819, 0.11507232, 0.18782237, 0.07677111,\n",
       "            0.08577891]],\n",
       "  \n",
       "          [[0.11683648, 0.19328329, 0.26175925, 0.1734583 , 0.13986294,\n",
       "            0.11479969],\n",
       "           [0.11586161, 0.10367282, 0.2615522 , 0.27205196, 0.10724632,\n",
       "            0.13961509],\n",
       "           [0.12109663, 0.10224108, 0.25071904, 0.2586504 , 0.10794617,\n",
       "            0.15934667],\n",
       "           ...,\n",
       "           [0.20365818, 0.07829523, 0.22254203, 0.17775498, 0.09606881,\n",
       "            0.2216808 ],\n",
       "           [0.2136627 , 0.08455251, 0.22714287, 0.17805493, 0.09864744,\n",
       "            0.19793952],\n",
       "           [0.21651646, 0.08826478, 0.2235058 , 0.17986837, 0.10312584,\n",
       "            0.18871878]],\n",
       "  \n",
       "          [[0.21667054, 0.21668491, 0.21097206, 0.19719903, 0.09675368,\n",
       "            0.0617197 ],\n",
       "           [0.10720108, 0.16197114, 0.24851774, 0.18046415, 0.12354313,\n",
       "            0.17830278],\n",
       "           [0.0861825 , 0.1641683 , 0.25770798, 0.18040457, 0.12386692,\n",
       "            0.1876697 ],\n",
       "           ...,\n",
       "           [0.17947806, 0.28346035, 0.14740983, 0.12837622, 0.1146702 ,\n",
       "            0.14660534],\n",
       "           [0.1736507 , 0.27004558, 0.15421812, 0.13194723, 0.11622442,\n",
       "            0.15391396],\n",
       "           [0.17603067, 0.24721302, 0.15128972, 0.13138102, 0.12200026,\n",
       "            0.17208534]]]], dtype=float32)>,\n",
       "  'decoder_layer2_block1': <tf.Tensor: shape=(1, 8, 40, 40), dtype=float32, numpy=\n",
       "  array([[[[1.        , 0.        , 0.        , ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           [0.36679682, 0.6332032 , 0.        , ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           [0.243149  , 0.3964324 , 0.36041856, ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           ...,\n",
       "           [0.03699693, 0.02776473, 0.02717025, ..., 0.02518462,\n",
       "            0.        , 0.        ],\n",
       "           [0.03310475, 0.02587387, 0.02495777, ..., 0.02506975,\n",
       "            0.02676941, 0.        ],\n",
       "           [0.02960135, 0.02309197, 0.02205813, ..., 0.02558223,\n",
       "            0.02696341, 0.02783795]],\n",
       "  \n",
       "          [[1.        , 0.        , 0.        , ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           [0.65720636, 0.34279367, 0.        , ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           [0.49330303, 0.23453417, 0.27216277, ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           ...,\n",
       "           [0.07954656, 0.02474432, 0.02217951, ..., 0.02536789,\n",
       "            0.        , 0.        ],\n",
       "           [0.0680101 , 0.02366067, 0.02155113, ..., 0.02598093,\n",
       "            0.02660144, 0.        ],\n",
       "           [0.05589539, 0.02267543, 0.02096454, ..., 0.02620408,\n",
       "            0.02655889, 0.0275497 ]],\n",
       "  \n",
       "          [[1.        , 0.        , 0.        , ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           [0.18538462, 0.8146154 , 0.        , ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           [0.06090255, 0.3818339 , 0.55726355, ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           ...,\n",
       "           [0.03626698, 0.04123104, 0.03812188, ..., 0.01373651,\n",
       "            0.        , 0.        ],\n",
       "           [0.0381591 , 0.04337295, 0.04004351, ..., 0.01421013,\n",
       "            0.01533541, 0.        ],\n",
       "           [0.03817386, 0.04089025, 0.03767557, ..., 0.01529702,\n",
       "            0.01661864, 0.01826217]],\n",
       "  \n",
       "          ...,\n",
       "  \n",
       "          [[1.        , 0.        , 0.        , ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           [0.4591064 , 0.5408936 , 0.        , ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           [0.32142505, 0.3689359 , 0.309639  , ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           ...,\n",
       "           [0.09058613, 0.211615  , 0.1928808 , ..., 0.01219106,\n",
       "            0.        , 0.        ],\n",
       "           [0.07082474, 0.22067131, 0.2020758 , ..., 0.01273676,\n",
       "            0.0145818 , 0.        ],\n",
       "           [0.06038598, 0.21157546, 0.192973  , ..., 0.01418035,\n",
       "            0.01607446, 0.01684475]],\n",
       "  \n",
       "          [[1.        , 0.        , 0.        , ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           [0.41407195, 0.5859281 , 0.        , ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           [0.22146547, 0.38690782, 0.39162675, ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           ...,\n",
       "           [0.0154001 , 0.0213718 , 0.01949315, ..., 0.02068388,\n",
       "            0.        , 0.        ],\n",
       "           [0.01550194, 0.02178646, 0.01971391, ..., 0.01994888,\n",
       "            0.01832826, 0.        ],\n",
       "           [0.01748316, 0.02394269, 0.02159014, ..., 0.01832799,\n",
       "            0.01640566, 0.01698479]],\n",
       "  \n",
       "          [[1.        , 0.        , 0.        , ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           [0.66620654, 0.3337935 , 0.        , ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           [0.5325295 , 0.246803  , 0.22066751, ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           ...,\n",
       "           [0.00896548, 0.00308972, 0.00264781, ..., 0.03263178,\n",
       "            0.        , 0.        ],\n",
       "           [0.00790715, 0.00273965, 0.00235817, ..., 0.03251086,\n",
       "            0.0397925 , 0.        ],\n",
       "           [0.0075408 , 0.00266943, 0.00231817, ..., 0.03198339,\n",
       "            0.03885521, 0.0423194 ]]]], dtype=float32)>,\n",
       "  'decoder_layer2_block2': <tf.Tensor: shape=(1, 8, 40, 6), dtype=float32, numpy=\n",
       "  array([[[[0.1589633 , 0.19935891, 0.17033185, 0.15619344, 0.19419882,\n",
       "            0.12095378],\n",
       "           [0.23357505, 0.2438207 , 0.16490093, 0.12509002, 0.11223707,\n",
       "            0.12037617],\n",
       "           [0.23472412, 0.24871556, 0.16511443, 0.12232467, 0.10384765,\n",
       "            0.12527356],\n",
       "           ...,\n",
       "           [0.12257112, 0.21002705, 0.27795383, 0.14069745, 0.07067466,\n",
       "            0.17807585],\n",
       "           [0.12401702, 0.20819408, 0.27270073, 0.15002298, 0.0740764 ,\n",
       "            0.17098877],\n",
       "           [0.12213024, 0.20279178, 0.26862365, 0.15844123, 0.07615471,\n",
       "            0.1718584 ]],\n",
       "  \n",
       "          [[0.18834527, 0.2086816 , 0.08821235, 0.16717012, 0.1830062 ,\n",
       "            0.16458455],\n",
       "           [0.20173223, 0.1445479 , 0.07518919, 0.11535869, 0.23384076,\n",
       "            0.22933124],\n",
       "           [0.2163034 , 0.12389354, 0.07097761, 0.09965618, 0.24215882,\n",
       "            0.24701044],\n",
       "           ...,\n",
       "           [0.30206147, 0.16476849, 0.0611373 , 0.09843902, 0.27722183,\n",
       "            0.09637184],\n",
       "           [0.30583078, 0.17415453, 0.06357543, 0.10103773, 0.26655084,\n",
       "            0.08885074],\n",
       "           [0.30479938, 0.18028338, 0.06470414, 0.10300772, 0.2657727 ,\n",
       "            0.08143265]],\n",
       "  \n",
       "          [[0.26271   , 0.17634596, 0.0872089 , 0.1270571 , 0.1298926 ,\n",
       "            0.21678548],\n",
       "           [0.18776673, 0.21383464, 0.09686949, 0.13855153, 0.15519299,\n",
       "            0.2077846 ],\n",
       "           [0.19885267, 0.22201896, 0.09967254, 0.14113572, 0.15391389,\n",
       "            0.18440622],\n",
       "           ...,\n",
       "           [0.18065043, 0.2060088 , 0.17598832, 0.20511982, 0.07986493,\n",
       "            0.15236762],\n",
       "           [0.18896607, 0.19999531, 0.16836853, 0.19487835, 0.08182646,\n",
       "            0.1659653 ],\n",
       "           [0.20089245, 0.19549039, 0.16043988, 0.18974221, 0.08630648,\n",
       "            0.1671286 ]],\n",
       "  \n",
       "          ...,\n",
       "  \n",
       "          [[0.25032753, 0.10114019, 0.12087099, 0.16168419, 0.16356649,\n",
       "            0.20241061],\n",
       "           [0.37466258, 0.09661026, 0.13055971, 0.12573738, 0.14439611,\n",
       "            0.12803389],\n",
       "           [0.36912814, 0.10348997, 0.12998149, 0.12484995, 0.15051745,\n",
       "            0.12203296],\n",
       "           ...,\n",
       "           [0.21723485, 0.15223522, 0.15705991, 0.17057864, 0.12299026,\n",
       "            0.1799011 ],\n",
       "           [0.22207886, 0.15141171, 0.15329728, 0.1686069 , 0.12113187,\n",
       "            0.18347335],\n",
       "           [0.21887341, 0.15337388, 0.14942127, 0.16645207, 0.11855762,\n",
       "            0.19332172]],\n",
       "  \n",
       "          [[0.1226762 , 0.31199685, 0.18290767, 0.1378274 , 0.10583302,\n",
       "            0.13875887],\n",
       "           [0.15579008, 0.2885604 , 0.1423417 , 0.1382519 , 0.10537167,\n",
       "            0.16968425],\n",
       "           [0.15078153, 0.27220747, 0.13400874, 0.14206973, 0.11453496,\n",
       "            0.1863976 ],\n",
       "           ...,\n",
       "           [0.08082179, 0.24151129, 0.13415137, 0.25823224, 0.12045596,\n",
       "            0.16482736],\n",
       "           [0.08295161, 0.23094758, 0.13362329, 0.2568526 , 0.12720583,\n",
       "            0.16841906],\n",
       "           [0.08479729, 0.22524893, 0.13635214, 0.2570506 , 0.13222978,\n",
       "            0.16432127]],\n",
       "  \n",
       "          [[0.02135618, 0.45601544, 0.20930858, 0.14408556, 0.07234719,\n",
       "            0.09688708],\n",
       "           [0.03756375, 0.38802004, 0.21732958, 0.19957918, 0.07158538,\n",
       "            0.08592208],\n",
       "           [0.03700903, 0.373995  , 0.22957706, 0.21361624, 0.06719926,\n",
       "            0.07860341],\n",
       "           ...,\n",
       "           [0.03193874, 0.3112196 , 0.19715147, 0.18132979, 0.05800104,\n",
       "            0.22035936],\n",
       "           [0.03281466, 0.31827623, 0.18694617, 0.17639628, 0.05901523,\n",
       "            0.2265514 ],\n",
       "           [0.03410782, 0.3230739 , 0.18059714, 0.1731822 , 0.05861569,\n",
       "            0.23042321]]]], dtype=float32)>,\n",
       "  'decoder_layer3_block1': <tf.Tensor: shape=(1, 8, 40, 40), dtype=float32, numpy=\n",
       "  array([[[[1.        , 0.        , 0.        , ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           [0.6094427 , 0.39055732, 0.        , ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           [0.40060616, 0.262004  , 0.33738983, ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           ...,\n",
       "           [0.05211992, 0.03786847, 0.04507338, ..., 0.02046137,\n",
       "            0.        , 0.        ],\n",
       "           [0.05233163, 0.03608099, 0.04265045, ..., 0.02006201,\n",
       "            0.01961447, 0.        ],\n",
       "           [0.05288159, 0.03370051, 0.03943672, ..., 0.02013183,\n",
       "            0.01958079, 0.02043851]],\n",
       "  \n",
       "          [[1.        , 0.        , 0.        , ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           [0.34316555, 0.6568345 , 0.        , ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           [0.18605374, 0.37422538, 0.43972087, ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           ...,\n",
       "           [0.02085481, 0.05449428, 0.05215329, ..., 0.02793701,\n",
       "            0.        , 0.        ],\n",
       "           [0.01909292, 0.05457699, 0.05269515, ..., 0.02609012,\n",
       "            0.02401808, 0.        ],\n",
       "           [0.01676161, 0.05138613, 0.05054335, ..., 0.02524717,\n",
       "            0.02315138, 0.02277812]],\n",
       "  \n",
       "          [[1.        , 0.        , 0.        , ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           [0.5302223 , 0.4697777 , 0.        , ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           [0.37682953, 0.32222235, 0.3009481 , ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           ...,\n",
       "           [0.07104141, 0.03354189, 0.0293505 , ..., 0.02561359,\n",
       "            0.        , 0.        ],\n",
       "           [0.07189139, 0.03423178, 0.02996938, ..., 0.02501859,\n",
       "            0.02383984, 0.        ],\n",
       "           [0.07296648, 0.03446998, 0.03010064, ..., 0.02426921,\n",
       "            0.02336748, 0.02363737]],\n",
       "  \n",
       "          ...,\n",
       "  \n",
       "          [[1.        , 0.        , 0.        , ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           [0.34701374, 0.6529863 , 0.        , ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           [0.17814913, 0.37319306, 0.4486578 , ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           ...,\n",
       "           [0.00843334, 0.02253263, 0.02566992, ..., 0.02625108,\n",
       "            0.        , 0.        ],\n",
       "           [0.00758479, 0.02201992, 0.02506742, ..., 0.02563102,\n",
       "            0.02940312, 0.        ],\n",
       "           [0.00679559, 0.02112297, 0.02369678, ..., 0.02498324,\n",
       "            0.02854961, 0.03006081]],\n",
       "  \n",
       "          [[1.        , 0.        , 0.        , ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           [0.25396577, 0.7460342 , 0.        , ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           [0.13007613, 0.38187653, 0.48804736, ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           ...,\n",
       "           [0.01860716, 0.04064405, 0.04123995, ..., 0.02022432,\n",
       "            0.        , 0.        ],\n",
       "           [0.01754016, 0.03653253, 0.03692444, ..., 0.02071412,\n",
       "            0.02165112, 0.        ],\n",
       "           [0.01683941, 0.03468425, 0.03523487, ..., 0.02051961,\n",
       "            0.02156537, 0.02328683]],\n",
       "  \n",
       "          [[1.        , 0.        , 0.        , ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           [0.53741103, 0.46258897, 0.        , ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           [0.33122957, 0.30943495, 0.35933548, ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           ...,\n",
       "           [0.01986687, 0.01918782, 0.02495412, ..., 0.02131092,\n",
       "            0.        , 0.        ],\n",
       "           [0.01932568, 0.01779833, 0.02308487, ..., 0.02076863,\n",
       "            0.01844146, 0.        ],\n",
       "           [0.01974128, 0.01709331, 0.02200871, ..., 0.01990399,\n",
       "            0.01781599, 0.01721767]]]], dtype=float32)>,\n",
       "  'decoder_layer3_block2': <tf.Tensor: shape=(1, 8, 40, 6), dtype=float32, numpy=\n",
       "  array([[[[0.23377301, 0.17527893, 0.15906812, 0.13581303, 0.12695181,\n",
       "            0.16911513],\n",
       "           [0.1850176 , 0.13767393, 0.14599654, 0.14418238, 0.15341273,\n",
       "            0.23371679],\n",
       "           [0.17287295, 0.12044919, 0.1466849 , 0.14369568, 0.17158495,\n",
       "            0.24471232],\n",
       "           ...,\n",
       "           [0.13900375, 0.08855433, 0.12401835, 0.25398678, 0.20174998,\n",
       "            0.19268683],\n",
       "           [0.142146  , 0.09197754, 0.13254985, 0.26087344, 0.18831617,\n",
       "            0.18413693],\n",
       "           [0.14700888, 0.09141453, 0.13907681, 0.2610491 , 0.17934549,\n",
       "            0.18210521]],\n",
       "  \n",
       "          [[0.09060506, 0.15146281, 0.18155277, 0.34993297, 0.13811666,\n",
       "            0.08832973],\n",
       "           [0.12051234, 0.09488611, 0.16668865, 0.36415294, 0.16397546,\n",
       "            0.08978453],\n",
       "           [0.12613921, 0.09526168, 0.1697787 , 0.35491124, 0.16840203,\n",
       "            0.08550714],\n",
       "           ...,\n",
       "           [0.14028196, 0.08379085, 0.14043751, 0.2801334 , 0.26216775,\n",
       "            0.09318847],\n",
       "           [0.13975911, 0.08266947, 0.14000474, 0.27649   , 0.26906726,\n",
       "            0.09200939],\n",
       "           [0.13400717, 0.08363125, 0.14329165, 0.27783862, 0.26565656,\n",
       "            0.09557478]],\n",
       "  \n",
       "          [[0.19817957, 0.2732738 , 0.1634663 , 0.12944058, 0.15436196,\n",
       "            0.08127784],\n",
       "           [0.20113917, 0.25607747, 0.15244643, 0.1230069 , 0.15125497,\n",
       "            0.11607508],\n",
       "           [0.1745236 , 0.22441354, 0.16496308, 0.11975339, 0.17798182,\n",
       "            0.1383645 ],\n",
       "           ...,\n",
       "           [0.0722603 , 0.13640788, 0.1481807 , 0.16703077, 0.40357405,\n",
       "            0.07254628],\n",
       "           [0.07292283, 0.1359587 , 0.14794323, 0.16534829, 0.40547508,\n",
       "            0.0723519 ],\n",
       "           [0.07237842, 0.13741176, 0.14909533, 0.1633952 , 0.4050457 ,\n",
       "            0.0726736 ]],\n",
       "  \n",
       "          ...,\n",
       "  \n",
       "          [[0.19985223, 0.258151  , 0.19907318, 0.09654458, 0.11915883,\n",
       "            0.12722021],\n",
       "           [0.20333603, 0.31356585, 0.19530714, 0.09266528, 0.0858625 ,\n",
       "            0.10926321],\n",
       "           [0.21759722, 0.30167288, 0.19311342, 0.09555993, 0.08588202,\n",
       "            0.10617453],\n",
       "           ...,\n",
       "           [0.13810848, 0.24654666, 0.29740328, 0.09427513, 0.06873854,\n",
       "            0.15492795],\n",
       "           [0.13154429, 0.25217617, 0.29950365, 0.09341255, 0.06791321,\n",
       "            0.15545012],\n",
       "           [0.12928721, 0.25618735, 0.29721528, 0.09230896, 0.06867931,\n",
       "            0.15632185]],\n",
       "  \n",
       "          [[0.2252761 , 0.19597849, 0.11329728, 0.10272565, 0.23251626,\n",
       "            0.13020623],\n",
       "           [0.20681888, 0.15635282, 0.11262476, 0.09988534, 0.24275349,\n",
       "            0.18156473],\n",
       "           [0.18804494, 0.17351481, 0.12052961, 0.10032178, 0.23490511,\n",
       "            0.18268384],\n",
       "           ...,\n",
       "           [0.27744928, 0.13798158, 0.09511826, 0.10463805, 0.26216933,\n",
       "            0.12264345],\n",
       "           [0.28576407, 0.13519837, 0.09494542, 0.10711723, 0.25680074,\n",
       "            0.12017423],\n",
       "           [0.2838295 , 0.13495424, 0.09683499, 0.1100316 , 0.25572175,\n",
       "            0.11862788]],\n",
       "  \n",
       "          [[0.0828097 , 0.07437439, 0.14235799, 0.23100434, 0.36243746,\n",
       "            0.10701609],\n",
       "           [0.14607888, 0.08071215, 0.13462476, 0.23398049, 0.32933423,\n",
       "            0.07526944],\n",
       "           [0.16055748, 0.09005178, 0.1443543 , 0.22953314, 0.29820314,\n",
       "            0.07730017],\n",
       "           ...,\n",
       "           [0.11924089, 0.07533463, 0.12227536, 0.23879123, 0.32771194,\n",
       "            0.11664587],\n",
       "           [0.11806878, 0.07757976, 0.12337352, 0.23954545, 0.32023013,\n",
       "            0.12120236],\n",
       "           [0.11755218, 0.0777076 , 0.12250115, 0.24071178, 0.31883612,\n",
       "            0.12269116]]]], dtype=float32)>,\n",
       "  'decoder_layer4_block1': <tf.Tensor: shape=(1, 8, 40, 40), dtype=float32, numpy=\n",
       "  array([[[[1.        , 0.        , 0.        , ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           [0.35633993, 0.64366007, 0.        , ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           [0.20209433, 0.39204443, 0.4058613 , ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           ...,\n",
       "           [0.01876266, 0.0218163 , 0.02012921, ..., 0.02319622,\n",
       "            0.        , 0.        ],\n",
       "           [0.01786948, 0.01979326, 0.01826745, ..., 0.0232225 ,\n",
       "            0.02485088, 0.        ],\n",
       "           [0.01714311, 0.01868103, 0.01727654, ..., 0.02302786,\n",
       "            0.02472129, 0.02631823]],\n",
       "  \n",
       "          [[1.        , 0.        , 0.        , ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           [0.5518553 , 0.44814464, 0.        , ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           [0.35729575, 0.3044671 , 0.3382372 , ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           ...,\n",
       "           [0.0488242 , 0.04810072, 0.05347211, ..., 0.01855626,\n",
       "            0.        , 0.        ],\n",
       "           [0.04715625, 0.04897884, 0.05439899, ..., 0.01795558,\n",
       "            0.01834501, 0.        ],\n",
       "           [0.04469902, 0.04913964, 0.05479531, ..., 0.01744832,\n",
       "            0.01782572, 0.01842903]],\n",
       "  \n",
       "          [[1.        , 0.        , 0.        , ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           [0.44373846, 0.55626154, 0.        , ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           [0.26439458, 0.3376606 , 0.39794484, ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           ...,\n",
       "           [0.03700463, 0.0351182 , 0.0446453 , ..., 0.01937137,\n",
       "            0.        , 0.        ],\n",
       "           [0.03545572, 0.03361228, 0.04295012, ..., 0.01917494,\n",
       "            0.02018623, 0.        ],\n",
       "           [0.03413328, 0.03300234, 0.04220887, ..., 0.01884699,\n",
       "            0.01983829, 0.01971276]],\n",
       "  \n",
       "          ...,\n",
       "  \n",
       "          [[1.        , 0.        , 0.        , ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           [0.5912874 , 0.4087126 , 0.        , ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           [0.445633  , 0.29508677, 0.2592802 , ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           ...,\n",
       "           [0.05235238, 0.04824977, 0.04169079, ..., 0.0208662 ,\n",
       "            0.        , 0.        ],\n",
       "           [0.04747855, 0.04524006, 0.03983532, ..., 0.02044563,\n",
       "            0.02042289, 0.        ],\n",
       "           [0.04273208, 0.04271245, 0.03819087, ..., 0.02026031,\n",
       "            0.0202138 , 0.02016762]],\n",
       "  \n",
       "          [[1.        , 0.        , 0.        , ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           [0.5262519 , 0.47374812, 0.        , ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           [0.32576984, 0.3285137 , 0.34571642, ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           ...,\n",
       "           [0.03246913, 0.03290895, 0.03392908, ..., 0.02673681,\n",
       "            0.        , 0.        ],\n",
       "           [0.03117993, 0.03178583, 0.03275676, ..., 0.0261866 ,\n",
       "            0.02359871, 0.        ],\n",
       "           [0.02937868, 0.0296913 , 0.0305102 , ..., 0.02591283,\n",
       "            0.02326193, 0.02104812]],\n",
       "  \n",
       "          [[1.        , 0.        , 0.        , ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           [0.42876172, 0.5712383 , 0.        , ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           [0.29488552, 0.3923633 , 0.31275117, ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           ...,\n",
       "           [0.03609934, 0.06148371, 0.05047567, ..., 0.01984764,\n",
       "            0.        , 0.        ],\n",
       "           [0.03511211, 0.06007314, 0.0492982 , ..., 0.01964443,\n",
       "            0.02009584, 0.        ],\n",
       "           [0.03470078, 0.05922699, 0.04861551, ..., 0.01929837,\n",
       "            0.01971899, 0.02015565]]]], dtype=float32)>,\n",
       "  'decoder_layer4_block2': <tf.Tensor: shape=(1, 8, 40, 6), dtype=float32, numpy=\n",
       "  array([[[[0.16163452, 0.14313911, 0.10229741, 0.14971425, 0.16325149,\n",
       "            0.27996328],\n",
       "           [0.15245703, 0.12379481, 0.06020167, 0.09506019, 0.21647213,\n",
       "            0.35201415],\n",
       "           [0.1642332 , 0.11920552, 0.05292064, 0.08178974, 0.21761568,\n",
       "            0.36423522],\n",
       "           ...,\n",
       "           [0.16413093, 0.20417127, 0.12243874, 0.19992998, 0.14551619,\n",
       "            0.1638129 ],\n",
       "           [0.16514224, 0.21108024, 0.12172031, 0.19745907, 0.14606132,\n",
       "            0.15853685],\n",
       "           [0.16221778, 0.21976006, 0.12283478, 0.19786276, 0.14245136,\n",
       "            0.15487322]],\n",
       "  \n",
       "          [[0.07414702, 0.13437106, 0.13983381, 0.1862536 , 0.23041777,\n",
       "            0.2349768 ],\n",
       "           [0.07655162, 0.09733374, 0.14115295, 0.2030641 , 0.14879824,\n",
       "            0.3330993 ],\n",
       "           [0.07987561, 0.09433176, 0.1475973 , 0.19882733, 0.13402212,\n",
       "            0.3453459 ],\n",
       "           ...,\n",
       "           [0.05557065, 0.14461716, 0.24315381, 0.22430086, 0.13974684,\n",
       "            0.19261067],\n",
       "           [0.05683113, 0.14809437, 0.2423876 , 0.22228281, 0.14202042,\n",
       "            0.18838367],\n",
       "           [0.05646694, 0.15123357, 0.24180146, 0.21980248, 0.14277984,\n",
       "            0.18791574]],\n",
       "  \n",
       "          [[0.22323953, 0.10927415, 0.1043424 , 0.09204934, 0.16380842,\n",
       "            0.30728614],\n",
       "           [0.22022207, 0.12009241, 0.09875295, 0.07763985, 0.21465422,\n",
       "            0.26863846],\n",
       "           [0.22061282, 0.11834491, 0.09954564, 0.07162701, 0.22460988,\n",
       "            0.26525974],\n",
       "           ...,\n",
       "           [0.2137171 , 0.19251794, 0.09641606, 0.09597559, 0.16552731,\n",
       "            0.235846  ],\n",
       "           [0.21256147, 0.19494721, 0.09778351, 0.09576987, 0.16143693,\n",
       "            0.2375009 ],\n",
       "           [0.2114898 , 0.19823912, 0.09930866, 0.09456598, 0.15523195,\n",
       "            0.24116455]],\n",
       "  \n",
       "          ...,\n",
       "  \n",
       "          [[0.28257865, 0.26242125, 0.13247482, 0.07392271, 0.09159476,\n",
       "            0.15700778],\n",
       "           [0.30234596, 0.22603081, 0.09502578, 0.06098244, 0.1125156 ,\n",
       "            0.20309937],\n",
       "           [0.3008867 , 0.22985984, 0.09660781, 0.06427826, 0.11331752,\n",
       "            0.19504991],\n",
       "           ...,\n",
       "           [0.4335517 , 0.14916043, 0.05986989, 0.0487816 , 0.09946677,\n",
       "            0.20916958],\n",
       "           [0.43863738, 0.1461562 , 0.0589215 , 0.04880698, 0.10017661,\n",
       "            0.20730129],\n",
       "           [0.4366995 , 0.14506991, 0.05864084, 0.04904715, 0.10228544,\n",
       "            0.2082571 ]],\n",
       "  \n",
       "          [[0.11970162, 0.1084971 , 0.121549  , 0.16379222, 0.23443888,\n",
       "            0.25202116],\n",
       "           [0.08429789, 0.11936251, 0.1308032 , 0.14957227, 0.22662753,\n",
       "            0.28933662],\n",
       "           [0.07969066, 0.12549092, 0.1387544 , 0.14633046, 0.21777289,\n",
       "            0.29196063],\n",
       "           ...,\n",
       "           [0.10249632, 0.12054517, 0.13275687, 0.13527888, 0.28312334,\n",
       "            0.22579941],\n",
       "           [0.10076299, 0.12183522, 0.13592806, 0.13798024, 0.27725375,\n",
       "            0.22623973],\n",
       "           [0.10087837, 0.1220603 , 0.13726273, 0.13880093, 0.27446663,\n",
       "            0.22653104]],\n",
       "  \n",
       "          [[0.20808321, 0.1148254 , 0.18915622, 0.20871136, 0.14642908,\n",
       "            0.13279472],\n",
       "           [0.21158421, 0.09303804, 0.17948717, 0.20234545, 0.17529476,\n",
       "            0.13825038],\n",
       "           [0.19431423, 0.09192296, 0.18905196, 0.21456686, 0.16850615,\n",
       "            0.14163785],\n",
       "           ...,\n",
       "           [0.29114777, 0.08215231, 0.14913538, 0.13901527, 0.17716467,\n",
       "            0.16138455],\n",
       "           [0.28304908, 0.0833137 , 0.1526171 , 0.14171705, 0.17504802,\n",
       "            0.16425508],\n",
       "           [0.27839074, 0.08423955, 0.15562248, 0.14281033, 0.17369948,\n",
       "            0.16523744]]]], dtype=float32)>,\n",
       "  'decoder_layer5_block1': <tf.Tensor: shape=(1, 8, 40, 40), dtype=float32, numpy=\n",
       "  array([[[[1.        , 0.        , 0.        , ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           [0.29344746, 0.7065525 , 0.        , ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           [0.14340407, 0.37964725, 0.4769486 , ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           ...,\n",
       "           [0.02983445, 0.05163673, 0.05644266, ..., 0.02192205,\n",
       "            0.        , 0.        ],\n",
       "           [0.02914893, 0.05048337, 0.05517628, ..., 0.02153325,\n",
       "            0.02063881, 0.        ],\n",
       "           [0.02876369, 0.04937754, 0.05385633, ..., 0.02102161,\n",
       "            0.02015827, 0.01942869]],\n",
       "  \n",
       "          [[1.        , 0.        , 0.        , ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           [0.62746954, 0.37253046, 0.        , ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           [0.47496217, 0.27910036, 0.2459375 , ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           ...,\n",
       "           [0.03448937, 0.02417758, 0.02119294, ..., 0.02766921,\n",
       "            0.        , 0.        ],\n",
       "           [0.03422748, 0.02366327, 0.02064434, ..., 0.02670858,\n",
       "            0.02672323, 0.        ],\n",
       "           [0.03339341, 0.02311379, 0.02013931, ..., 0.02582387,\n",
       "            0.0258288 , 0.02565542]],\n",
       "  \n",
       "          [[1.        , 0.        , 0.        , ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           [0.53607404, 0.46392596, 0.        , ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           [0.34638718, 0.31915617, 0.3344567 , ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           ...,\n",
       "           [0.01668251, 0.01521515, 0.01515286, ..., 0.03202642,\n",
       "            0.        , 0.        ],\n",
       "           [0.01589806, 0.01469692, 0.01467191, ..., 0.03102957,\n",
       "            0.03005865, 0.        ],\n",
       "           [0.01519488, 0.01414781, 0.0141563 , ..., 0.0301854 ,\n",
       "            0.02923892, 0.02861274]],\n",
       "  \n",
       "          ...,\n",
       "  \n",
       "          [[1.        , 0.        , 0.        , ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           [0.442044  , 0.557956  , 0.        , ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           [0.26972306, 0.33196813, 0.39830884, ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           ...,\n",
       "           [0.02787577, 0.02451484, 0.02816205, ..., 0.02327628,\n",
       "            0.        , 0.        ],\n",
       "           [0.02665938, 0.02308781, 0.02644928, ..., 0.0230088 ,\n",
       "            0.02337393, 0.        ],\n",
       "           [0.02544678, 0.02198779, 0.02505935, ..., 0.02269767,\n",
       "            0.02305476, 0.02372899]],\n",
       "  \n",
       "          [[1.        , 0.        , 0.        , ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           [0.47497627, 0.5250237 , 0.        , ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           [0.3234042 , 0.35937175, 0.317224  , ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           ...,\n",
       "           [0.02081854, 0.02119023, 0.01973918, ..., 0.02986137,\n",
       "            0.        , 0.        ],\n",
       "           [0.01940343, 0.02000979, 0.0186565 , ..., 0.02919745,\n",
       "            0.02902158, 0.        ],\n",
       "           [0.01846453, 0.01925948, 0.0179526 , ..., 0.02846294,\n",
       "            0.02829052, 0.02769775]],\n",
       "  \n",
       "          [[1.        , 0.        , 0.        , ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           [0.5005003 , 0.49949968, 0.        , ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           [0.3324628 , 0.3324027 , 0.33513454, ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           ...,\n",
       "           [0.00809221, 0.00878405, 0.00968854, ..., 0.02432177,\n",
       "            0.        , 0.        ],\n",
       "           [0.00789336, 0.00865522, 0.00956716, ..., 0.02355088,\n",
       "            0.02370878, 0.        ],\n",
       "           [0.00757523, 0.00830578, 0.00917358, ..., 0.02286742,\n",
       "            0.02302813, 0.02330948]]]], dtype=float32)>,\n",
       "  'decoder_layer5_block2': <tf.Tensor: shape=(1, 8, 40, 6), dtype=float32, numpy=\n",
       "  array([[[[0.08926252, 0.34021327, 0.18490902, 0.16082725, 0.07895553,\n",
       "            0.14583243],\n",
       "           [0.07347225, 0.347456  , 0.20338245, 0.16337295, 0.08498964,\n",
       "            0.12732671],\n",
       "           [0.07354168, 0.34440538, 0.20708957, 0.16882113, 0.08622129,\n",
       "            0.11992091],\n",
       "           ...,\n",
       "           [0.1264912 , 0.331864  , 0.16889621, 0.12888321, 0.11432222,\n",
       "            0.12954317],\n",
       "           [0.12657122, 0.3397428 , 0.16589862, 0.12718551, 0.11137827,\n",
       "            0.12922361],\n",
       "           [0.12438435, 0.3494743 , 0.16343798, 0.1259686 , 0.10892386,\n",
       "            0.1278109 ]],\n",
       "  \n",
       "          [[0.27672437, 0.10295878, 0.1285396 , 0.16488758, 0.24071434,\n",
       "            0.08617529],\n",
       "           [0.29090178, 0.10934639, 0.1363007 , 0.1598153 , 0.21121687,\n",
       "            0.09241892],\n",
       "           [0.28881586, 0.10948691, 0.13956322, 0.15457857, 0.21139139,\n",
       "            0.09616408],\n",
       "           ...,\n",
       "           [0.26289353, 0.08399883, 0.09970973, 0.14896497, 0.201447  ,\n",
       "            0.20298596],\n",
       "           [0.2640735 , 0.08279461, 0.09875803, 0.14779288, 0.20124608,\n",
       "            0.20533492],\n",
       "           [0.26594698, 0.08119316, 0.09779973, 0.14527875, 0.20457532,\n",
       "            0.20520605]],\n",
       "  \n",
       "          [[0.18389702, 0.12948269, 0.12377833, 0.15365207, 0.11027321,\n",
       "            0.29891664],\n",
       "           [0.16950528, 0.14009328, 0.12836307, 0.13830134, 0.13138299,\n",
       "            0.292354  ],\n",
       "           [0.16619237, 0.14040148, 0.12964904, 0.13400538, 0.14026482,\n",
       "            0.28948694],\n",
       "           ...,\n",
       "           [0.22939199, 0.13237824, 0.11790941, 0.1442868 , 0.15169172,\n",
       "            0.22434177],\n",
       "           [0.22869581, 0.13324964, 0.11598633, 0.14070994, 0.15438583,\n",
       "            0.22697243],\n",
       "           [0.23029114, 0.13323705, 0.11428636, 0.13781884, 0.15490623,\n",
       "            0.22946039]],\n",
       "  \n",
       "          ...,\n",
       "  \n",
       "          [[0.29587275, 0.21079405, 0.12595934, 0.1088635 , 0.07201561,\n",
       "            0.18649477],\n",
       "           [0.34224543, 0.15334344, 0.11714473, 0.13372813, 0.07902028,\n",
       "            0.17451805],\n",
       "           [0.34622014, 0.1464232 , 0.11461997, 0.13758199, 0.07696714,\n",
       "            0.17818749],\n",
       "           ...,\n",
       "           [0.2598066 , 0.14589809, 0.1257306 , 0.19573146, 0.11151694,\n",
       "            0.16131628],\n",
       "           [0.25840297, 0.14983663, 0.1272232 , 0.19633138, 0.1095167 ,\n",
       "            0.15868917],\n",
       "           [0.25575104, 0.15287153, 0.12864903, 0.19649531, 0.10841709,\n",
       "            0.157816  ]],\n",
       "  \n",
       "          [[0.1506581 , 0.19499873, 0.17794895, 0.08389765, 0.08038232,\n",
       "            0.31211427],\n",
       "           [0.15557629, 0.17712772, 0.1601454 , 0.0705331 , 0.06917211,\n",
       "            0.36744535],\n",
       "           [0.1617799 , 0.17054991, 0.1535695 , 0.0657609 , 0.0695857 ,\n",
       "            0.37875405],\n",
       "           ...,\n",
       "           [0.16019139, 0.15797043, 0.222563  , 0.10255083, 0.06899045,\n",
       "            0.2877339 ],\n",
       "           [0.16178755, 0.15763752, 0.22566287, 0.10259923, 0.06804731,\n",
       "            0.28426552],\n",
       "           [0.16546372, 0.15613203, 0.22726524, 0.10267264, 0.06630206,\n",
       "            0.2821643 ]],\n",
       "  \n",
       "          [[0.3630215 , 0.08271216, 0.07844152, 0.20441903, 0.17134959,\n",
       "            0.10005624],\n",
       "           [0.33569932, 0.08078372, 0.06944061, 0.2217256 , 0.19081002,\n",
       "            0.10154068],\n",
       "           [0.32110798, 0.07516254, 0.06544886, 0.21878272, 0.21154921,\n",
       "            0.10794879],\n",
       "           ...,\n",
       "           [0.27606976, 0.07125061, 0.06071952, 0.22640139, 0.21388221,\n",
       "            0.15167645],\n",
       "           [0.27499816, 0.07046268, 0.06008706, 0.22076368, 0.21733   ,\n",
       "            0.15635844],\n",
       "           [0.27157447, 0.06965501, 0.06015287, 0.22228017, 0.21997355,\n",
       "            0.15636389]]]], dtype=float32)>,\n",
       "  'decoder_layer6_block1': <tf.Tensor: shape=(1, 8, 40, 40), dtype=float32, numpy=\n",
       "  array([[[[1.        , 0.        , 0.        , ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           [0.49825457, 0.50174546, 0.        , ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           [0.31868806, 0.32086784, 0.36044416, ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           ...,\n",
       "           [0.01361949, 0.01252489, 0.01302432, ..., 0.03164195,\n",
       "            0.        , 0.        ],\n",
       "           [0.01300017, 0.01191531, 0.0124028 , ..., 0.03087923,\n",
       "            0.03060052, 0.        ],\n",
       "           [0.01246351, 0.01136985, 0.01184121, ..., 0.03009032,\n",
       "            0.02982286, 0.02945742]],\n",
       "  \n",
       "          [[1.        , 0.        , 0.        , ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           [0.428643  , 0.571357  , 0.        , ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           [0.27857447, 0.36063564, 0.3607899 , ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           ...,\n",
       "           [0.01018426, 0.01540162, 0.015599  , ..., 0.03046309,\n",
       "            0.        , 0.        ],\n",
       "           [0.00989874, 0.01493903, 0.01509669, ..., 0.02964643,\n",
       "            0.0290812 , 0.        ],\n",
       "           [0.00958055, 0.0144965 , 0.01465644, ..., 0.02885639,\n",
       "            0.02831399, 0.02739182]],\n",
       "  \n",
       "          [[1.        , 0.        , 0.        , ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           [0.5339279 , 0.46607202, 0.        , ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           [0.37571388, 0.32696295, 0.2973232 , ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           ...,\n",
       "           [0.02575974, 0.02393823, 0.02354679, ..., 0.02599708,\n",
       "            0.        , 0.        ],\n",
       "           [0.02459444, 0.0232902 , 0.02300839, ..., 0.02543144,\n",
       "            0.0256524 , 0.        ],\n",
       "           [0.02370325, 0.02271769, 0.02248475, ..., 0.02478771,\n",
       "            0.0250112 , 0.02495868]],\n",
       "  \n",
       "          ...,\n",
       "  \n",
       "          [[1.        , 0.        , 0.        , ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           [0.46931368, 0.53068626, 0.        , ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           [0.31572893, 0.35329244, 0.33097863, ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           ...,\n",
       "           [0.03120807, 0.0333543 , 0.03002246, ..., 0.02491541,\n",
       "            0.        , 0.        ],\n",
       "           [0.03060224, 0.03262533, 0.02930897, ..., 0.02423759,\n",
       "            0.02481257, 0.        ],\n",
       "           [0.03009492, 0.03188525, 0.0285682 , ..., 0.02363039,\n",
       "            0.02420397, 0.02479986]],\n",
       "  \n",
       "          [[1.        , 0.        , 0.        , ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           [0.38167682, 0.61832315, 0.        , ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           [0.22387078, 0.3573389 , 0.4187903 , ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           ...,\n",
       "           [0.01682482, 0.02590507, 0.02901606, ..., 0.02738199,\n",
       "            0.        , 0.        ],\n",
       "           [0.01644463, 0.02519817, 0.0281121 , ..., 0.02655788,\n",
       "            0.02719556, 0.        ],\n",
       "           [0.01626137, 0.02461901, 0.02729418, ..., 0.02580506,\n",
       "            0.02643785, 0.02673479]],\n",
       "  \n",
       "          [[1.        , 0.        , 0.        , ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           [0.40272003, 0.59727997, 0.        , ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           [0.24537648, 0.36192057, 0.3927029 , ..., 0.        ,\n",
       "            0.        , 0.        ],\n",
       "           ...,\n",
       "           [0.01322511, 0.0172874 , 0.01808399, ..., 0.02676911,\n",
       "            0.        , 0.        ],\n",
       "           [0.0129636 , 0.01694178, 0.01774149, ..., 0.02588633,\n",
       "            0.02497119, 0.        ],\n",
       "           [0.01261808, 0.01650194, 0.01731254, ..., 0.02509438,\n",
       "            0.02421418, 0.02341297]]]], dtype=float32)>,\n",
       "  'decoder_layer6_block2': <tf.Tensor: shape=(1, 8, 40, 6), dtype=float32, numpy=\n",
       "  array([[[[0.06562534, 0.27648634, 0.2814583 , 0.12644102, 0.18468878,\n",
       "            0.06530018],\n",
       "           [0.06676915, 0.28384745, 0.24178818, 0.11863773, 0.21188013,\n",
       "            0.07707734],\n",
       "           [0.06389597, 0.28992462, 0.23657547, 0.12231684, 0.20779012,\n",
       "            0.07949702],\n",
       "           ...,\n",
       "           [0.03768702, 0.32223102, 0.2977098 , 0.11396641, 0.14997831,\n",
       "            0.0784274 ],\n",
       "           [0.03721924, 0.32350168, 0.29829705, 0.1133974 , 0.14903624,\n",
       "            0.07854837],\n",
       "           [0.03706226, 0.32370853, 0.29900587, 0.11307243, 0.14895643,\n",
       "            0.07819448]],\n",
       "  \n",
       "          [[0.13119963, 0.10443199, 0.15367691, 0.220865  , 0.1967377 ,\n",
       "            0.19308874],\n",
       "           [0.13986026, 0.08525664, 0.12352215, 0.21807274, 0.2580828 ,\n",
       "            0.17520544],\n",
       "           [0.13800147, 0.08345909, 0.12104338, 0.21844341, 0.26654863,\n",
       "            0.17250401],\n",
       "           ...,\n",
       "           [0.08103472, 0.11204322, 0.14366746, 0.21510825, 0.2587955 ,\n",
       "            0.18935084],\n",
       "           [0.0810241 , 0.1115029 , 0.14374068, 0.21376926, 0.258082  ,\n",
       "            0.19188108],\n",
       "           [0.08072162, 0.11141677, 0.14366452, 0.212413  , 0.2586124 ,\n",
       "            0.19317171]],\n",
       "  \n",
       "          [[0.07800732, 0.16189624, 0.18481575, 0.14080884, 0.09343304,\n",
       "            0.34103885],\n",
       "           [0.08029184, 0.15854535, 0.18765123, 0.1405103 , 0.11925872,\n",
       "            0.31374258],\n",
       "           [0.0824036 , 0.15339237, 0.18067473, 0.14018528, 0.13814281,\n",
       "            0.3052012 ],\n",
       "           ...,\n",
       "           [0.09866663, 0.15890749, 0.16825204, 0.16881505, 0.10767245,\n",
       "            0.29768637],\n",
       "           [0.09881646, 0.16052347, 0.16970736, 0.17116423, 0.10750671,\n",
       "            0.29228175],\n",
       "           [0.10064423, 0.16317442, 0.17174329, 0.17319317, 0.10714353,\n",
       "            0.28410137]],\n",
       "  \n",
       "          ...,\n",
       "  \n",
       "          [[0.22218126, 0.10629433, 0.17101869, 0.09753698, 0.17486228,\n",
       "            0.22810647],\n",
       "           [0.23719218, 0.10786288, 0.16636406, 0.09925358, 0.15670884,\n",
       "            0.2326185 ],\n",
       "           [0.23926876, 0.10413099, 0.1583267 , 0.0985357 , 0.15410908,\n",
       "            0.24562873],\n",
       "           ...,\n",
       "           [0.20359041, 0.10730637, 0.18022165, 0.12435568, 0.13852382,\n",
       "            0.24600205],\n",
       "           [0.20367657, 0.10558877, 0.17896369, 0.12402193, 0.1401259 ,\n",
       "            0.24762313],\n",
       "           [0.20313887, 0.10459013, 0.17793314, 0.12396213, 0.14225702,\n",
       "            0.24811877]],\n",
       "  \n",
       "          [[0.08930893, 0.17407033, 0.22634807, 0.16455048, 0.06245961,\n",
       "            0.28326252],\n",
       "           [0.10569178, 0.19850516, 0.21141596, 0.15217353, 0.05832461,\n",
       "            0.27388898],\n",
       "           [0.11125763, 0.21992178, 0.2016065 , 0.14911827, 0.05857966,\n",
       "            0.25951615],\n",
       "           ...,\n",
       "           [0.06734017, 0.26770416, 0.21503927, 0.18310967, 0.07148115,\n",
       "            0.19532557],\n",
       "           [0.06550389, 0.2652552 , 0.21582635, 0.1856238 , 0.07119145,\n",
       "            0.19659922],\n",
       "           [0.0644839 , 0.26242813, 0.21641524, 0.18699203, 0.07125996,\n",
       "            0.19842064]],\n",
       "  \n",
       "          [[0.10919492, 0.1899209 , 0.18818396, 0.12954496, 0.21132797,\n",
       "            0.17182729],\n",
       "           [0.11723785, 0.17454112, 0.19154894, 0.11937632, 0.21394017,\n",
       "            0.18335555],\n",
       "           [0.11943781, 0.1651787 , 0.1984442 , 0.11715941, 0.21135041,\n",
       "            0.1884294 ],\n",
       "           ...,\n",
       "           [0.11141384, 0.2079159 , 0.21595718, 0.12624632, 0.19147168,\n",
       "            0.14699513],\n",
       "           [0.11018959, 0.20927313, 0.2152497 , 0.12544829, 0.19137411,\n",
       "            0.14846513],\n",
       "           [0.10918193, 0.20809728, 0.21456575, 0.12608212, 0.19104594,\n",
       "            0.15102695]]]], dtype=float32)>})"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_eng_to_chi(transformer_3, \"how are you?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_eng_to_chi(transformer, sentence):\n",
    "    result, attention_weights = evaluate_eng_to_chi(transformer, sentence)\n",
    "\n",
    "    predicted_sentence = tokenizer_chi_2.decode([i for i in result\n",
    "                                              if i < tokenizer_chi_2.vocab_size])\n",
    "\n",
    "#     print('Input: {}'.format(sentence))\n",
    "#     print('Predicted Translation: {}'.format(predicted_sentence))\n",
    "    \n",
    "    return predicted_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'讓我們放讓我們放這是湯姆這是湯姆這是湯姆這是湯姆這是湯姆這是湯姆這是湯姆這是湯姆這是湯姆這是湯姆這是湯姆這是湯姆這是湯姆這是湯姆這是湯姆這是湯姆這是湯姆這是湯姆這是湯姆這是湯姆這是湯姆這是湯姆這是湯姆這是湯姆這是湯姆這是湯姆這是湯姆這是湯姆這是湯姆這是湯姆這是湯姆這是湯姆這是湯姆這是湯姆這是湯姆這是湯姆這是湯姆這是湯姆'"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate_eng_to_chi(transformer_3, \"how are you?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x23cd27c8708>"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer_eng_to_chi = Transformer(config=config,\n",
    "                          target_vocab_size=target_vocab_size_3,\n",
    "                          bert_config_file=bert_config_file)\n",
    "  \n",
    "# fn_out, _ = transformer_eng_to_chi(inp, tar_inp, \n",
    "#                         True,\n",
    "#                         look_ahead_mask=None,\n",
    "#                         dec_padding_mask=None)\n",
    "transformer_eng_to_chi.load_weights('/weights/nmt_en_2_chi_bert_ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'你要去哪裡哪儿，你的哪处。'"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate_eng_to_chi(transformer_eng_to_chi, \"where are you going?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'我會去去路。'"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = '你要去哪裡？'\n",
    "\n",
    "chi_to_eng = translate_chi_to_eng(transformer_chi_to_eng, sentence)\n",
    "convo = chatbot(transformer_chatbot, chi_to_eng)\n",
    "translate_eng_to_chi(transformer_eng_to_chi, convo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nmt_and_chatbot(sentence):\n",
    "    chi_to_eng = translate_chi_to_eng(transformer_chi_to_eng, sentence)\n",
    "    convo = chatbot(transformer_chatbot, chi_to_eng)\n",
    "    eng_to_chi = translate_eng_to_chi(transformer_eng_to_chi, convo)\n",
    "    \n",
    "    print('Input: {}'.format(sentence))\n",
    "    print('Predicted Translation (chi to eng): {}'.format(chi_to_eng))\n",
    "    print('Predicted Response (chatbot): {}'.format(convo))\n",
    "    print('Predicted Translation (eng to chi): {}'.format(eng_to_chi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 你要去哪裡？\n",
      "Predicted Translation (chi to eng): where are you going to go ?\n",
      "Predicted Response (chatbot): i ll go .\n",
      "Predicted Translation (eng to chi): 我會去去路。\n"
     ]
    }
   ],
   "source": [
    "nmt_and_chatbot('你要去哪裡？')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 很高興認識你\n",
      "Predicted Translation (chi to eng): mary knew mary s happy .\n",
      "Predicted Response (chatbot): he was dead .\n",
      "Predicted Translation (eng to chi): 他在死死了。\n"
     ]
    }
   ],
   "source": [
    "nmt_and_chatbot('很高興認識你')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 我飽了\n",
      "Predicted Translation (chi to eng): i m wrong .\n",
      "Predicted Response (chatbot): what ?\n",
      "Predicted Translation (eng to chi): “什麼，甚麼？\n"
     ]
    }
   ],
   "source": [
    "nmt_and_chatbot('我飽了')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 我愛你\n",
      "Predicted Translation (chi to eng): i love the much .\n",
      "Predicted Response (chatbot): it s a bit of men for men for men for men for men for men for men for men for men for men for men for men for men for men for men for men for men for men \n",
      "Predicted Translation (eng to chi): 男人是男人是當男人。\n"
     ]
    }
   ],
   "source": [
    "nmt_and_chatbot('我愛你')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
