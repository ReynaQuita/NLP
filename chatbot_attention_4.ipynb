{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "yWGYVfHNrqxx"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import unicodedata\n",
    "import re\n",
    "import numpy as np\n",
    "import os\n",
    "import io\n",
    "import time\n",
    "import  nltk.translate.bleu_score as bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "gAs78PAOr2jd"
   },
   "outputs": [],
   "source": [
    "path_to_zip = tf.keras.utils.get_file(\n",
    "    'cornell_movie_dialogs.zip',\n",
    "    origin=\n",
    "    'http://www.cs.cornell.edu/~cristian/data/cornell_movie_dialogs_corpus.zip',\n",
    "    extract=True)\n",
    "\n",
    "path_to_dataset = os.path.join(\n",
    "    os.path.dirname(path_to_zip), \"cornell movie-dialogs corpus\")\n",
    "\n",
    "path_to_movie_lines = os.path.join(path_to_dataset, 'movie_lines.txt')\n",
    "path_to_movie_conversations = os.path.join(path_to_dataset,\n",
    "                                           'movie_conversations.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "61HzaIDjud1G"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "UuHIIHb7sEv8"
   },
   "outputs": [],
   "source": [
    "def preprocess_sentence(sentence):\n",
    "  sentence = sentence.lower().strip()\n",
    "  # creating a space between a word and the punctuation following it\n",
    "  # eg: \"he is a boy.\" => \"he is a boy .\"\n",
    "  sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)\n",
    "  sentence = re.sub(r'[\" \"]+', \" \", sentence)\n",
    "  # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n",
    "  sentence = re.sub(r\"[^a-zA-Z?.!,]+\", \" \", sentence)\n",
    "  sentence = sentence.strip()\n",
    "  # adding a start and an end token to the sentence\n",
    "  # so that the model know when to start and stop predicting.\n",
    "  sentence = '<start> ' + sentence + ' <end>'\n",
    "  return sentence\n",
    "\n",
    "\n",
    "def load_conversations(num1,num2):\n",
    "  # dictionary of line id to text\n",
    "  id2line = {}\n",
    "  with open(path_to_movie_lines, errors='ignore') as file:\n",
    "    lines = file.readlines()\n",
    "  for line in lines:\n",
    "    parts = line.replace('\\n', '').split(' +++$+++ ')\n",
    "    id2line[parts[0]] = parts[4]\n",
    "\n",
    "  inputs, outputs = [], []\n",
    "  with open(path_to_movie_conversations, 'r') as file:\n",
    "    lines = file.readlines()\n",
    "  for line in lines[num1:num2]:\n",
    "    parts = line.replace('\\n', '').split(' +++$+++ ')\n",
    "    # get conversation in a list of line ID\n",
    "    conversation = [line[1:-1] for line in parts[3][1:-1].split(', ')]\n",
    "    for i in range(len(conversation) - 1):\n",
    "      inputs.append(preprocess_sentence(id2line[conversation[i]]))\n",
    "      outputs.append(preprocess_sentence(id2line[conversation[i + 1]]))\n",
    "#       if len(inputs) >= MAX_SAMPLES:\n",
    "#         return inputs, outputs\n",
    "  return inputs, outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Jja8pH4AsLs0",
    "outputId": "b780fdd1-84ac-4490-a31f-7229236dd207"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> may i borrow this book ? <end>\n",
      "b'<start> puedo tomar prestado este libro ? <end>'\n"
     ]
    }
   ],
   "source": [
    "en_sentence = u\"May I borrow this book?\"\n",
    "sp_sentence = u\"Â¿Puedo tomar prestado este libro?\"\n",
    "print(preprocess_sentence(en_sentence))\n",
    "print(preprocess_sentence(sp_sentence).encode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "BkiB6-dksQXE"
   },
   "outputs": [],
   "source": [
    "def tokenize(lang):\n",
    "  lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "      filters='')\n",
    "  lang_tokenizer.fit_on_texts(lang)\n",
    "\n",
    "  tensor = lang_tokenizer.texts_to_sequences(lang)\n",
    "\n",
    "  tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor,\n",
    "                                                         padding='post')\n",
    "\n",
    "  return tensor, lang_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "7ZmYB5Jmsnj0"
   },
   "outputs": [],
   "source": [
    "def load_dataset(num1,num2):\n",
    "  # creating cleaned input, output pairs\n",
    "  inp_lang, targ_lang = load_conversations(num1,num2)\n",
    "\n",
    "  input_tensor, inp_lang_tokenizer = tokenize(inp_lang)\n",
    "  target_tensor, targ_lang_tokenizer = tokenize(targ_lang)\n",
    "\n",
    "  return input_tensor, target_tensor, inp_lang_tokenizer, targ_lang_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "4an2k3cZs6x2"
   },
   "outputs": [],
   "source": [
    "num1 = 0\n",
    "num2 = 5000\n",
    "input_tensor, target_tensor, inp_lang, targ_lang = load_dataset(num1,num2)\n",
    "\n",
    "# Calculate max_length of the target tensors\n",
    "max_length_targ, max_length_inp = target_tensor.shape[1], input_tensor.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: '.',\n",
       " 2: '<start>',\n",
       " 3: '<end>',\n",
       " 4: ',',\n",
       " 5: 'you',\n",
       " 6: 'i',\n",
       " 7: '?',\n",
       " 8: 'the',\n",
       " 9: 'to',\n",
       " 10: 'a',\n",
       " 11: 's',\n",
       " 12: 'it',\n",
       " 13: 't',\n",
       " 14: 'that',\n",
       " 15: 'and',\n",
       " 16: 'what',\n",
       " 17: 'of',\n",
       " 18: '!',\n",
       " 19: 'me',\n",
       " 20: 'in',\n",
       " 21: 'is',\n",
       " 22: 'we',\n",
       " 23: 'he',\n",
       " 24: 'this',\n",
       " 25: 'for',\n",
       " 26: 'don',\n",
       " 27: 'do',\n",
       " 28: 'm',\n",
       " 29: 'my',\n",
       " 30: 'your',\n",
       " 31: 'have',\n",
       " 32: 'know',\n",
       " 33: 'not',\n",
       " 34: 'was',\n",
       " 35: 'be',\n",
       " 36: 'are',\n",
       " 37: 're',\n",
       " 38: 'on',\n",
       " 39: 'no',\n",
       " 40: 'with',\n",
       " 41: 'but',\n",
       " 42: 'can',\n",
       " 43: 'they',\n",
       " 44: 'like',\n",
       " 45: 'just',\n",
       " 46: 'all',\n",
       " 47: 'about',\n",
       " 48: 'get',\n",
       " 49: 'how',\n",
       " 50: 'so',\n",
       " 51: 'if',\n",
       " 52: 'there',\n",
       " 53: 'here',\n",
       " 54: 'll',\n",
       " 55: 'out',\n",
       " 56: 'up',\n",
       " 57: 'him',\n",
       " 58: 'think',\n",
       " 59: 'got',\n",
       " 60: 'want',\n",
       " 61: 'one',\n",
       " 62: 'go',\n",
       " 63: 've',\n",
       " 64: 'why',\n",
       " 65: 'at',\n",
       " 66: 'now',\n",
       " 67: 'she',\n",
       " 68: 'yes',\n",
       " 69: 'did',\n",
       " 70: 'who',\n",
       " 71: 'right',\n",
       " 72: 'well',\n",
       " 73: 'her',\n",
       " 74: 'see',\n",
       " 75: 'going',\n",
       " 76: 'd',\n",
       " 77: 'tell',\n",
       " 78: 'as',\n",
       " 79: 'been',\n",
       " 80: 'some',\n",
       " 81: 'where',\n",
       " 82: 'good',\n",
       " 83: 'oh',\n",
       " 84: 'when',\n",
       " 85: 'man',\n",
       " 86: 'let',\n",
       " 87: 'an',\n",
       " 88: 'his',\n",
       " 89: 'from',\n",
       " 90: 'say',\n",
       " 91: 'would',\n",
       " 92: 'or',\n",
       " 93: 'yeah',\n",
       " 94: 'will',\n",
       " 95: 'were',\n",
       " 96: 'didn',\n",
       " 97: 'come',\n",
       " 98: 'then',\n",
       " 99: 'could',\n",
       " 100: 'look',\n",
       " 101: 'back',\n",
       " 102: 'time',\n",
       " 103: 'them',\n",
       " 104: 'really',\n",
       " 105: 'something',\n",
       " 106: 'had',\n",
       " 107: 'us',\n",
       " 108: 'take',\n",
       " 109: 'too',\n",
       " 110: 'okay',\n",
       " 111: 'any',\n",
       " 112: 'by',\n",
       " 113: 'never',\n",
       " 114: 'little',\n",
       " 115: 'gonna',\n",
       " 116: 'more',\n",
       " 117: 'way',\n",
       " 118: 'very',\n",
       " 119: 'mean',\n",
       " 120: 'need',\n",
       " 121: 'make',\n",
       " 122: 'people',\n",
       " 123: 'only',\n",
       " 124: 'much',\n",
       " 125: 'has',\n",
       " 126: 'our',\n",
       " 127: 'thing',\n",
       " 128: 'off',\n",
       " 129: 'maybe',\n",
       " 130: 'over',\n",
       " 131: 'two',\n",
       " 132: 'love',\n",
       " 133: 'said',\n",
       " 134: 'am',\n",
       " 135: 'sorry',\n",
       " 136: 'down',\n",
       " 137: 'sir',\n",
       " 138: 'money',\n",
       " 139: 'talk',\n",
       " 140: 'anything',\n",
       " 141: 'work',\n",
       " 142: 'should',\n",
       " 143: 'these',\n",
       " 144: 'sure',\n",
       " 145: 'doing',\n",
       " 146: 'give',\n",
       " 147: 'into',\n",
       " 148: 'father',\n",
       " 149: 'mr',\n",
       " 150: 'better',\n",
       " 151: 'because',\n",
       " 152: 'life',\n",
       " 153: 'even',\n",
       " 154: 'nothing',\n",
       " 155: 'must',\n",
       " 156: 'other',\n",
       " 157: 'than',\n",
       " 158: 'please',\n",
       " 159: 'still',\n",
       " 160: 'first',\n",
       " 161: 'thought',\n",
       " 162: 'before',\n",
       " 163: 'guy',\n",
       " 164: 'home',\n",
       " 165: 'hey',\n",
       " 166: 'night',\n",
       " 167: 'last',\n",
       " 168: 'call',\n",
       " 169: 'told',\n",
       " 170: 'girl',\n",
       " 171: 'ever',\n",
       " 172: 'isn',\n",
       " 173: 'shit',\n",
       " 174: 'does',\n",
       " 175: 'long',\n",
       " 176: 'won',\n",
       " 177: 'guess',\n",
       " 178: 'after',\n",
       " 179: 'help',\n",
       " 180: 'around',\n",
       " 181: 'ask',\n",
       " 182: 'thank',\n",
       " 183: 'find',\n",
       " 184: 'place',\n",
       " 185: 'put',\n",
       " 186: 'their',\n",
       " 187: 'things',\n",
       " 188: 'always',\n",
       " 189: 'great',\n",
       " 190: 'god',\n",
       " 191: 'bad',\n",
       " 192: 'name',\n",
       " 193: 'doesn',\n",
       " 194: 'course',\n",
       " 195: 'lot',\n",
       " 196: 'new',\n",
       " 197: 'big',\n",
       " 198: 'wait',\n",
       " 199: 'those',\n",
       " 200: 'listen',\n",
       " 201: 'old',\n",
       " 202: 'another',\n",
       " 203: 'day',\n",
       " 204: 'leave',\n",
       " 205: 'jack',\n",
       " 206: 'understand',\n",
       " 207: 'which',\n",
       " 208: 'made',\n",
       " 209: 'stop',\n",
       " 210: 'guys',\n",
       " 211: 'u',\n",
       " 212: 'gotta',\n",
       " 213: 'heard',\n",
       " 214: 'kill',\n",
       " 215: 'wrong',\n",
       " 216: 'happened',\n",
       " 217: 'may',\n",
       " 218: 'dead',\n",
       " 219: 'might',\n",
       " 220: 'fuck',\n",
       " 221: 'best',\n",
       " 222: 'years',\n",
       " 223: 'nice',\n",
       " 224: 'real',\n",
       " 225: 'believe',\n",
       " 226: 'son',\n",
       " 227: 'fucking',\n",
       " 228: 'job',\n",
       " 229: 'wasn',\n",
       " 230: 'ain',\n",
       " 231: 'wanted',\n",
       " 232: 'away',\n",
       " 233: 'talking',\n",
       " 234: 'everything',\n",
       " 235: 'again',\n",
       " 236: 'mind',\n",
       " 237: 'keep',\n",
       " 238: 'ya',\n",
       " 239: 'world',\n",
       " 240: 'done',\n",
       " 241: 'wouldn',\n",
       " 242: 'someone',\n",
       " 243: 'kind',\n",
       " 244: 'own',\n",
       " 245: 'else',\n",
       " 246: 'feel',\n",
       " 247: 'being',\n",
       " 248: 'friend',\n",
       " 249: 'three',\n",
       " 250: 'hell',\n",
       " 251: 'boy',\n",
       " 252: 'hear',\n",
       " 253: 'left',\n",
       " 254: 'enough',\n",
       " 255: 'saw',\n",
       " 256: 'morning',\n",
       " 257: 'show',\n",
       " 258: 'uh',\n",
       " 259: 'same',\n",
       " 260: 'came',\n",
       " 261: 'five',\n",
       " 262: 'mrs',\n",
       " 263: 'tonight',\n",
       " 264: 'yourself',\n",
       " 265: 'matter',\n",
       " 266: 'getting',\n",
       " 267: 'shot',\n",
       " 268: 'found',\n",
       " 269: 'says',\n",
       " 270: 'woman',\n",
       " 271: 'remember',\n",
       " 272: 'killed',\n",
       " 273: 'house',\n",
       " 274: 'pretty',\n",
       " 275: 'went',\n",
       " 276: 'coming',\n",
       " 277: 'few',\n",
       " 278: 'yet',\n",
       " 279: 'called',\n",
       " 280: 'while',\n",
       " 281: 'try',\n",
       " 282: 'stay',\n",
       " 283: 'frank',\n",
       " 284: 'men',\n",
       " 285: 'looking',\n",
       " 286: 'huh',\n",
       " 287: 'mother',\n",
       " 288: 'start',\n",
       " 289: 'haven',\n",
       " 290: 'many',\n",
       " 291: 'dad',\n",
       " 292: 'knew',\n",
       " 293: 'hello',\n",
       " 294: 'wife',\n",
       " 295: 'care',\n",
       " 296: 'four',\n",
       " 297: 'tomorrow',\n",
       " 298: 'business',\n",
       " 299: 'since',\n",
       " 300: 'alone',\n",
       " 301: 'every',\n",
       " 302: 'gun',\n",
       " 303: 'kid',\n",
       " 304: 'head',\n",
       " 305: 'fine',\n",
       " 306: 'ago',\n",
       " 307: 'days',\n",
       " 308: 'through',\n",
       " 309: 'problem',\n",
       " 310: 'thanks',\n",
       " 311: 'stuff',\n",
       " 312: 'dr',\n",
       " 313: 'pay',\n",
       " 314: 'seen',\n",
       " 315: 'excuse',\n",
       " 316: 'alright',\n",
       " 317: 'mozart',\n",
       " 318: 'next',\n",
       " 319: 'thousand',\n",
       " 320: 'whole',\n",
       " 321: 'myself',\n",
       " 322: 'use',\n",
       " 323: 'david',\n",
       " 324: 'fire',\n",
       " 325: 'wants',\n",
       " 326: 'working',\n",
       " 327: 'took',\n",
       " 328: 'most',\n",
       " 329: 'such',\n",
       " 330: 'friends',\n",
       " 331: 'aren',\n",
       " 332: 'already',\n",
       " 333: 'without',\n",
       " 334: 'husband',\n",
       " 335: 'hundred',\n",
       " 336: 'anyone',\n",
       " 337: 'young',\n",
       " 338: 'word',\n",
       " 339: 'change',\n",
       " 340: 'supposed',\n",
       " 341: 'eat',\n",
       " 342: 'game',\n",
       " 343: 'hope',\n",
       " 344: 'probably',\n",
       " 345: 'trying',\n",
       " 346: 'crazy',\n",
       " 347: 'jeffrey',\n",
       " 348: 'used',\n",
       " 349: 'exactly',\n",
       " 350: 'makes',\n",
       " 351: 'black',\n",
       " 352: 'car',\n",
       " 353: 'true',\n",
       " 354: 'play',\n",
       " 355: 'west',\n",
       " 356: 'mine',\n",
       " 357: 'town',\n",
       " 358: 'somebody',\n",
       " 359: 'peel',\n",
       " 360: 'school',\n",
       " 361: 'hi',\n",
       " 362: 'idea',\n",
       " 363: 'both',\n",
       " 364: 'couldn',\n",
       " 365: 'party',\n",
       " 366: 'minutes',\n",
       " 367: 'live',\n",
       " 368: 'run',\n",
       " 369: 'doctor',\n",
       " 370: 'actually',\n",
       " 371: 'gone',\n",
       " 372: 'meet',\n",
       " 373: 'deal',\n",
       " 374: 'question',\n",
       " 375: 'six',\n",
       " 376: 'police',\n",
       " 377: 'bank',\n",
       " 378: 'looks',\n",
       " 379: 'part',\n",
       " 380: 'forget',\n",
       " 381: 'different',\n",
       " 382: 'sleep',\n",
       " 383: 'case',\n",
       " 384: 'knows',\n",
       " 385: 'once',\n",
       " 386: 'elaine',\n",
       " 387: 'dickson',\n",
       " 388: 'austin',\n",
       " 389: 'ready',\n",
       " 390: 'story',\n",
       " 391: 'daddy',\n",
       " 392: 'boys',\n",
       " 393: 'afraid',\n",
       " 394: 'quite',\n",
       " 395: 'ten',\n",
       " 396: 'second',\n",
       " 397: 'em',\n",
       " 398: 'thinking',\n",
       " 399: 'c',\n",
       " 400: 'point',\n",
       " 401: 'twenty',\n",
       " 402: 'hate',\n",
       " 403: 'patrick',\n",
       " 404: 'number',\n",
       " 405: 'ass',\n",
       " 406: 'person',\n",
       " 407: 'together',\n",
       " 408: 'saying',\n",
       " 409: 'half',\n",
       " 410: 'hurt',\n",
       " 411: 'hard',\n",
       " 412: 'until',\n",
       " 413: 'sick',\n",
       " 414: 'minute',\n",
       " 415: 'speak',\n",
       " 416: 'jesus',\n",
       " 417: 'soon',\n",
       " 418: 'each',\n",
       " 419: 'married',\n",
       " 420: 'comes',\n",
       " 421: 'everybody',\n",
       " 422: 'trouble',\n",
       " 423: 'making',\n",
       " 424: 'bullshit',\n",
       " 425: 'shut',\n",
       " 426: 'under',\n",
       " 427: 'jesse',\n",
       " 428: 'everyone',\n",
       " 429: 'lost',\n",
       " 430: 'asked',\n",
       " 431: 'possible',\n",
       " 432: 'open',\n",
       " 433: 'miss',\n",
       " 434: 'door',\n",
       " 435: 'mom',\n",
       " 436: 'year',\n",
       " 437: 'today',\n",
       " 438: 'women',\n",
       " 439: 'bring',\n",
       " 440: 'yours',\n",
       " 441: 'phone',\n",
       " 442: 'anymore',\n",
       " 443: 'ted',\n",
       " 444: 'brother',\n",
       " 445: 'herr',\n",
       " 446: 'ma',\n",
       " 447: 'sometimes',\n",
       " 448: 'end',\n",
       " 449: 'buy',\n",
       " 450: 'anyway',\n",
       " 451: 'hand',\n",
       " 452: 'family',\n",
       " 453: 'room',\n",
       " 454: 'waiting',\n",
       " 455: 'goddamn',\n",
       " 456: 'wanna',\n",
       " 457: 'week',\n",
       " 458: 'stupid',\n",
       " 459: 'answer',\n",
       " 460: 'captain',\n",
       " 461: 'majesty',\n",
       " 462: 'anybody',\n",
       " 463: 'move',\n",
       " 464: 'shall',\n",
       " 465: 'lady',\n",
       " 466: 'evil',\n",
       " 467: 'cole',\n",
       " 468: 'twombley',\n",
       " 469: 'brian',\n",
       " 470: 'promise',\n",
       " 471: 'girls',\n",
       " 472: 'suppose',\n",
       " 473: 'against',\n",
       " 474: 'gave',\n",
       " 475: 'truth',\n",
       " 476: 'news',\n",
       " 477: 'face',\n",
       " 478: 'happy',\n",
       " 479: 'hours',\n",
       " 480: 'stephen',\n",
       " 481: 'having',\n",
       " 482: 'hot',\n",
       " 483: 'whatever',\n",
       " 484: 'bitch',\n",
       " 485: 'die',\n",
       " 486: 'taking',\n",
       " 487: 'control',\n",
       " 488: 'telling',\n",
       " 489: 'dear',\n",
       " 490: 'check',\n",
       " 491: 'cop',\n",
       " 492: 'apartment',\n",
       " 493: 'hit',\n",
       " 494: 'least',\n",
       " 495: 'months',\n",
       " 496: 'between',\n",
       " 497: 'office',\n",
       " 498: 'dunbar',\n",
       " 499: 'set',\n",
       " 500: 'body',\n",
       " 501: 'serious',\n",
       " 502: 'watch',\n",
       " 503: 'honey',\n",
       " 504: 'ah',\n",
       " 505: 'also',\n",
       " 506: 'worry',\n",
       " 507: 'turn',\n",
       " 508: 'cause',\n",
       " 509: 'hotel',\n",
       " 510: 'nobody',\n",
       " 511: 'damn',\n",
       " 512: 'kids',\n",
       " 513: 'late',\n",
       " 514: 'dollars',\n",
       " 515: 'president',\n",
       " 516: 'happen',\n",
       " 517: 'hold',\n",
       " 518: 'christ',\n",
       " 519: 'dorothy',\n",
       " 520: 'trust',\n",
       " 521: 'beautiful',\n",
       " 522: 'music',\n",
       " 523: 'pick',\n",
       " 524: 'gets',\n",
       " 525: 'full',\n",
       " 526: 'weeks',\n",
       " 527: 'rest',\n",
       " 528: 'wish',\n",
       " 529: 'brought',\n",
       " 530: 'coffee',\n",
       " 531: 'bit',\n",
       " 532: 'stand',\n",
       " 533: 'almost',\n",
       " 534: 'baby',\n",
       " 535: 'debbie',\n",
       " 536: 'break',\n",
       " 537: 'couple',\n",
       " 538: 'thirty',\n",
       " 539: 'death',\n",
       " 540: 'himself',\n",
       " 541: 'easy',\n",
       " 542: 'leaving',\n",
       " 543: 'read',\n",
       " 544: 'fuckin',\n",
       " 545: 'detective',\n",
       " 546: 'ahead',\n",
       " 547: 'daughter',\n",
       " 548: 'close',\n",
       " 549: 'hour',\n",
       " 550: 'james',\n",
       " 551: 'plan',\n",
       " 552: 'sex',\n",
       " 553: 'sit',\n",
       " 554: 'send',\n",
       " 555: 'small',\n",
       " 556: 'side',\n",
       " 557: 'running',\n",
       " 558: 'reason',\n",
       " 559: 'tried',\n",
       " 560: 'either',\n",
       " 561: 'write',\n",
       " 562: 'drink',\n",
       " 563: 'dinner',\n",
       " 564: 'ganz',\n",
       " 565: 'piece',\n",
       " 566: 'perfect',\n",
       " 567: 'city',\n",
       " 568: 'army',\n",
       " 569: 'gary',\n",
       " 570: 'though',\n",
       " 571: 'chance',\n",
       " 572: 'seems',\n",
       " 573: 'involved',\n",
       " 574: 'far',\n",
       " 575: 'war',\n",
       " 576: 'mad',\n",
       " 577: 'worse',\n",
       " 578: 'met',\n",
       " 579: 'middle',\n",
       " 580: 'rick',\n",
       " 581: 'paul',\n",
       " 582: 'mantan',\n",
       " 583: 'sister',\n",
       " 584: 'goes',\n",
       " 585: 'quit',\n",
       " 586: 'means',\n",
       " 587: 'food',\n",
       " 588: 'feeling',\n",
       " 589: 'questions',\n",
       " 590: 'o',\n",
       " 591: 'important',\n",
       " 592: 'walk',\n",
       " 593: 'funny',\n",
       " 594: 'plane',\n",
       " 595: 'scared',\n",
       " 596: 'asking',\n",
       " 597: 'looked',\n",
       " 598: 'tired',\n",
       " 599: 'forgive',\n",
       " 600: 'blood',\n",
       " 601: 'alive',\n",
       " 602: 'cut',\n",
       " 603: 'later',\n",
       " 604: 'luck',\n",
       " 605: 'film',\n",
       " 606: 'bet',\n",
       " 607: 'times',\n",
       " 608: 'shoot',\n",
       " 609: 'air',\n",
       " 610: 'y',\n",
       " 611: 'tom',\n",
       " 612: 'weather',\n",
       " 613: 'date',\n",
       " 614: 'high',\n",
       " 615: 'price',\n",
       " 616: 'killing',\n",
       " 617: 'seem',\n",
       " 618: 'mon',\n",
       " 619: 'dance',\n",
       " 620: 'rather',\n",
       " 621: 'water',\n",
       " 622: 'happens',\n",
       " 623: 'somethin',\n",
       " 624: 'straight',\n",
       " 625: 'street',\n",
       " 626: 'inside',\n",
       " 627: 'murder',\n",
       " 628: 'moment',\n",
       " 629: 'safe',\n",
       " 630: 'john',\n",
       " 631: 'wade',\n",
       " 632: 'kendall',\n",
       " 633: 'eve',\n",
       " 634: 'eyes',\n",
       " 635: 'asshole',\n",
       " 636: 'near',\n",
       " 637: 'chief',\n",
       " 638: 'fool',\n",
       " 639: 'front',\n",
       " 640: 'works',\n",
       " 641: 'along',\n",
       " 642: 'dream',\n",
       " 643: 'behind',\n",
       " 644: 'shouldn',\n",
       " 645: 'white',\n",
       " 646: 'bout',\n",
       " 647: 'till',\n",
       " 648: 'bed',\n",
       " 649: 'sounds',\n",
       " 650: 'sort',\n",
       " 651: 'drunk',\n",
       " 652: 'bill',\n",
       " 653: 'nick',\n",
       " 654: 'adam',\n",
       " 655: 'unless',\n",
       " 656: 'picture',\n",
       " 657: 'slow',\n",
       " 658: 'able',\n",
       " 659: 'land',\n",
       " 660: 'thinks',\n",
       " 661: 'parents',\n",
       " 662: 'department',\n",
       " 663: 'poor',\n",
       " 664: 'learn',\n",
       " 665: 'service',\n",
       " 666: 'outta',\n",
       " 667: 'mommy',\n",
       " 668: 'lying',\n",
       " 669: 'smart',\n",
       " 670: 'meeting',\n",
       " 671: 'line',\n",
       " 672: 'eight',\n",
       " 673: 'heart',\n",
       " 674: 'church',\n",
       " 675: 'matt',\n",
       " 676: 'pike',\n",
       " 677: 'powers',\n",
       " 678: 'milo',\n",
       " 679: 'started',\n",
       " 680: 'fun',\n",
       " 681: 'blow',\n",
       " 682: 'interesting',\n",
       " 683: 'weren',\n",
       " 684: 'paid',\n",
       " 685: 'beat',\n",
       " 686: 'perhaps',\n",
       " 687: 'ship',\n",
       " 688: 'country',\n",
       " 689: 'light',\n",
       " 690: 'marriage',\n",
       " 691: 'york',\n",
       " 692: 'hal',\n",
       " 693: 'strange',\n",
       " 694: 'gang',\n",
       " 695: 'hungry',\n",
       " 696: 'living',\n",
       " 697: 'its',\n",
       " 698: 'machine',\n",
       " 699: 'n',\n",
       " 700: 'gordon',\n",
       " 701: 'secret',\n",
       " 702: 'george',\n",
       " 703: 'owen',\n",
       " 704: 'cosgrove',\n",
       " 705: 'kidding',\n",
       " 706: 'become',\n",
       " 707: 'completely',\n",
       " 708: 'uncle',\n",
       " 709: 'past',\n",
       " 710: 'lie',\n",
       " 711: 'law',\n",
       " 712: 'kiss',\n",
       " 713: 'seven',\n",
       " 714: 'taken',\n",
       " 715: 'sent',\n",
       " 716: 'hospital',\n",
       " 717: 'fight',\n",
       " 718: 'report',\n",
       " 719: 'save',\n",
       " 720: 'hang',\n",
       " 721: 'died',\n",
       " 722: 'mission',\n",
       " 723: 'hands',\n",
       " 724: 'clear',\n",
       " 725: 'alice',\n",
       " 726: 'vienna',\n",
       " 727: 'fault',\n",
       " 728: 'ones',\n",
       " 729: 'playing',\n",
       " 730: 'missing',\n",
       " 731: 'busy',\n",
       " 732: 'fifty',\n",
       " 733: 'imagine',\n",
       " 734: 'besides',\n",
       " 735: 'absolutely',\n",
       " 736: 'follow',\n",
       " 737: 'return',\n",
       " 738: 'eddie',\n",
       " 739: 'glad',\n",
       " 740: 'finished',\n",
       " 741: 'million',\n",
       " 742: 'free',\n",
       " 743: 'cold',\n",
       " 744: 'dave',\n",
       " 745: 'drive',\n",
       " 746: 'private',\n",
       " 747: 'luther',\n",
       " 748: 'bother',\n",
       " 749: 'lose',\n",
       " 750: 'dumb',\n",
       " 751: 'mister',\n",
       " 752: 'boss',\n",
       " 753: 'figure',\n",
       " 754: 'careful',\n",
       " 755: 'yesterday',\n",
       " 756: 'field',\n",
       " 757: 'charlie',\n",
       " 758: 'bateman',\n",
       " 759: 'mueller',\n",
       " 760: 'hair',\n",
       " 761: 'month',\n",
       " 762: 'short',\n",
       " 763: 'state',\n",
       " 764: 'interested',\n",
       " 765: 'doin',\n",
       " 766: 'needs',\n",
       " 767: 'lunch',\n",
       " 768: 'others',\n",
       " 769: 'fair',\n",
       " 770: 'drugs',\n",
       " 771: 'problems',\n",
       " 772: 'folks',\n",
       " 773: 'building',\n",
       " 774: 'difference',\n",
       " 775: 'impossible',\n",
       " 776: 'darling',\n",
       " 777: 'flight',\n",
       " 778: 'seeing',\n",
       " 779: 'company',\n",
       " 780: 'moving',\n",
       " 781: 'explain',\n",
       " 782: 'lucky',\n",
       " 783: 'stones',\n",
       " 784: 'welcome',\n",
       " 785: 'nine',\n",
       " 786: 'none',\n",
       " 787: 'felt',\n",
       " 788: 'ways',\n",
       " 789: 'opera',\n",
       " 790: 'silly',\n",
       " 791: 'vanessa',\n",
       " 792: 'crap',\n",
       " 793: 'goin',\n",
       " 794: 'beer',\n",
       " 795: 'cool',\n",
       " 796: 'win',\n",
       " 797: 'given',\n",
       " 798: 'swear',\n",
       " 799: 'clothes',\n",
       " 800: 'book',\n",
       " 801: 'happening',\n",
       " 802: 'stopped',\n",
       " 803: 'scene',\n",
       " 804: 'ride',\n",
       " 805: 'information',\n",
       " 806: 'test',\n",
       " 807: 'wonderful',\n",
       " 808: 'handle',\n",
       " 809: 'hurry',\n",
       " 810: 'calling',\n",
       " 811: 'suit',\n",
       " 812: 'dreams',\n",
       " 813: 'lots',\n",
       " 814: 'using',\n",
       " 815: 'fifteen',\n",
       " 816: 'ball',\n",
       " 817: 'terrible',\n",
       " 818: 'anywhere',\n",
       " 819: 'finally',\n",
       " 820: 'wedding',\n",
       " 821: 'helen',\n",
       " 822: 'railroad',\n",
       " 823: 'sloan',\n",
       " 824: 'laz',\n",
       " 825: 'likes',\n",
       " 826: 'act',\n",
       " 827: 'except',\n",
       " 828: 'experience',\n",
       " 829: 'known',\n",
       " 830: 'pleasure',\n",
       " 831: 'movie',\n",
       " 832: 'bucks',\n",
       " 833: 'bought',\n",
       " 834: 'force',\n",
       " 835: 'child',\n",
       " 836: 'risk',\n",
       " 837: 'marry',\n",
       " 838: 'certain',\n",
       " 839: 'famous',\n",
       " 840: 'station',\n",
       " 841: 'partner',\n",
       " 842: 'drug',\n",
       " 843: 'sweetheart',\n",
       " 844: 'clock',\n",
       " 845: 'mouth',\n",
       " 846: 'american',\n",
       " 847: 'worked',\n",
       " 848: 'kept',\n",
       " 849: 'aw',\n",
       " 850: 'sense',\n",
       " 851: 'low',\n",
       " 852: 'earth',\n",
       " 853: 'interest',\n",
       " 854: 'class',\n",
       " 855: 'surprised',\n",
       " 856: 'promised',\n",
       " 857: 'loved',\n",
       " 858: 'fall',\n",
       " 859: 'human',\n",
       " 860: 'box',\n",
       " 861: 'broke',\n",
       " 862: 'ear',\n",
       " 863: 'papa',\n",
       " 864: 'dela',\n",
       " 865: 'extra',\n",
       " 866: 'bianca',\n",
       " 867: 'upset',\n",
       " 868: 'expect',\n",
       " 869: 'burn',\n",
       " 870: 'insane',\n",
       " 871: 'choice',\n",
       " 872: 'sound',\n",
       " 873: 'spend',\n",
       " 874: 'wake',\n",
       " 875: 'cash',\n",
       " 876: 'lives',\n",
       " 877: 'prove',\n",
       " 878: 'special',\n",
       " 879: 'cannot',\n",
       " 880: 'accept',\n",
       " 881: 'mostly',\n",
       " 882: 'card',\n",
       " 883: 'shower',\n",
       " 884: 'tv',\n",
       " 885: 'message',\n",
       " 886: 'g',\n",
       " 887: 'shoes',\n",
       " 888: 'takes',\n",
       " 889: 'killer',\n",
       " 890: 'bag',\n",
       " 891: 'dog',\n",
       " 892: 'director',\n",
       " 893: 'during',\n",
       " 894: 'burned',\n",
       " 895: 'hasn',\n",
       " 896: 'fact',\n",
       " 897: 'certainly',\n",
       " 898: 'fast',\n",
       " 899: 'moon',\n",
       " 900: 'truck',\n",
       " 901: 'protect',\n",
       " 902: 'instead',\n",
       " 903: 'personal',\n",
       " 904: 'system',\n",
       " 905: 'screw',\n",
       " 906: 'outside',\n",
       " 907: 'board',\n",
       " 908: 'lord',\n",
       " 909: 'smoking',\n",
       " 910: 'fellow',\n",
       " 911: 'picked',\n",
       " 912: 'somewhere',\n",
       " 913: 'post',\n",
       " 914: 'teddy',\n",
       " 915: 'accident',\n",
       " 916: 'bachelor',\n",
       " 917: 'farm',\n",
       " 918: 'jill',\n",
       " 919: 'top',\n",
       " 920: 'pilot',\n",
       " 921: 'sergeant',\n",
       " 922: 'offer',\n",
       " 923: 'ho',\n",
       " 924: 'calvin',\n",
       " 925: 'cost',\n",
       " 926: 'figured',\n",
       " 927: 'joey',\n",
       " 928: 'wonder',\n",
       " 929: 'forever',\n",
       " 930: 'starts',\n",
       " 931: 'rules',\n",
       " 932: 'teach',\n",
       " 933: 'angry',\n",
       " 934: 'rock',\n",
       " 935: 'reading',\n",
       " 936: 'appreciate',\n",
       " 937: 'whose',\n",
       " 938: 'cover',\n",
       " 939: 'agree',\n",
       " 940: 'simply',\n",
       " 941: 'listening',\n",
       " 942: 'visit',\n",
       " 943: 'address',\n",
       " 944: 'lawyer',\n",
       " 945: 'biggest',\n",
       " 946: 'giving',\n",
       " 947: 'table',\n",
       " 948: 'career',\n",
       " 949: 'checked',\n",
       " 950: 'crime',\n",
       " 951: 'fucked',\n",
       " 952: 'girlfriend',\n",
       " 953: 'ha',\n",
       " 954: 'guns',\n",
       " 955: 'clean',\n",
       " 956: 'continued',\n",
       " 957: 'bye',\n",
       " 958: 'stories',\n",
       " 959: 'gas',\n",
       " 960: 'drop',\n",
       " 961: 'weird',\n",
       " 962: 'bastard',\n",
       " 963: 'less',\n",
       " 964: 'awful',\n",
       " 965: 'wondering',\n",
       " 966: 'honest',\n",
       " 967: 'power',\n",
       " 968: 'eye',\n",
       " 969: 'single',\n",
       " 970: 'floor',\n",
       " 971: 'worth',\n",
       " 972: 'officer',\n",
       " 973: 'cards',\n",
       " 974: 'bomb',\n",
       " 975: 'l',\n",
       " 976: 'blue',\n",
       " 977: 'ray',\n",
       " 978: 'simple',\n",
       " 979: 'wolfi',\n",
       " 980: 'sire',\n",
       " 981: 'hirsch',\n",
       " 982: 'cluett',\n",
       " 983: 'williams',\n",
       " 984: 'younger',\n",
       " 985: 'frozen',\n",
       " 986: 'negro',\n",
       " 987: 'mau',\n",
       " 988: 'levi',\n",
       " 989: 'public',\n",
       " 990: 'boyfriend',\n",
       " 991: 'dating',\n",
       " 992: 'huge',\n",
       " 993: 'normal',\n",
       " 994: 'totally',\n",
       " 995: 'group',\n",
       " 996: 'liked',\n",
       " 997: 'direct',\n",
       " 998: 'legs',\n",
       " 999: 'fit',\n",
       " 1000: 'pissed',\n",
       " ...}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp_lang.index_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KrpqJ9fWtKFG",
    "outputId": "196d27ef-c5ad-4e59-8a12-cf14c568f08e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "242"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_length_targ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2nSuKGOktLk7",
    "outputId": "e574bf55-6164-47ed-8bd6-59caf3da3385"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "273"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_length_inp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "YP5WydUKtMz6"
   },
   "outputs": [],
   "source": [
    "input_tensor_train = input_tensor\n",
    "target_tensor_train = target_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "_CKDtpietdmc"
   },
   "outputs": [],
   "source": [
    "def convert(lang, tensor):\n",
    "  for t in tensor:\n",
    "    if t!=0:\n",
    "      print (\"%d ----> %s\" % (t, lang.index_word[t]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6uR-DKcZthr1",
    "outputId": "aeb63de6-65d3-449b-fcb3-057313cd7e8b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Language; index to word mapping\n",
      "2 ----> <start>\n",
      "42 ----> can\n",
      "22 ----> we\n",
      "121 ----> make\n",
      "24 ----> this\n",
      "1309 ----> quick\n",
      "7 ----> ?\n",
      "4914 ----> roxanne\n",
      "4915 ----> korrine\n",
      "15 ----> and\n",
      "4916 ----> andrew\n",
      "4917 ----> barrett\n",
      "36 ----> are\n",
      "481 ----> having\n",
      "87 ----> an\n",
      "1617 ----> incredibly\n",
      "4918 ----> horrendous\n",
      "989 ----> public\n",
      "536 ----> break\n",
      "56 ----> up\n",
      "38 ----> on\n",
      "8 ----> the\n",
      "4919 ----> quad\n",
      "1 ----> .\n",
      "235 ----> again\n",
      "1 ----> .\n",
      "3 ----> <end>\n",
      "\n",
      "Target Language; index to word mapping\n",
      "2 ----> <start>\n",
      "68 ----> well\n",
      "4 ----> ,\n",
      "5 ----> i\n",
      "159 ----> thought\n",
      "24 ----> we\n",
      "81 ----> d\n",
      "321 ----> start\n",
      "41 ----> with\n",
      "4838 ----> pronunciation\n",
      "4 ----> ,\n",
      "51 ----> if\n",
      "14 ----> that\n",
      "11 ----> s\n",
      "102 ----> okay\n",
      "41 ----> with\n",
      "6 ----> you\n",
      "1 ----> .\n",
      "3 ----> <end>\n"
     ]
    }
   ],
   "source": [
    "print (\"Input Language; index to word mapping\")\n",
    "convert(inp_lang, input_tensor_train[0])\n",
    "print ()\n",
    "print (\"Target Language; index to word mapping\")\n",
    "convert(targ_lang, target_tensor_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "kl1HpXWxtpzF"
   },
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 20000 #len(input_tensor_train)\n",
    "BATCH_SIZE = 4 #64\n",
    "steps_per_epoch = len(input_tensor_train)//BATCH_SIZE\n",
    "embedding_dim = 4 #512\n",
    "units = 64 # 1024\n",
    "vocab_inp_size = len(inp_lang.word_index)+1\n",
    "vocab_tar_size = len(targ_lang.word_index)+1\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cacTQejAtz11",
    "outputId": "afd044c5-6c6e-44d5-9400-f99d4396289a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([4, 273]), TensorShape([4, 242]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_input_batch, example_target_batch = next(iter(dataset))\n",
    "example_input_batch.shape, example_target_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "VyI4ylstt1Gl"
   },
   "outputs": [],
   "source": [
    "# Write the encoder and decoder model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "xmJbU2IHt8OV"
   },
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "  def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
    "    super(Encoder, self).__init__()\n",
    "    self.batch_sz = batch_sz\n",
    "    self.enc_units = enc_units\n",
    "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "    self.gru = tf.keras.layers.GRU(self.enc_units,\n",
    "                                   return_sequences=True,\n",
    "                                   return_state=True,\n",
    "                                   recurrent_initializer='glorot_uniform')\n",
    "\n",
    "  def call(self, x, hidden):\n",
    "    x = self.embedding(x)\n",
    "    output, state = self.gru(x, initial_state = hidden)\n",
    "    return output, state\n",
    "\n",
    "  def initialize_hidden_state(self):\n",
    "    return tf.zeros((self.batch_sz, self.enc_units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2_OUIL-GuAvG",
    "outputId": "bb6619ba-f15e-4500-c31f-70b09b78be7c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder output shape: (batch size, sequence length, units) (4, 273, 64)\n",
      "Encoder Hidden state shape: (batch size, units) (4, 64)\n"
     ]
    }
   ],
   "source": [
    "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
    "\n",
    "# sample input\n",
    "sample_hidden = encoder.initialize_hidden_state()\n",
    "sample_output, sample_hidden = encoder(example_input_batch, sample_hidden)\n",
    "print ('Encoder output shape: (batch size, sequence length, units) {}'.format(sample_output.shape))\n",
    "print ('Encoder Hidden state shape: (batch size, units) {}'.format(sample_hidden.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "DoYZDXZGuETm"
   },
   "outputs": [],
   "source": [
    "class BahdanauAttention(tf.keras.layers.Layer):\n",
    "  def __init__(self, units):\n",
    "    super(BahdanauAttention, self).__init__()\n",
    "    self.W1 = tf.keras.layers.Dense(units)\n",
    "    self.W2 = tf.keras.layers.Dense(units)\n",
    "    self.V = tf.keras.layers.Dense(1)\n",
    "\n",
    "  def call(self, query, values):\n",
    "    # query hidden state shape == (batch_size, hidden size)\n",
    "    # query_with_time_axis shape == (batch_size, 1, hidden size)\n",
    "    # values shape == (batch_size, max_len, hidden size)\n",
    "    # we are doing this to broadcast addition along the time axis to calculate the score\n",
    "    query_with_time_axis = tf.expand_dims(query, 1)\n",
    "\n",
    "    # score shape == (batch_size, max_length, 1)\n",
    "    # we get 1 at the last axis because we are applying score to self.V\n",
    "    # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
    "    score = self.V(tf.nn.tanh(\n",
    "        self.W1(query_with_time_axis) + self.W2(values)))\n",
    "\n",
    "    # attention_weights shape == (batch_size, max_length, 1)\n",
    "    attention_weights = tf.nn.softmax(score, axis=1)\n",
    "\n",
    "    # context_vector shape after sum == (batch_size, hidden_size)\n",
    "    context_vector = attention_weights * values\n",
    "    context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "\n",
    "    return context_vector, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AoioBRykugEV",
    "outputId": "6d59172f-afa1-4f50-97f0-ba6d493e709b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention result shape: (batch size, units) (4, 64)\n",
      "Attention weights shape: (batch_size, sequence_length, 1) (4, 273, 1)\n"
     ]
    }
   ],
   "source": [
    "attention_layer = BahdanauAttention(10)\n",
    "attention_result, attention_weights = attention_layer(sample_hidden, sample_output)\n",
    "\n",
    "print(\"Attention result shape: (batch size, units) {}\".format(attention_result.shape))\n",
    "print(\"Attention weights shape: (batch_size, sequence_length, 1) {}\".format(attention_weights.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "igf0F39gulE9"
   },
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "  def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
    "    super(Decoder, self).__init__()\n",
    "    self.batch_sz = batch_sz\n",
    "    self.dec_units = dec_units\n",
    "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "    self.gru = tf.keras.layers.GRU(self.dec_units,\n",
    "                                   return_sequences=True,\n",
    "                                   return_state=True,\n",
    "                                   recurrent_initializer='glorot_uniform')\n",
    "    self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "    # used for attention\n",
    "    self.attention = BahdanauAttention(self.dec_units)\n",
    "\n",
    "  def call(self, x, hidden, enc_output):\n",
    "    # enc_output shape == (batch_size, max_length, hidden_size)\n",
    "    context_vector, attention_weights = self.attention(hidden, enc_output)\n",
    "\n",
    "    # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
    "    x = self.embedding(x)\n",
    "\n",
    "    # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
    "    x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "\n",
    "    # passing the concatenated vector to the GRU\n",
    "    output, state = self.gru(x)\n",
    "\n",
    "    # output shape == (batch_size * 1, hidden_size)\n",
    "    output = tf.reshape(output, (-1, output.shape[2]))\n",
    "\n",
    "    # output shape == (batch_size, vocab)\n",
    "    x = self.fc(output)\n",
    "\n",
    "    return x, state, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gHeZF3sauoOP",
    "outputId": "9c0e12cd-b806-4393-8f42-147658c06344"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder output shape: (batch_size, vocab size) (4, 9430)\n"
     ]
    }
   ],
   "source": [
    "decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)\n",
    "\n",
    "sample_decoder_output, _, _ = decoder(tf.random.uniform((BATCH_SIZE, 1)),\n",
    "                                      sample_hidden, sample_output)\n",
    "\n",
    "print ('Decoder output shape: (batch_size, vocab size) {}'.format(sample_decoder_output.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "OUQ6s7SnupmJ"
   },
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "  loss_ = loss_object(real, pred)\n",
    "\n",
    "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "  loss_ *= mask\n",
    "\n",
    "  return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "4RovC5QlusLl"
   },
   "outputs": [],
   "source": [
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
    "                                 encoder=encoder,\n",
    "                                 decoder=decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "UacbegMhuwa-"
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(inp, targ, enc_hidden):\n",
    "  loss = 0\n",
    "\n",
    "  with tf.GradientTape() as tape:\n",
    "    enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
    "\n",
    "    dec_hidden = enc_hidden\n",
    "\n",
    "    dec_input = tf.expand_dims([targ_lang.word_index['<start>']] * BATCH_SIZE, 1)\n",
    "\n",
    "    # Teacher forcing - feeding the target as the next input\n",
    "    for t in range(1, targ.shape[1]):\n",
    "      # passing enc_output to the decoder\n",
    "      predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
    "\n",
    "      loss += loss_function(targ[:, t], predictions)\n",
    "\n",
    "      # using teacher forcing\n",
    "      dec_input = tf.expand_dims(targ[:, t], 1)\n",
    "\n",
    "  batch_loss = (loss / int(targ.shape[1]))\n",
    "\n",
    "  variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "\n",
    "  gradients = tape.gradient(loss, variables)\n",
    "\n",
    "  optimizer.apply_gradients(zip(gradients, variables))\n",
    "\n",
    "  return batch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "_NS3ws-xu0Mh"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Loss 0.6523\n",
      "Epoch 1 Batch 100 Loss 0.2041\n",
      "Epoch 1 Batch 200 Loss 0.5143\n",
      "Epoch 1 Batch 300 Loss 0.3473\n",
      "Epoch 1 Batch 400 Loss 0.1570\n",
      "Epoch 1 Batch 500 Loss 0.2720\n",
      "Epoch 1 Batch 600 Loss 0.2232\n",
      "Epoch 1 Batch 700 Loss 0.2171\n",
      "Epoch 1 Batch 800 Loss 0.2330\n",
      "Epoch 1 Batch 900 Loss 0.2187\n",
      "Epoch 1 Batch 1000 Loss 0.2210\n",
      "Epoch 1 Batch 1100 Loss 0.3514\n",
      "Epoch 1 Batch 1200 Loss 0.6012\n",
      "Epoch 1 Batch 1300 Loss 0.2414\n",
      "Epoch 1 Batch 1400 Loss 0.1896\n",
      "Epoch 1 Batch 1500 Loss 0.2836\n",
      "Epoch 1 Batch 1600 Loss 0.1859\n",
      "Epoch 1 Batch 1700 Loss 0.2657\n",
      "Epoch 1 Batch 1800 Loss 0.5869\n",
      "Epoch 1 Batch 1900 Loss 0.2211\n",
      "Epoch 1 Batch 2000 Loss 0.4091\n",
      "Epoch 1 Batch 2100 Loss 0.2966\n",
      "Epoch 1 Batch 2200 Loss 0.1534\n",
      "Epoch 1 Batch 2300 Loss 0.6176\n",
      "Epoch 1 Batch 2400 Loss 0.2589\n",
      "Epoch 1 Batch 2500 Loss 0.7386\n",
      "Epoch 1 Batch 2600 Loss 0.1870\n",
      "Epoch 1 Batch 2700 Loss 0.2606\n",
      "Epoch 1 Batch 2800 Loss 0.1379\n",
      "Epoch 1 Batch 2900 Loss 0.2756\n",
      "Epoch 1 Batch 3000 Loss 0.1089\n",
      "Epoch 1 Batch 3100 Loss 0.3971\n",
      "Epoch 1 Batch 3200 Loss 0.2462\n",
      "Epoch 1 Loss 0.3320\n",
      "Time taken for 1 epoch 1248.5321419239044 sec\n",
      "\n",
      "Epoch 2 Batch 0 Loss 0.8621\n",
      "Epoch 2 Batch 100 Loss 0.3321\n",
      "Epoch 2 Batch 200 Loss 0.3895\n",
      "Epoch 2 Batch 300 Loss 0.3715\n",
      "Epoch 2 Batch 400 Loss 0.3070\n",
      "Epoch 2 Batch 500 Loss 0.3034\n",
      "Epoch 2 Batch 600 Loss 0.1809\n",
      "Epoch 2 Batch 700 Loss 0.4217\n",
      "Epoch 2 Batch 800 Loss 0.3929\n",
      "Epoch 2 Batch 900 Loss 0.6989\n",
      "Epoch 2 Batch 1000 Loss 0.1819\n",
      "Epoch 2 Batch 1100 Loss 0.5252\n",
      "Epoch 2 Batch 1200 Loss 0.1060\n",
      "Epoch 2 Batch 1300 Loss 0.3813\n",
      "Epoch 2 Batch 1400 Loss 0.4567\n",
      "Epoch 2 Batch 1500 Loss 0.3150\n",
      "Epoch 2 Batch 1600 Loss 0.2810\n",
      "Epoch 2 Batch 1700 Loss 0.3194\n",
      "Epoch 2 Batch 1800 Loss 0.2069\n",
      "Epoch 2 Batch 1900 Loss 0.5099\n",
      "Epoch 2 Batch 2000 Loss 0.2509\n",
      "Epoch 2 Batch 2100 Loss 0.1896\n",
      "Epoch 2 Batch 2200 Loss 0.2475\n",
      "Epoch 2 Batch 2300 Loss 0.2498\n",
      "Epoch 2 Batch 2400 Loss 0.6576\n",
      "Epoch 2 Batch 2500 Loss 0.4076\n",
      "Epoch 2 Batch 2600 Loss 0.0936\n",
      "Epoch 2 Batch 2700 Loss 0.0834\n",
      "Epoch 2 Batch 2800 Loss 0.5545\n",
      "Epoch 2 Batch 2900 Loss 0.1504\n",
      "Epoch 2 Batch 3000 Loss 0.1756\n",
      "Epoch 2 Batch 3100 Loss 0.1818\n",
      "Epoch 2 Batch 3200 Loss 0.1273\n",
      "Epoch 2 Loss 0.3030\n",
      "Time taken for 1 epoch 940.5748898983002 sec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 2\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "  start = time.time()\n",
    "\n",
    "  enc_hidden = encoder.initialize_hidden_state()\n",
    "  total_loss = 0\n",
    "\n",
    "  for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
    "    batch_loss = train_step(inp, targ, enc_hidden)\n",
    "    total_loss += batch_loss\n",
    "\n",
    "    if batch % 100 == 0:\n",
    "      print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                                   batch,\n",
    "                                                   batch_loss.numpy()))\n",
    "  # saving (checkpoint) the model every 2 epochs\n",
    "  if (epoch + 1) % 2 == 0:\n",
    "    checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "\n",
    "  print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                      total_loss / steps_per_epoch))\n",
    "  print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "qKYUZy13HRpI"
   },
   "outputs": [],
   "source": [
    "def evaluate(sentence):\n",
    "  attention_plot = np.zeros((max_length_targ, max_length_inp))\n",
    "\n",
    "  sentence = preprocess_sentence(sentence)\n",
    "\n",
    "  inputs = [inp_lang.word_index[i] for i in sentence.split(' ')]\n",
    "  inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n",
    "                                                         maxlen=max_length_inp,\n",
    "                                                         padding='post')\n",
    "  inputs = tf.convert_to_tensor(inputs)\n",
    "\n",
    "  result = ''\n",
    "\n",
    "  hidden = [tf.zeros((1, units))]\n",
    "  enc_out, enc_hidden = encoder(inputs, hidden)\n",
    "\n",
    "  dec_hidden = enc_hidden\n",
    "  dec_input = tf.expand_dims([targ_lang.word_index['<start>']], 0)\n",
    "\n",
    "  for t in range(max_length_targ):\n",
    "    predictions, dec_hidden, attention_weights = decoder(dec_input,\n",
    "                                                         dec_hidden,\n",
    "                                                         enc_out)\n",
    "\n",
    "    # storing the attention weights to plot later on\n",
    "    attention_weights = tf.reshape(attention_weights, (-1, ))\n",
    "    attention_plot[t] = attention_weights.numpy()\n",
    "\n",
    "    predicted_id = tf.argmax(predictions[0]).numpy()\n",
    "\n",
    "    result += targ_lang.index_word[predicted_id] + ' '\n",
    "\n",
    "    if targ_lang.index_word[predicted_id] == '<end>':\n",
    "      return result, sentence, attention_plot\n",
    "\n",
    "    # the predicted ID is fed back into the model\n",
    "    dec_input = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "  return result, sentence, attention_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "oz6hP31cHSwH"
   },
   "outputs": [],
   "source": [
    "# function for plotting the attention weights\n",
    "def plot_attention(attention, sentence, predicted_sentence):\n",
    "  fig = plt.figure(figsize=(10,10))\n",
    "  ax = fig.add_subplot(1, 1, 1)\n",
    "  ax.matshow(attention, cmap='viridis')\n",
    "\n",
    "  fontdict = {'fontsize': 14}\n",
    "\n",
    "  ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n",
    "  ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n",
    "\n",
    "  ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "  ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "qx8Pwd-2HWCf"
   },
   "outputs": [],
   "source": [
    "def chatbot(sentence):\n",
    "  result, sentence, attention_plot = evaluate(sentence)\n",
    "\n",
    "  print('Input: %s' % (sentence))\n",
    "  print('Predicted translation: {}'.format(result))\n",
    "\n",
    "  # attention_plot = attention_plot[:len(result.split(' ')), :len(sentence.split(' '))]\n",
    "  # plot_attention(attention_plot, sentence.split(' '), result.split(' '))\n",
    "  return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "V1L-WdEWHYyP"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x1ddf0561b48>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# restoring the latest checkpoint in checkpoint_dir\n",
    "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "PbTdwdNHHbW_"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: <start> who are you ? <end>\n",
      "Predicted translation: i m a little . <end> \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'i m a little . <end> '"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chatbot('who are you?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "vIi5SY_WHgx8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: <start> what did you eat for breakfast ? <end>\n",
      "Predicted translation: i m a little . <end> \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'i m a little . <end> '"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chatbot('what did you eat for breakfast?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "I-_ZdvINHw00"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: <start> did you finish your homework ? <end>\n",
      "Predicted translation: i m a little . <end> \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'i m a little . <end> '"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chatbot('did you finish your homework?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1634, 1634)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions_val, answers_val = load_conversations(80001,80500)\n",
    "len(questions_val),len(answers_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bleuplus = 0\n",
    "\n",
    "# for ques_, ans_ in zip(questions_val[0,1],answers_val[0,1]):\n",
    "#     predicted = chatbot(ques_)\n",
    "#     print('Real Response: {}'.format(ans_))\n",
    "#     bleu_ = bleu.sentence_bleu([ans_.split()], predicted.split())\n",
    "#     print(\"BLEU Score: \", bleu_)\n",
    "#     print(\"----\" *20)\n",
    "\n",
    "#     bleuplus += bleu_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: <start> start what is it , son ? end <end>\n",
      "Predicted translation: i m a little . <end> \n",
      "Real Response: <start> two more hogs got the fever . <end>\n",
      "BLEU Score:  4.597292750167502e-155\n",
      "--------------------------------------------------------------------------------\n",
      "Input: <start> start who s he ? end <end>\n",
      "Predicted translation: i m a little . <end> \n",
      "Real Response: <start> best we move that pig . <end>\n",
      "BLEU Score:  5.431059661402121e-155\n",
      "--------------------------------------------------------------------------------\n",
      "Input: <start> start three . end <end>\n",
      "Predicted translation: i m a little . <end> \n",
      "Real Response: <start> three ? that ain t bad considerin . <end>\n",
      "BLEU Score:  3.891524296988902e-155\n",
      "--------------------------------------------------------------------------------\n",
      "Input: <start> start that fella come by . . . tom . end <end>\n",
      "Predicted translation: i m a little . <end> \n",
      "Real Response: <start> tom ? <end>\n",
      "BLEU Score:  1.1640469867513693e-231\n",
      "--------------------------------------------------------------------------------\n",
      "Input: <start> start did you . . . did you . . . ? end <end>\n",
      "Predicted translation: i m a little . <end> \n",
      "Real Response: <start> did i what ? <end>\n",
      "BLEU Score:  1.384292958842266e-231\n",
      "--------------------------------------------------------------------------------\n",
      "Input: <start> start all that money . . . i mean . . . did you . . . ? end <end>\n",
      "Predicted translation: i m a little . <end> \n",
      "Real Response: <start> steal it ? naw , i didn t steal it . <end>\n",
      "BLEU Score:  2.6121332065565987e-155\n",
      "--------------------------------------------------------------------------------\n",
      "Input: <start> start who you are end <end>\n",
      "Predicted translation: i m a little . <end> \n",
      "Real Response: <start> just a girl , i work at magnin s <end>\n",
      "BLEU Score:  6.657922819542466e-232\n",
      "--------------------------------------------------------------------------------\n",
      "Input: <start> start and how you happen to be living here . end <end>\n",
      "Predicted translation: i m a little . <end> \n",
      "Real Response: <start> it s a place to live , that s all . <end>\n",
      "BLEU Score:  2.6121332065565987e-155\n",
      "--------------------------------------------------------------------------------\n",
      "Input: <start> start you ruined my life , mister . . . me and my wife . . . and i am going to ruin yours . . . you don t have to go out there to see that girl . we been going four years . four years . . . my wife s been crying herself to sleep what they , what , what they did to her sister . end <end>\n",
      "Predicted translation: i m a little . <end> \n",
      "Real Response: <start> i swear to you i wouldn t have turned the offer down unless i thought that i could win the case . . . <end>\n",
      "BLEU Score:  2.992429604980366e-156\n",
      "--------------------------------------------------------------------------------\n",
      "Input: <start> start if i could accept the offer right now , i would . they took it back . end <end>\n",
      "Predicted translation: i m a little . <end> \n",
      "Real Response: <start> i understand . i went to the bar association . they tell me you re going to be disbarred . <end>\n",
      "BLEU Score:  5.828457007080054e-156\n",
      "--------------------------------------------------------------------------------\n",
      "Input: <start> start yes . end <end>\n",
      "Predicted translation: i m a little . <end> \n",
      "Real Response: <start> well , then what are you doing here ? <end>\n",
      "BLEU Score:  5.058927350602078e-232\n",
      "--------------------------------------------------------------------------------\n",
      "Input: <start> start i m going to help her . end <end>\n",
      "Predicted translation: i m a little . <end> \n",
      "Real Response: <start> to do what . . . ? to do what , for chrissake . . . ? to help her to do what ? she s dead . . . <end>\n",
      "BLEU Score:  9.94733275414122e-157\n",
      "--------------------------------------------------------------------------------\n",
      "Input: <start> start what was you shootin at us for , anyhow ? end <end>\n",
      "Predicted translation: i m a little . <end> \n",
      "Real Response: <start> thought you was followin me . <end>\n",
      "BLEU Score:  5.431059661402121e-155\n",
      "--------------------------------------------------------------------------------\n",
      "Input: <start> start . . . and the desk sergeant is actually trying to tell me he can t release you ? can you believe that ? you weren t even charged . new york police jesus . i want to take pictures of your face to bring to the d . a . first thing in the morning . end <end>\n",
      "Predicted translation: i m a little . <end> \n",
      "Real Response: <start> just forget about it . <end>\n",
      "BLEU Score:  6.416038883891965e-155\n",
      "--------------------------------------------------------------------------------\n",
      "Input: <start> start i have this thing in front of a grand jury by monday . end <end>\n",
      "Predicted translation: i m a little . <end> \n",
      "Real Response: <start> edie , please . i don t want to hear this right now . what did renault and fortier say ? <end>\n",
      "BLEU Score:  9.010518705502864e-233\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "bleuplus = 0\n",
    "\n",
    "for i in [0,1,4,5,11,13,1000,1002,500,504,700,749,22,90,91]:\n",
    "    ques_ = questions_val[i]\n",
    "    ans_ = answers_val[i]\n",
    "    predicted = chatbot(ques_)\n",
    "    print('Real Response: {}'.format(ans_))\n",
    "    bleu_ = bleu.sentence_bleu([ans_.split()], predicted.split())\n",
    "    print(\"BLEU Score: \", bleu_)\n",
    "    print(\"----\" *20)\n",
    "\n",
    "    bleuplus += bleu_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.1315202437142175e-155"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bleuplus/15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: <start> how are you doing ? <end>\n",
      "Predicted translation: i m a little . <end> \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'i m a little . <end> '"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chatbot('how are you doing?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "chatbot_attention 4.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
